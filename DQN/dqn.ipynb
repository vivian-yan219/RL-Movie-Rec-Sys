{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_data() -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and preprocess MovieLens 100k data.\n",
    "    Returns (ratings_df, users_df, movies_df)\n",
    "\n",
    "    ratings_df: user_id, movie_id, rating, timestamp\n",
    "    users_df: user_id, gender, age, occupation, zip\n",
    "    movies_df: movie_id, title, genres (one-hot genre columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    ROOT_DIR = os.getcwd()\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR, '..'))\n",
    "    DATA_DIR = os.path.join(ROOT_DIR, 'ml-100k')\n",
    "\n",
    "    ratings = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'))\n",
    "    ratings.to_csv(os.path.join(DATA_DIR, 'mod_ratings.csv'), sep='\\t', index=False)\n",
    "    movies = pd.read_csv(os.path.join(DATA_DIR, 'movies.csv'))\n",
    "    movies.to_csv(os.path.join(DATA_DIR, 'mod_movies.csv'), sep='\\t', index=False)\n",
    "\n",
    "    #Loading datasets\n",
    "    ratings_list = [i.strip().split(\"\\t\") for i in open(os.path.join(DATA_DIR,'mod_ratings.csv'), 'r').readlines()]\n",
    "    ratings_df = pd.DataFrame(ratings_list[1:], columns = ['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "    ratings_df['user_id'] = ratings_df['user_id'].apply(pd.to_numeric)\n",
    "    ratings_df['movie_id'] = ratings_df['movie_id'].apply(pd.to_numeric)\n",
    "    ratings_df['rating'] = ratings_df['rating'].astype(float)\n",
    "\n",
    "    movies_list = [i.strip().split(\"\\t\") for i in open(os.path.join(DATA_DIR,'mod_movies.csv'),encoding='latin-1').readlines()]\n",
    "    movies_df = pd.DataFrame(movies_list[1:], columns = ['movie_id', 'title', 'genres'])\n",
    "    movies_df['movie_id'] = movies_df['movie_id'].apply(pd.to_numeric)\n",
    "\n",
    "    # Expand genres into one-hot\n",
    "    genres_list = sorted({g for sub in movies_df['genres'].str.split('|') for g in sub})\n",
    "    for genre in genres_list:\n",
    "        movies_df[genre] = movies_df['genres'].str.contains(genre).astype(int)\n",
    "\n",
    "    movies_df.drop(columns=['genres'], inplace=True)\n",
    "\n",
    "    users_1m_path = os.path.join(DATA_DIR, 'users-1m.dat')\n",
    "    users_1m_list = [i.strip().split(\"::\") for i in open(users_1m_path, 'r').readlines()]\n",
    "    users_1m_df = pd.DataFrame(users_1m_list, columns=['user_id', 'gender', 'age', 'occupation', 'zip_code'])\n",
    "    users_1m_df['user_id'] = users_1m_df['user_id'].apply(pd.to_numeric)\n",
    "\n",
    "    users_100k = users_1m_df[users_1m_df['user_id'].isin(ratings_df['user_id'])]\n",
    "    users_100k.to_csv(os.path.join(DATA_DIR, 'users.csv'), sep='\\t', index=False)\n",
    "\n",
    "    users_list = [i.strip().split(\"\\t\") for i in open(os.path.join(DATA_DIR,'users.csv'), 'r').readlines()]\n",
    "    users_df = pd.DataFrame(users_list[1:], columns=['user_id', 'gender', 'age', 'occupation', 'zip_code'])\n",
    "\n",
    "    return ratings_df, users_df, movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/tqkwxbbd1l96g8tr45l86pmw0000gn/T/ipykernel_44942/1118964766.py:37: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  movies_df[genre] = movies_df['genres'].str.contains(genre).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        1         1     4.0  964982703\n",
       "1        1         3     4.0  964981247\n",
       "2        1         6     4.0  964982224\n",
       "3        1        47     5.0  964983815\n",
       "4        1        50     5.0  964982931"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df, users_df, movies_df = load_data()\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id gender age occupation zip_code\n",
       "0       1      F   1         10    48067\n",
       "1       2      M  56         16    70072\n",
       "2       3      M  25         15    55117\n",
       "3       4      M  45          7    02460\n",
       "4       5      M  25         20    55455"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>(no genres listed)</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                               title  (no genres listed)  Action  \\\n",
       "0         1                    Toy Story (1995)                   0       0   \n",
       "1         2                      Jumanji (1995)                   0       0   \n",
       "2         3             Grumpier Old Men (1995)                   0       0   \n",
       "3         4            Waiting to Exhale (1995)                   0       0   \n",
       "4         5  Father of the Bride Part II (1995)                   0       0   \n",
       "\n",
       "   Adventure  Animation  Children  Comedy  Crime  Documentary  ...  Film-Noir  \\\n",
       "0          1          1         1       1      0            0  ...          0   \n",
       "1          1          0         1       0      0            0  ...          0   \n",
       "2          0          0         0       1      0            0  ...          0   \n",
       "3          0          0         0       1      0            0  ...          0   \n",
       "4          0          0         0       1      0            0  ...          0   \n",
       "\n",
       "   Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "0       0     0        0        0        0       0         0    0        0  \n",
       "1       0     0        0        0        0       0         0    0        0  \n",
       "2       0     0        0        0        1       0         0    0        0  \n",
       "3       0     0        0        0        1       0         0    0        0  \n",
       "4       0     0        0        0        0       0         0    0        0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RL env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MovieLensSimpleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A clean and stable MovieLens environment for Keras-RL.\n",
    "    - States: History of watched movie IDs (normalized once).\n",
    "    - Actions: Recommend a movie (by action index).\n",
    "    - Reward: +1 if user liked it (rating >= 4), else 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings_df, movies_df, users_df, state_size=10, max_steps=10):\n",
    "        super(MovieLensSimpleEnv, self).__init__()\n",
    "\n",
    "        self.ratings_df = ratings_df\n",
    "        self.movies_df = movies_df\n",
    "        self.users_df = users_df\n",
    "        self.state_size = state_size\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.user_ids = ratings_df['user_id'].unique()\n",
    "        self.movie_ids = movies_df['movie_id'].unique()\n",
    "        self.num_movies = len(self.movie_ids)\n",
    "\n",
    "        self.movie_id_to_idx = {mid: idx for idx, mid in enumerate(self.movie_ids)}\n",
    "        self.idx_to_movie_id = {idx: mid for idx, mid in enumerate(self.movie_ids)}\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.num_movies)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(self.state_size,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_user = random.choice(self.user_ids)\n",
    "        self.user_history = self.ratings_df[self.ratings_df['user_id'] == self.current_user]\n",
    "        self.liked_movies = set(\n",
    "            self.user_history[self.user_history['rating'] >= 4.0]['movie_id'].values\n",
    "        )\n",
    "        self.all_movies = set(self.user_history['movie_id'].values)\n",
    "\n",
    "        sampled_movies = np.random.choice(list(self.all_movies), size=self.state_size, replace=True)\n",
    "        self.state = np.array(\n",
    "            [self.movie_id_to_idx[mid] / self.num_movies for mid in sampled_movies],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.steps = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        movie_id = self.idx_to_movie_id[action]\n",
    "        \n",
    "        reward = 1 if movie_id in self.liked_movies else 0\n",
    "\n",
    "        # Move state window\n",
    "        new_state_value = action / self.num_movies\n",
    "        self.state = np.roll(self.state, -1)\n",
    "        self.state[-1] = new_state_value\n",
    "        \n",
    "        self.steps += 1\n",
    "        done = self.steps >= self.max_steps\n",
    "\n",
    "        # info = {'true items': [int(m) for m in self.liked_movies]}  # No problematic info dictionary\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"User {self.current_user}, State: {self.state}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# actions:  9742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.08499281, 0.15623075, 0.9135701 , 0.6784028 , 0.19903511,\n",
       "        0.9564771 , 0.19903511, 0.06312872, 0.06292342, 0.9097721 ],\n",
       "       dtype=float32),\n",
       " 0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create environment\n",
    "env = MovieLensSimpleEnv(ratings_df, movies_df, users_df)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "print('# actions: ', nb_actions)\n",
    "\n",
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_dqn_model(state_size, num_movies, nb_actions):\n",
    "    \"\"\"\n",
    "    Build a stable DQN model for movie recommendation.\n",
    "    \"\"\"\n",
    "    embedding_dim = 32\n",
    "\n",
    "    movie_input = Input(shape=(1, state_size), dtype='float32')  # <- Notice (1, state_size)\n",
    "\n",
    "    # Flatten first to remove window_length dimension\n",
    "    flat_input = Flatten()(movie_input)\n",
    "\n",
    "    discrete_movie_indices = flat_input * num_movies\n",
    "    discrete_movie_indices = tf.cast(discrete_movie_indices, tf.int32)\n",
    "\n",
    "    embeddings = Embedding(input_dim=num_movies, output_dim=embedding_dim)(discrete_movie_indices)\n",
    "    x = Flatten()(embeddings)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(nb_actions, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=movie_input, outputs=output)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:37:58.432927: W tensorflow/c/c_api.cc:291] Operation '{name:'embedding_14/embeddings/Assign' id:13613 op device:{requested: '', assigned: ''} def:{{{node embedding_14/embeddings/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](embedding_14/embeddings, embedding_14/embeddings/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Build model\n",
    "model = build_dqn_model(\n",
    "    state_size=env.state_size,\n",
    "    num_movies=len(env.movie_ids),\n",
    "    nb_actions=len(env.movie_ids)\n",
    ")\n",
    "\n",
    "# Configure agent\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=20000)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=len(env.movie_ids),\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=500,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy,\n",
    "    enable_double_dqn=True\n",
    ")\n",
    "\n",
    "# Compile agent\n",
    "dqn.compile(optimizer=Adam(learning_rate=1e-4), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "   10/10000: episode: 1, duration: 0.027s, episode steps:  10, steps per second: 364, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4574.000 [1030.000, 8837.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20/10000: episode: 2, duration: 0.022s, episode steps:  10, steps per second: 457, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3416.100 [610.000, 7431.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30/10000: episode: 3, duration: 0.020s, episode steps:  10, steps per second: 507, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4153.200 [1155.000, 7621.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40/10000: episode: 4, duration: 0.019s, episode steps:  10, steps per second: 523, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4812.200 [801.000, 8044.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   50/10000: episode: 5, duration: 0.019s, episode steps:  10, steps per second: 514, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6106.100 [2476.000, 9409.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   60/10000: episode: 6, duration: 0.019s, episode steps:  10, steps per second: 519, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3677.100 [6.000, 6901.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   70/10000: episode: 7, duration: 0.018s, episode steps:  10, steps per second: 559, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4510.200 [690.000, 9495.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   80/10000: episode: 8, duration: 0.018s, episode steps:  10, steps per second: 563, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6994.400 [1295.000, 9283.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   90/10000: episode: 9, duration: 0.017s, episode steps:  10, steps per second: 574, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5038.200 [1333.000, 8970.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  100/10000: episode: 10, duration: 0.017s, episode steps:  10, steps per second: 581, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5888.900 [1368.000, 9310.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  110/10000: episode: 11, duration: 0.020s, episode steps:  10, steps per second: 490, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5482.400 [711.000, 9128.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  120/10000: episode: 12, duration: 0.020s, episode steps:  10, steps per second: 498, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5560.600 [328.000, 8170.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  130/10000: episode: 13, duration: 0.017s, episode steps:  10, steps per second: 591, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5741.900 [1085.000, 8894.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  140/10000: episode: 14, duration: 0.016s, episode steps:  10, steps per second: 626, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5385.500 [1388.000, 9684.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  150/10000: episode: 15, duration: 0.017s, episode steps:  10, steps per second: 603, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5586.300 [1557.000, 9259.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  160/10000: episode: 16, duration: 0.016s, episode steps:  10, steps per second: 639, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5494.600 [521.000, 9453.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  170/10000: episode: 17, duration: 0.017s, episode steps:  10, steps per second: 588, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4095.700 [117.000, 9072.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  180/10000: episode: 18, duration: 0.016s, episode steps:  10, steps per second: 620, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4855.600 [227.000, 9011.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  190/10000: episode: 19, duration: 0.032s, episode steps:  10, steps per second: 308, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5024.800 [879.000, 8737.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  200/10000: episode: 20, duration: 0.016s, episode steps:  10, steps per second: 640, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4267.200 [111.000, 9552.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  210/10000: episode: 21, duration: 0.015s, episode steps:  10, steps per second: 659, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4299.700 [418.000, 8588.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  220/10000: episode: 22, duration: 0.015s, episode steps:  10, steps per second: 647, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4552.700 [434.000, 8227.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  230/10000: episode: 23, duration: 0.017s, episode steps:  10, steps per second: 573, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4212.500 [602.000, 9447.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  240/10000: episode: 24, duration: 0.019s, episode steps:  10, steps per second: 535, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6112.000 [1016.000, 9706.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  250/10000: episode: 25, duration: 0.019s, episode steps:  10, steps per second: 539, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5713.900 [678.000, 8776.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  260/10000: episode: 26, duration: 0.016s, episode steps:  10, steps per second: 620, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4543.900 [1310.000, 8134.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  270/10000: episode: 27, duration: 0.015s, episode steps:  10, steps per second: 646, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5830.800 [1905.000, 9571.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  280/10000: episode: 28, duration: 0.016s, episode steps:  10, steps per second: 617, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4953.500 [109.000, 9213.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  290/10000: episode: 29, duration: 0.017s, episode steps:  10, steps per second: 597, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3728.200 [636.000, 9120.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  300/10000: episode: 30, duration: 0.017s, episode steps:  10, steps per second: 591, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4543.100 [358.000, 9522.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  310/10000: episode: 31, duration: 0.016s, episode steps:  10, steps per second: 623, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5594.800 [77.000, 8696.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  320/10000: episode: 32, duration: 0.017s, episode steps:  10, steps per second: 591, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5582.300 [1547.000, 9554.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  330/10000: episode: 33, duration: 0.016s, episode steps:  10, steps per second: 634, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3608.200 [337.000, 8311.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  340/10000: episode: 34, duration: 0.014s, episode steps:  10, steps per second: 699, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3439.200 [337.000, 9141.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  350/10000: episode: 35, duration: 0.017s, episode steps:  10, steps per second: 575, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4954.700 [213.000, 9721.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  360/10000: episode: 36, duration: 0.020s, episode steps:  10, steps per second: 507, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4172.400 [122.000, 8509.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  370/10000: episode: 37, duration: 0.018s, episode steps:  10, steps per second: 552, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5856.300 [1767.000, 9346.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  380/10000: episode: 38, duration: 0.018s, episode steps:  10, steps per second: 565, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5228.000 [337.000, 9078.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  390/10000: episode: 39, duration: 0.018s, episode steps:  10, steps per second: 562, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5146.400 [267.000, 9740.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  400/10000: episode: 40, duration: 0.018s, episode steps:  10, steps per second: 555, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5258.600 [8.000, 8683.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  410/10000: episode: 41, duration: 0.017s, episode steps:  10, steps per second: 576, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6182.500 [1699.000, 9283.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  420/10000: episode: 42, duration: 0.019s, episode steps:  10, steps per second: 520, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5709.400 [1147.000, 9570.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  430/10000: episode: 43, duration: 0.017s, episode steps:  10, steps per second: 579, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4198.500 [719.000, 9582.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  440/10000: episode: 44, duration: 0.015s, episode steps:  10, steps per second: 653, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4287.000 [477.000, 8069.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  450/10000: episode: 45, duration: 0.015s, episode steps:  10, steps per second: 671, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4903.800 [995.000, 8520.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  460/10000: episode: 46, duration: 0.015s, episode steps:  10, steps per second: 654, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5008.700 [71.000, 7756.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  470/10000: episode: 47, duration: 0.016s, episode steps:  10, steps per second: 621, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5057.300 [337.000, 8898.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  480/10000: episode: 48, duration: 0.018s, episode steps:  10, steps per second: 567, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 2983.900 [745.000, 7368.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  490/10000: episode: 49, duration: 0.014s, episode steps:  10, steps per second: 691, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3970.800 [384.000, 7799.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  500/10000: episode: 50, duration: 0.015s, episode steps:  10, steps per second: 661, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5851.800 [558.000, 9330.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  510/10000: episode: 51, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4928.300 [749.000, 9267.000],  loss: 0.002435, mae: 0.013396, mean_q: 0.129411, mean_eps: 0.977275\n",
      "  520/10000: episode: 52, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5859.200 [121.000, 9702.000],  loss: 0.001618, mae: 0.013850, mean_q: 0.133749, mean_eps: 0.976847\n",
      "  530/10000: episode: 53, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4839.800 [1478.000, 9246.000],  loss: 0.005942, mae: 0.014387, mean_q: 0.143658, mean_eps: 0.976398\n",
      "  540/10000: episode: 54, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3556.000 [959.000, 9208.000],  loss: 0.004265, mae: 0.015188, mean_q: 0.160742, mean_eps: 0.975947\n",
      "  550/10000: episode: 55, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3556.500 [52.000, 8114.000],  loss: 0.001056, mae: 0.015613, mean_q: 0.173600, mean_eps: 0.975497\n",
      "  560/10000: episode: 56, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5692.000 [2320.000, 9714.000],  loss: 0.002354, mae: 0.016262, mean_q: 0.181528, mean_eps: 0.975047\n",
      "  570/10000: episode: 57, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 5180.700 [959.000, 9663.000],  loss: 0.004286, mae: 0.017530, mean_q: 0.195789, mean_eps: 0.974598\n",
      "  580/10000: episode: 58, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4679.900 [1998.000, 9678.000],  loss: 0.006255, mae: 0.018430, mean_q: 0.205671, mean_eps: 0.974148\n",
      "  590/10000: episode: 59, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6267.700 [2556.000, 9084.000],  loss: 0.004964, mae: 0.019552, mean_q: 0.218807, mean_eps: 0.973697\n",
      "  600/10000: episode: 60, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3581.100 [706.000, 7324.000],  loss: 0.003609, mae: 0.021953, mean_q: 0.253875, mean_eps: 0.973248\n",
      "  610/10000: episode: 61, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3545.000 [731.000, 6337.000],  loss: 0.002606, mae: 0.023743, mean_q: 0.279678, mean_eps: 0.972798\n",
      "  620/10000: episode: 62, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6088.200 [2436.000, 9451.000],  loss: 0.001844, mae: 0.025925, mean_q: 0.301379, mean_eps: 0.972347\n",
      "  630/10000: episode: 63, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3800.300 [690.000, 7496.000],  loss: 0.003663, mae: 0.028442, mean_q: 0.323911, mean_eps: 0.971898\n",
      "  640/10000: episode: 64, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5511.100 [2502.000, 8520.000],  loss: 0.002787, mae: 0.031050, mean_q: 0.344189, mean_eps: 0.971448\n",
      "  650/10000: episode: 65, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4200.600 [366.000, 8345.000],  loss: 0.002757, mae: 0.033504, mean_q: 0.364790, mean_eps: 0.970998\n",
      "  660/10000: episode: 66, duration: 0.401s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6324.600 [959.000, 9591.000],  loss: 0.003841, mae: 0.037214, mean_q: 0.399159, mean_eps: 0.970548\n",
      "  670/10000: episode: 67, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4288.000 [926.000, 9239.000],  loss: 0.003567, mae: 0.040417, mean_q: 0.438571, mean_eps: 0.970097\n",
      "  680/10000: episode: 68, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4687.600 [389.000, 9186.000],  loss: 0.004638, mae: 0.043817, mean_q: 0.492981, mean_eps: 0.969648\n",
      "  690/10000: episode: 69, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4506.900 [365.000, 9290.000],  loss: 0.004419, mae: 0.047671, mean_q: 0.538459, mean_eps: 0.969197\n",
      "  700/10000: episode: 70, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5126.300 [959.000, 9165.000],  loss: 0.005150, mae: 0.052886, mean_q: 0.590020, mean_eps: 0.968748\n",
      "  710/10000: episode: 71, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5021.900 [11.000, 9586.000],  loss: 0.009478, mae: 0.057615, mean_q: 0.622258, mean_eps: 0.968298\n",
      "  720/10000: episode: 72, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5026.400 [2432.000, 8177.000],  loss: 0.005295, mae: 0.063416, mean_q: 0.660887, mean_eps: 0.967847\n",
      "  730/10000: episode: 73, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4610.700 [970.000, 8248.000],  loss: 0.008309, mae: 0.071633, mean_q: 0.742214, mean_eps: 0.967398\n",
      "  740/10000: episode: 74, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6269.400 [1834.000, 9704.000],  loss: 0.006363, mae: 0.077245, mean_q: 0.786469, mean_eps: 0.966947\n",
      "  750/10000: episode: 75, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5673.300 [2133.000, 9215.000],  loss: 0.009362, mae: 0.083929, mean_q: 0.843690, mean_eps: 0.966498\n",
      "  760/10000: episode: 76, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5132.100 [1144.000, 8646.000],  loss: 0.012690, mae: 0.092447, mean_q: 0.912519, mean_eps: 0.966047\n",
      "  770/10000: episode: 77, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3594.300 [414.000, 9532.000],  loss: 0.010570, mae: 0.101866, mean_q: 0.984951, mean_eps: 0.965597\n",
      "  780/10000: episode: 78, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5220.800 [1071.000, 9675.000],  loss: 0.012380, mae: 0.113921, mean_q: 1.069513, mean_eps: 0.965147\n",
      "  790/10000: episode: 79, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6006.800 [786.000, 9410.000],  loss: 0.022183, mae: 0.119282, mean_q: 1.097574, mean_eps: 0.964697\n",
      "  800/10000: episode: 80, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3305.000 [629.000, 7981.000],  loss: 0.013943, mae: 0.129294, mean_q: 1.146726, mean_eps: 0.964248\n",
      "  810/10000: episode: 81, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4271.800 [959.000, 9260.000],  loss: 0.017406, mae: 0.141050, mean_q: 1.230948, mean_eps: 0.963797\n",
      "  820/10000: episode: 82, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4619.100 [1651.000, 8940.000],  loss: 0.014980, mae: 0.151776, mean_q: 1.317497, mean_eps: 0.963348\n",
      "  830/10000: episode: 83, duration: 0.329s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4366.800 [1329.000, 9520.000],  loss: 0.021406, mae: 0.163467, mean_q: 1.387931, mean_eps: 0.962898\n",
      "  840/10000: episode: 84, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3675.800 [1299.000, 6249.000],  loss: 0.021689, mae: 0.172610, mean_q: 1.448580, mean_eps: 0.962447\n",
      "  850/10000: episode: 85, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5196.000 [357.000, 9461.000],  loss: 0.025233, mae: 0.184264, mean_q: 1.474529, mean_eps: 0.961998\n",
      "  860/10000: episode: 86, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5091.200 [959.000, 9485.000],  loss: 0.025895, mae: 0.199203, mean_q: 1.490637, mean_eps: 0.961547\n",
      "  870/10000: episode: 87, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5090.600 [1626.000, 9456.000],  loss: 0.024956, mae: 0.193056, mean_q: 1.419161, mean_eps: 0.961098\n",
      "  880/10000: episode: 88, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5587.000 [485.000, 9657.000],  loss: 0.022394, mae: 0.184213, mean_q: 1.346424, mean_eps: 0.960647\n",
      "  890/10000: episode: 89, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3669.600 [6.000, 8147.000],  loss: 0.036610, mae: 0.189617, mean_q: 1.459718, mean_eps: 0.960197\n",
      "  900/10000: episode: 90, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3926.400 [778.000, 9715.000],  loss: 0.032741, mae: 0.218790, mean_q: 1.731668, mean_eps: 0.959748\n",
      "  910/10000: episode: 91, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5153.500 [1130.000, 9664.000],  loss: 0.025970, mae: 0.240940, mean_q: 1.751011, mean_eps: 0.959297\n",
      "  920/10000: episode: 92, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6033.900 [2008.000, 9024.000],  loss: 0.034523, mae: 0.239007, mean_q: 1.694461, mean_eps: 0.958848\n",
      "  930/10000: episode: 93, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4781.900 [1608.000, 7837.000],  loss: 0.028180, mae: 0.225573, mean_q: 1.601697, mean_eps: 0.958398\n",
      "  940/10000: episode: 94, duration: 0.248s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4402.600 [624.000, 8715.000],  loss: 0.031131, mae: 0.238371, mean_q: 1.685074, mean_eps: 0.957947\n",
      "  950/10000: episode: 95, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5412.300 [1266.000, 9523.000],  loss: 0.038459, mae: 0.252619, mean_q: 1.771908, mean_eps: 0.957498\n",
      "  960/10000: episode: 96, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4374.300 [383.000, 7958.000],  loss: 0.031979, mae: 0.266250, mean_q: 1.888507, mean_eps: 0.957048\n",
      "  970/10000: episode: 97, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6688.700 [2453.000, 9449.000],  loss: 0.040389, mae: 0.283648, mean_q: 2.063612, mean_eps: 0.956598\n",
      "  980/10000: episode: 98, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5668.100 [1851.000, 9358.000],  loss: 0.056061, mae: 0.299324, mean_q: 2.191858, mean_eps: 0.956147\n",
      "  990/10000: episode: 99, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2992.600 [10.000, 8891.000],  loss: 0.041095, mae: 0.306317, mean_q: 2.240486, mean_eps: 0.955697\n",
      " 1000/10000: episode: 100, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3639.000 [1026.000, 8207.000],  loss: 0.048679, mae: 0.330600, mean_q: 2.405395, mean_eps: 0.955247\n",
      " 1010/10000: episode: 101, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5136.400 [191.000, 9495.000],  loss: 0.063742, mae: 0.345520, mean_q: 2.521289, mean_eps: 0.954797\n",
      " 1020/10000: episode: 102, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6634.500 [2371.000, 8783.000],  loss: 0.088692, mae: 0.362839, mean_q: 2.682841, mean_eps: 0.954348\n",
      " 1030/10000: episode: 103, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6254.200 [1362.000, 8906.000],  loss: 0.058146, mae: 0.389332, mean_q: 2.880770, mean_eps: 0.953897\n",
      " 1040/10000: episode: 104, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3864.100 [112.000, 9461.000],  loss: 0.071477, mae: 0.415383, mean_q: 3.060320, mean_eps: 0.953448\n",
      " 1050/10000: episode: 105, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3471.800 [203.000, 9397.000],  loss: 0.087972, mae: 0.435242, mean_q: 3.192565, mean_eps: 0.952998\n",
      " 1060/10000: episode: 106, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3446.700 [846.000, 7979.000],  loss: 0.106259, mae: 0.463751, mean_q: 3.311982, mean_eps: 0.952547\n",
      " 1070/10000: episode: 107, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3940.400 [520.000, 9138.000],  loss: 0.129378, mae: 0.478780, mean_q: 2.934958, mean_eps: 0.952098\n",
      " 1080/10000: episode: 108, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4316.600 [178.000, 9167.000],  loss: 0.091064, mae: 0.485130, mean_q: 2.787715, mean_eps: 0.951647\n",
      " 1090/10000: episode: 109, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4522.900 [508.000, 9380.000],  loss: 0.095261, mae: 0.464911, mean_q: 2.695725, mean_eps: 0.951198\n",
      " 1100/10000: episode: 110, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6254.800 [2581.000, 9683.000],  loss: 0.101652, mae: 0.474379, mean_q: 2.813095, mean_eps: 0.950747\n",
      " 1110/10000: episode: 111, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5223.300 [515.000, 9514.000],  loss: 0.111844, mae: 0.503960, mean_q: 3.006716, mean_eps: 0.950297\n",
      " 1120/10000: episode: 112, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4234.300 [327.000, 9021.000],  loss: 0.115475, mae: 0.522624, mean_q: 3.068574, mean_eps: 0.949848\n",
      " 1130/10000: episode: 113, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4488.700 [666.000, 8731.000],  loss: 0.138681, mae: 0.551057, mean_q: 3.192714, mean_eps: 0.949397\n",
      " 1140/10000: episode: 114, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6346.400 [2494.000, 8431.000],  loss: 0.101285, mae: 0.530640, mean_q: 3.058477, mean_eps: 0.948948\n",
      " 1150/10000: episode: 115, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4412.900 [600.000, 7801.000],  loss: 0.113570, mae: 0.529121, mean_q: 3.049548, mean_eps: 0.948497\n",
      " 1160/10000: episode: 116, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5381.300 [434.000, 9276.000],  loss: 0.119662, mae: 0.509806, mean_q: 2.949714, mean_eps: 0.948047\n",
      " 1170/10000: episode: 117, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6283.900 [618.000, 9284.000],  loss: 0.125186, mae: 0.508251, mean_q: 2.944315, mean_eps: 0.947597\n",
      " 1180/10000: episode: 118, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3814.400 [603.000, 8405.000],  loss: 0.091123, mae: 0.521800, mean_q: 2.996700, mean_eps: 0.947148\n",
      " 1190/10000: episode: 119, duration: 0.282s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5518.300 [651.000, 9541.000],  loss: 0.098515, mae: 0.537077, mean_q: 3.066249, mean_eps: 0.946698\n",
      " 1200/10000: episode: 120, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3769.500 [114.000, 8692.000],  loss: 0.114034, mae: 0.555178, mean_q: 3.162670, mean_eps: 0.946247\n",
      " 1210/10000: episode: 121, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5342.800 [2159.000, 8956.000],  loss: 0.125944, mae: 0.584225, mean_q: 3.317258, mean_eps: 0.945798\n",
      " 1220/10000: episode: 122, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4930.500 [1469.000, 8861.000],  loss: 0.128207, mae: 0.604753, mean_q: 3.423805, mean_eps: 0.945348\n",
      " 1230/10000: episode: 123, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4146.500 [1430.000, 8511.000],  loss: 0.100745, mae: 0.613399, mean_q: 3.413255, mean_eps: 0.944897\n",
      " 1240/10000: episode: 124, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3301.200 [146.000, 9469.000],  loss: 0.110969, mae: 0.636010, mean_q: 3.432476, mean_eps: 0.944448\n",
      " 1250/10000: episode: 125, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5936.100 [509.000, 9379.000],  loss: 0.129756, mae: 0.630804, mean_q: 3.453713, mean_eps: 0.943998\n",
      " 1260/10000: episode: 126, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4485.200 [492.000, 9007.000],  loss: 0.207384, mae: 0.643106, mean_q: 3.543187, mean_eps: 0.943547\n",
      " 1270/10000: episode: 127, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4343.300 [813.000, 9329.000],  loss: 0.132506, mae: 0.658396, mean_q: 3.831716, mean_eps: 0.943098\n",
      " 1280/10000: episode: 128, duration: 0.312s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7228.900 [2332.000, 9450.000],  loss: 0.160581, mae: 0.689895, mean_q: 4.168362, mean_eps: 0.942647\n",
      " 1290/10000: episode: 129, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5513.300 [64.000, 9626.000],  loss: 0.130357, mae: 0.726028, mean_q: 4.429742, mean_eps: 0.942198\n",
      " 1300/10000: episode: 130, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4730.000 [173.000, 9652.000],  loss: 0.124636, mae: 0.766884, mean_q: 4.680837, mean_eps: 0.941747\n",
      " 1310/10000: episode: 131, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5986.000 [2043.000, 9345.000],  loss: 0.192384, mae: 0.762981, mean_q: 4.650453, mean_eps: 0.941298\n",
      " 1320/10000: episode: 132, duration: 0.253s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4161.600 [195.000, 8392.000],  loss: 0.263833, mae: 0.797573, mean_q: 4.490321, mean_eps: 0.940847\n",
      " 1330/10000: episode: 133, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4796.200 [390.000, 7398.000],  loss: 0.172541, mae: 0.781413, mean_q: 4.197789, mean_eps: 0.940397\n",
      " 1340/10000: episode: 134, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5376.100 [962.000, 9483.000],  loss: 0.240103, mae: 0.725260, mean_q: 3.910430, mean_eps: 0.939948\n",
      " 1350/10000: episode: 135, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4648.300 [356.000, 9485.000],  loss: 0.204899, mae: 0.689306, mean_q: 3.699642, mean_eps: 0.939497\n",
      " 1360/10000: episode: 136, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4229.000 [384.000, 9095.000],  loss: 0.250199, mae: 0.766038, mean_q: 4.122907, mean_eps: 0.939048\n",
      " 1370/10000: episode: 137, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4361.100 [574.000, 8726.000],  loss: 0.140194, mae: 0.733344, mean_q: 3.994657, mean_eps: 0.938597\n",
      " 1380/10000: episode: 138, duration: 0.318s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5474.100 [384.000, 9330.000],  loss: 0.184257, mae: 0.776053, mean_q: 4.242085, mean_eps: 0.938147\n",
      " 1390/10000: episode: 139, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5390.200 [525.000, 9472.000],  loss: 0.180740, mae: 0.774235, mean_q: 4.208040, mean_eps: 0.937697\n",
      " 1400/10000: episode: 140, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4899.700 [2200.000, 9101.000],  loss: 0.180285, mae: 0.786801, mean_q: 4.253951, mean_eps: 0.937248\n",
      " 1410/10000: episode: 141, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3876.000 [604.000, 9339.000],  loss: 0.159198, mae: 0.831415, mean_q: 4.480165, mean_eps: 0.936798\n",
      " 1420/10000: episode: 142, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4684.500 [680.000, 8560.000],  loss: 0.225800, mae: 0.863839, mean_q: 4.641507, mean_eps: 0.936347\n",
      " 1430/10000: episode: 143, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6530.100 [1869.000, 9277.000],  loss: 0.240877, mae: 0.888057, mean_q: 4.760549, mean_eps: 0.935898\n",
      " 1440/10000: episode: 144, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5734.100 [3175.000, 9410.000],  loss: 0.228574, mae: 0.906852, mean_q: 4.846126, mean_eps: 0.935448\n",
      " 1450/10000: episode: 145, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5392.600 [760.000, 8888.000],  loss: 0.270868, mae: 0.931554, mean_q: 4.927835, mean_eps: 0.934997\n",
      " 1460/10000: episode: 146, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5658.500 [445.000, 9616.000],  loss: 0.241889, mae: 0.886928, mean_q: 4.612524, mean_eps: 0.934548\n",
      " 1470/10000: episode: 147, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5177.700 [1037.000, 8967.000],  loss: 0.258292, mae: 0.903844, mean_q: 4.727395, mean_eps: 0.934098\n",
      " 1480/10000: episode: 148, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4960.800 [384.000, 9680.000],  loss: 0.173354, mae: 0.867274, mean_q: 4.461196, mean_eps: 0.933648\n",
      " 1490/10000: episode: 149, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5257.200 [132.000, 9592.000],  loss: 0.200004, mae: 0.824360, mean_q: 4.256627, mean_eps: 0.933198\n",
      " 1500/10000: episode: 150, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5186.100 [505.000, 7869.000],  loss: 0.547982, mae: 0.823607, mean_q: 4.296244, mean_eps: 0.932747\n",
      " 1510/10000: episode: 151, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5326.900 [950.000, 9141.000],  loss: 0.273979, mae: 0.904486, mean_q: 4.973829, mean_eps: 0.932298\n",
      " 1520/10000: episode: 152, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5404.500 [530.000, 8939.000],  loss: 0.232795, mae: 0.981236, mean_q: 5.336720, mean_eps: 0.931847\n",
      " 1530/10000: episode: 153, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5822.100 [439.000, 8560.000],  loss: 0.375393, mae: 1.010448, mean_q: 5.336518, mean_eps: 0.931398\n",
      " 1540/10000: episode: 154, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5111.500 [1373.000, 8092.000],  loss: 0.335997, mae: 0.987834, mean_q: 5.202326, mean_eps: 0.930948\n",
      " 1550/10000: episode: 155, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3929.500 [577.000, 9276.000],  loss: 0.363315, mae: 1.088009, mean_q: 5.658983, mean_eps: 0.930497\n",
      " 1560/10000: episode: 156, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5997.600 [521.000, 9627.000],  loss: 0.306348, mae: 1.068801, mean_q: 5.517081, mean_eps: 0.930048\n",
      " 1570/10000: episode: 157, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5594.700 [2179.000, 8931.000],  loss: 0.305236, mae: 1.070401, mean_q: 5.525418, mean_eps: 0.929597\n",
      " 1580/10000: episode: 158, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4715.600 [1029.000, 9001.000],  loss: 0.211776, mae: 1.087051, mean_q: 5.602597, mean_eps: 0.929148\n",
      " 1590/10000: episode: 159, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5132.800 [184.000, 9418.000],  loss: 0.389617, mae: 1.140197, mean_q: 5.859381, mean_eps: 0.928697\n",
      " 1600/10000: episode: 160, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4816.500 [223.000, 9148.000],  loss: 0.385824, mae: 1.130330, mean_q: 5.774774, mean_eps: 0.928247\n",
      " 1610/10000: episode: 161, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4114.700 [887.000, 7857.000],  loss: 0.503927, mae: 1.128161, mean_q: 5.932069, mean_eps: 0.927797\n",
      " 1620/10000: episode: 162, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4270.800 [45.000, 7863.000],  loss: 0.227779, mae: 1.094890, mean_q: 5.906551, mean_eps: 0.927347\n",
      " 1630/10000: episode: 163, duration: 0.348s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3611.300 [677.000, 7359.000],  loss: 0.487169, mae: 1.105472, mean_q: 5.943204, mean_eps: 0.926898\n",
      " 1640/10000: episode: 164, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6802.200 [1200.000, 9648.000],  loss: 0.353644, mae: 1.143218, mean_q: 5.942622, mean_eps: 0.926447\n",
      " 1650/10000: episode: 165, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5858.900 [580.000, 9703.000],  loss: 0.324596, mae: 1.160971, mean_q: 5.900158, mean_eps: 0.925998\n",
      " 1660/10000: episode: 166, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5712.500 [408.000, 9464.000],  loss: 0.300298, mae: 1.242688, mean_q: 6.242767, mean_eps: 0.925548\n",
      " 1670/10000: episode: 167, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4222.400 [1605.000, 7646.000],  loss: 0.368688, mae: 1.217143, mean_q: 5.978167, mean_eps: 0.925097\n",
      " 1680/10000: episode: 168, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2862.000 [382.000, 7674.000],  loss: 0.464128, mae: 1.252366, mean_q: 6.116651, mean_eps: 0.924648\n",
      " 1690/10000: episode: 169, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4959.000 [2255.000, 9588.000],  loss: 0.693427, mae: 1.278338, mean_q: 6.217497, mean_eps: 0.924198\n",
      " 1700/10000: episode: 170, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5693.100 [762.000, 9657.000],  loss: 0.562940, mae: 1.323803, mean_q: 6.414520, mean_eps: 0.923748\n",
      " 1710/10000: episode: 171, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6524.500 [2549.000, 9735.000],  loss: 0.576600, mae: 1.226578, mean_q: 6.000287, mean_eps: 0.923297\n",
      " 1720/10000: episode: 172, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3533.900 [321.000, 8789.000],  loss: 0.529605, mae: 1.229055, mean_q: 6.023411, mean_eps: 0.922847\n",
      " 1730/10000: episode: 173, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5246.500 [1776.000, 9521.000],  loss: 0.551648, mae: 1.297899, mean_q: 6.342626, mean_eps: 0.922398\n",
      " 1740/10000: episode: 174, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5299.600 [176.000, 9193.000],  loss: 0.513409, mae: 1.327702, mean_q: 6.468499, mean_eps: 0.921947\n",
      " 1750/10000: episode: 175, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4940.700 [581.000, 9229.000],  loss: 0.380537, mae: 1.251713, mean_q: 6.013518, mean_eps: 0.921498\n",
      " 1760/10000: episode: 176, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5866.600 [1455.000, 9574.000],  loss: 0.620433, mae: 1.415439, mean_q: 6.746080, mean_eps: 0.921047\n",
      " 1770/10000: episode: 177, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4213.900 [1322.000, 8424.000],  loss: 0.436585, mae: 1.328996, mean_q: 6.323481, mean_eps: 0.920597\n",
      " 1780/10000: episode: 178, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5259.800 [201.000, 9671.000],  loss: 0.453885, mae: 1.374197, mean_q: 6.560927, mean_eps: 0.920148\n",
      " 1790/10000: episode: 179, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3567.400 [259.000, 9502.000],  loss: 0.279010, mae: 1.341713, mean_q: 6.420801, mean_eps: 0.919697\n",
      " 1800/10000: episode: 180, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4597.200 [361.000, 7937.000],  loss: 0.454487, mae: 1.374285, mean_q: 6.572546, mean_eps: 0.919248\n",
      " 1810/10000: episode: 181, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4344.100 [201.000, 7978.000],  loss: 0.466765, mae: 1.464162, mean_q: 7.001430, mean_eps: 0.918797\n",
      " 1820/10000: episode: 182, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5772.500 [1834.000, 8726.000],  loss: 0.669306, mae: 1.478987, mean_q: 7.053049, mean_eps: 0.918347\n",
      " 1830/10000: episode: 183, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6181.400 [1364.000, 9645.000],  loss: 0.661453, mae: 1.440602, mean_q: 6.854626, mean_eps: 0.917897\n",
      " 1840/10000: episode: 184, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4998.800 [1705.000, 9566.000],  loss: 0.399014, mae: 1.442330, mean_q: 6.868908, mean_eps: 0.917447\n",
      " 1850/10000: episode: 185, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4075.800 [229.000, 8552.000],  loss: 0.783880, mae: 1.552621, mean_q: 7.380374, mean_eps: 0.916998\n",
      " 1860/10000: episode: 186, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6051.000 [381.000, 9430.000],  loss: 0.509610, mae: 1.564937, mean_q: 7.423004, mean_eps: 0.916547\n",
      " 1870/10000: episode: 187, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 5428.300 [1505.000, 9507.000],  loss: 0.919437, mae: 1.552324, mean_q: 7.322785, mean_eps: 0.916098\n",
      " 1880/10000: episode: 188, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5230.800 [114.000, 9569.000],  loss: 0.545790, mae: 1.568224, mean_q: 7.254042, mean_eps: 0.915648\n",
      " 1890/10000: episode: 189, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5284.900 [2050.000, 7466.000],  loss: 0.970701, mae: 1.647825, mean_q: 7.638033, mean_eps: 0.915197\n",
      " 1900/10000: episode: 190, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3799.900 [443.000, 9455.000],  loss: 0.771835, mae: 1.601152, mean_q: 7.463162, mean_eps: 0.914748\n",
      " 1910/10000: episode: 191, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4636.500 [283.000, 8766.000],  loss: 0.796520, mae: 1.637558, mean_q: 8.442783, mean_eps: 0.914297\n",
      " 1920/10000: episode: 192, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5849.500 [960.000, 9675.000],  loss: 0.437358, mae: 1.655834, mean_q: 8.931949, mean_eps: 0.913848\n",
      " 1930/10000: episode: 193, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3991.200 [510.000, 9408.000],  loss: 0.963622, mae: 1.763747, mean_q: 9.608285, mean_eps: 0.913397\n",
      " 1940/10000: episode: 194, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5021.900 [2180.000, 9090.000],  loss: 1.002467, mae: 1.802309, mean_q: 9.821106, mean_eps: 0.912947\n",
      " 1950/10000: episode: 195, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3284.600 [314.000, 9322.000],  loss: 0.726679, mae: 1.900499, mean_q: 10.332971, mean_eps: 0.912498\n",
      " 1960/10000: episode: 196, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5216.700 [510.000, 8623.000],  loss: 1.185146, mae: 1.895413, mean_q: 10.259060, mean_eps: 0.912047\n",
      " 1970/10000: episode: 197, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3777.600 [510.000, 7474.000],  loss: 1.258030, mae: 2.020563, mean_q: 10.607222, mean_eps: 0.911598\n",
      " 1980/10000: episode: 198, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4385.000 [276.000, 8552.000],  loss: 1.251663, mae: 1.990490, mean_q: 9.435380, mean_eps: 0.911147\n",
      " 1990/10000: episode: 199, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5251.800 [210.000, 9344.000],  loss: 1.053055, mae: 1.951157, mean_q: 9.296757, mean_eps: 0.910697\n",
      " 2000/10000: episode: 200, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5386.700 [770.000, 9194.000],  loss: 1.239225, mae: 1.936652, mean_q: 9.263411, mean_eps: 0.910247\n",
      " 2010/10000: episode: 201, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5571.100 [657.000, 9625.000],  loss: 0.733645, mae: 1.960067, mean_q: 9.385276, mean_eps: 0.909798\n",
      " 2020/10000: episode: 202, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4794.100 [1213.000, 7888.000],  loss: 1.642363, mae: 1.898161, mean_q: 9.135018, mean_eps: 0.909348\n",
      " 2030/10000: episode: 203, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4973.200 [1735.000, 7963.000],  loss: 1.625715, mae: 1.940005, mean_q: 9.339207, mean_eps: 0.908897\n",
      " 2040/10000: episode: 204, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5249.500 [1323.000, 9727.000],  loss: 1.182197, mae: 2.029842, mean_q: 9.795200, mean_eps: 0.908448\n",
      " 2050/10000: episode: 205, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3819.800 [112.000, 7265.000],  loss: 1.268404, mae: 2.067991, mean_q: 9.734360, mean_eps: 0.907998\n",
      " 2060/10000: episode: 206, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3991.600 [18.000, 9698.000],  loss: 1.187087, mae: 2.062853, mean_q: 9.486247, mean_eps: 0.907547\n",
      " 2070/10000: episode: 207, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4853.900 [1642.000, 8770.000],  loss: 0.968041, mae: 2.119290, mean_q: 9.708830, mean_eps: 0.907098\n",
      " 2080/10000: episode: 208, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5561.300 [231.000, 9256.000],  loss: 1.084791, mae: 2.173226, mean_q: 9.947704, mean_eps: 0.906647\n",
      " 2090/10000: episode: 209, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4859.400 [689.000, 9661.000],  loss: 1.208164, mae: 2.241398, mean_q: 10.240099, mean_eps: 0.906197\n",
      " 2100/10000: episode: 210, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3390.600 [100.000, 6865.000],  loss: 1.272530, mae: 2.268074, mean_q: 10.341977, mean_eps: 0.905748\n",
      " 2110/10000: episode: 211, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5902.800 [3811.000, 8636.000],  loss: 1.226603, mae: 2.327690, mean_q: 10.593723, mean_eps: 0.905297\n",
      " 2120/10000: episode: 212, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4844.200 [355.000, 8865.000],  loss: 1.452024, mae: 2.350502, mean_q: 10.716479, mean_eps: 0.904848\n",
      " 2130/10000: episode: 213, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4533.300 [95.000, 7785.000],  loss: 1.214940, mae: 2.239352, mean_q: 10.253002, mean_eps: 0.904397\n",
      " 2140/10000: episode: 214, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5365.000 [1503.000, 8899.000],  loss: 1.304847, mae: 2.214894, mean_q: 10.050504, mean_eps: 0.903948\n",
      " 2150/10000: episode: 215, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3059.100 [1076.000, 8195.000],  loss: 1.692049, mae: 2.270380, mean_q: 10.113311, mean_eps: 0.903497\n",
      " 2160/10000: episode: 216, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5630.900 [3534.000, 8670.000],  loss: 1.635673, mae: 2.233754, mean_q: 10.027668, mean_eps: 0.903047\n",
      " 2170/10000: episode: 217, duration: 0.306s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4186.300 [1748.000, 6902.000],  loss: 1.175378, mae: 2.155861, mean_q: 9.786421, mean_eps: 0.902598\n",
      " 2180/10000: episode: 218, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4422.800 [855.000, 9247.000],  loss: 2.062774, mae: 2.036209, mean_q: 9.284444, mean_eps: 0.902147\n",
      " 2190/10000: episode: 219, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5490.900 [570.000, 9592.000],  loss: 1.052139, mae: 2.138512, mean_q: 9.745987, mean_eps: 0.901698\n",
      " 2200/10000: episode: 220, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5022.800 [160.000, 9488.000],  loss: 1.291854, mae: 2.137393, mean_q: 9.684799, mean_eps: 0.901247\n",
      " 2210/10000: episode: 221, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5050.200 [18.000, 9279.000],  loss: 1.287856, mae: 2.128530, mean_q: 9.481634, mean_eps: 0.900797\n",
      " 2220/10000: episode: 222, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4599.000 [293.000, 9039.000],  loss: 1.281221, mae: 2.426393, mean_q: 10.745556, mean_eps: 0.900347\n",
      " 2230/10000: episode: 223, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4694.900 [230.000, 9669.000],  loss: 1.965717, mae: 2.208276, mean_q: 9.782438, mean_eps: 0.899898\n",
      " 2240/10000: episode: 224, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5237.800 [2440.000, 6945.000],  loss: 1.800545, mae: 2.439583, mean_q: 10.902268, mean_eps: 0.899448\n",
      " 2250/10000: episode: 225, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5007.100 [218.000, 9051.000],  loss: 2.248165, mae: 2.274149, mean_q: 10.186771, mean_eps: 0.898997\n",
      " 2260/10000: episode: 226, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4726.200 [264.000, 8990.000],  loss: 1.147238, mae: 2.306585, mean_q: 10.349459, mean_eps: 0.898547\n",
      " 2270/10000: episode: 227, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4192.100 [157.000, 9139.000],  loss: 1.798483, mae: 2.317185, mean_q: 10.390060, mean_eps: 0.898098\n",
      " 2280/10000: episode: 228, duration: 0.321s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4731.300 [90.000, 9347.000],  loss: 1.372938, mae: 2.353982, mean_q: 10.387788, mean_eps: 0.897647\n",
      " 2290/10000: episode: 229, duration: 0.308s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4834.200 [658.000, 8875.000],  loss: 1.059435, mae: 2.521996, mean_q: 11.224755, mean_eps: 0.897198\n",
      " 2300/10000: episode: 230, duration: 0.306s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3702.000 [82.000, 8094.000],  loss: 1.870417, mae: 2.397891, mean_q: 10.757522, mean_eps: 0.896748\n",
      " 2310/10000: episode: 231, duration: 0.305s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5431.100 [772.000, 8440.000],  loss: 1.234917, mae: 2.458041, mean_q: 10.992233, mean_eps: 0.896298\n",
      " 2320/10000: episode: 232, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4423.400 [1217.000, 8268.000],  loss: 1.660899, mae: 2.533234, mean_q: 11.305347, mean_eps: 0.895848\n",
      " 2330/10000: episode: 233, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2845.000 [387.000, 5119.000],  loss: 1.922588, mae: 2.574075, mean_q: 11.477367, mean_eps: 0.895397\n",
      " 2340/10000: episode: 234, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6479.600 [1095.000, 9608.000],  loss: 1.645131, mae: 2.636736, mean_q: 11.732111, mean_eps: 0.894948\n",
      " 2350/10000: episode: 235, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3496.900 [939.000, 6113.000],  loss: 1.766890, mae: 2.448230, mean_q: 10.797167, mean_eps: 0.894497\n",
      " 2360/10000: episode: 236, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5430.800 [939.000, 9445.000],  loss: 1.959895, mae: 2.771610, mean_q: 12.053247, mean_eps: 0.894048\n",
      " 2370/10000: episode: 237, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4930.900 [691.000, 9443.000],  loss: 1.380439, mae: 2.750732, mean_q: 11.852053, mean_eps: 0.893598\n",
      " 2380/10000: episode: 238, duration: 0.400s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4655.900 [1589.000, 8832.000],  loss: 1.616021, mae: 2.763658, mean_q: 12.085419, mean_eps: 0.893147\n",
      " 2390/10000: episode: 239, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4839.400 [1238.000, 8678.000],  loss: 1.167051, mae: 2.746582, mean_q: 12.110818, mean_eps: 0.892698\n",
      " 2400/10000: episode: 240, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4602.600 [419.000, 7706.000],  loss: 1.741768, mae: 2.654399, mean_q: 11.716621, mean_eps: 0.892247\n",
      " 2410/10000: episode: 241, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3240.000 [166.000, 9651.000],  loss: 1.276540, mae: 2.781549, mean_q: 12.061806, mean_eps: 0.891798\n",
      " 2420/10000: episode: 242, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4629.800 [608.000, 9638.000],  loss: 1.685356, mae: 2.734906, mean_q: 12.060065, mean_eps: 0.891347\n",
      " 2430/10000: episode: 243, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2699.000 [458.000, 4882.000],  loss: 1.480487, mae: 2.607685, mean_q: 11.580082, mean_eps: 0.890897\n",
      " 2440/10000: episode: 244, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4161.800 [721.000, 8341.000],  loss: 1.487573, mae: 2.801730, mean_q: 12.372773, mean_eps: 0.890447\n",
      " 2450/10000: episode: 245, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3447.900 [143.000, 7300.000],  loss: 3.529696, mae: 2.796741, mean_q: 12.426295, mean_eps: 0.889997\n",
      " 2460/10000: episode: 246, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3496.700 [143.000, 9471.000],  loss: 2.327684, mae: 2.898931, mean_q: 12.865050, mean_eps: 0.889548\n",
      " 2470/10000: episode: 247, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4980.800 [746.000, 9478.000],  loss: 2.778265, mae: 3.216429, mean_q: 14.047486, mean_eps: 0.889097\n",
      " 2480/10000: episode: 248, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3735.900 [689.000, 8046.000],  loss: 1.767054, mae: 3.053827, mean_q: 13.173231, mean_eps: 0.888648\n",
      " 2490/10000: episode: 249, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2823.900 [143.000, 7109.000],  loss: 3.128885, mae: 3.032559, mean_q: 13.222687, mean_eps: 0.888198\n",
      " 2500/10000: episode: 250, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5394.400 [143.000, 9230.000],  loss: 2.186496, mae: 3.118498, mean_q: 14.359519, mean_eps: 0.887748\n",
      " 2510/10000: episode: 251, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3698.900 [663.000, 9050.000],  loss: 2.535096, mae: 3.117877, mean_q: 14.461401, mean_eps: 0.887298\n",
      " 2520/10000: episode: 252, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5642.600 [1725.000, 9218.000],  loss: 1.507490, mae: 3.140467, mean_q: 13.579004, mean_eps: 0.886848\n",
      " 2530/10000: episode: 253, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5314.100 [259.000, 9527.000],  loss: 3.264190, mae: 3.311177, mean_q: 14.340423, mean_eps: 0.886397\n",
      " 2540/10000: episode: 254, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5983.600 [3537.000, 9611.000],  loss: 2.724738, mae: 3.119888, mean_q: 13.341646, mean_eps: 0.885947\n",
      " 2550/10000: episode: 255, duration: 0.334s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5562.000 [2952.000, 8066.000],  loss: 3.136824, mae: 3.311260, mean_q: 14.128761, mean_eps: 0.885497\n",
      " 2560/10000: episode: 256, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5125.900 [112.000, 8695.000],  loss: 1.586033, mae: 3.339292, mean_q: 14.200488, mean_eps: 0.885048\n",
      " 2570/10000: episode: 257, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4956.000 [1224.000, 8326.000],  loss: 2.013525, mae: 3.347379, mean_q: 14.418502, mean_eps: 0.884597\n",
      " 2580/10000: episode: 258, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5647.800 [1272.000, 8914.000],  loss: 3.283911, mae: 3.182540, mean_q: 13.995843, mean_eps: 0.884147\n",
      " 2590/10000: episode: 259, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5954.200 [2157.000, 9346.000],  loss: 2.515049, mae: 3.044675, mean_q: 13.495107, mean_eps: 0.883698\n",
      " 2600/10000: episode: 260, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5134.500 [263.000, 9022.000],  loss: 2.721982, mae: 3.091606, mean_q: 13.696339, mean_eps: 0.883248\n",
      " 2610/10000: episode: 261, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6740.000 [2609.000, 9449.000],  loss: 2.197268, mae: 3.180640, mean_q: 14.058884, mean_eps: 0.882798\n",
      " 2620/10000: episode: 262, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6899.900 [3293.000, 9665.000],  loss: 4.838224, mae: 3.387288, mean_q: 14.931996, mean_eps: 0.882347\n",
      " 2630/10000: episode: 263, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4541.800 [156.000, 9116.000],  loss: 2.844470, mae: 3.544496, mean_q: 15.273546, mean_eps: 0.881898\n",
      " 2640/10000: episode: 264, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4519.200 [1022.000, 7606.000],  loss: 1.971405, mae: 3.513772, mean_q: 15.149996, mean_eps: 0.881447\n",
      " 2650/10000: episode: 265, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2974.800 [963.000, 8677.000],  loss: 1.907722, mae: 3.454176, mean_q: 14.887223, mean_eps: 0.880997\n",
      " 2660/10000: episode: 266, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5851.400 [1022.000, 9678.000],  loss: 3.034763, mae: 3.291028, mean_q: 14.163723, mean_eps: 0.880547\n",
      " 2670/10000: episode: 267, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4949.800 [385.000, 9559.000],  loss: 2.880248, mae: 3.538427, mean_q: 15.189504, mean_eps: 0.880097\n",
      " 2680/10000: episode: 268, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4836.200 [464.000, 8685.000],  loss: 4.426293, mae: 3.599255, mean_q: 15.434182, mean_eps: 0.879647\n",
      " 2690/10000: episode: 269, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3843.200 [470.000, 8429.000],  loss: 3.156572, mae: 3.650091, mean_q: 15.690099, mean_eps: 0.879197\n",
      " 2700/10000: episode: 270, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4678.400 [756.000, 8604.000],  loss: 5.054362, mae: 3.948272, mean_q: 16.877528, mean_eps: 0.878748\n",
      " 2710/10000: episode: 271, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4121.600 [849.000, 7592.000],  loss: 5.120429, mae: 3.771741, mean_q: 16.103140, mean_eps: 0.878298\n",
      " 2720/10000: episode: 272, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4514.600 [1453.000, 7662.000],  loss: 3.917431, mae: 3.838540, mean_q: 16.624680, mean_eps: 0.877848\n",
      " 2730/10000: episode: 273, duration: 0.485s, episode steps:  10, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4918.700 [1514.000, 8793.000],  loss: 3.172348, mae: 3.467536, mean_q: 15.149570, mean_eps: 0.877398\n",
      " 2740/10000: episode: 274, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4959.600 [147.000, 8224.000],  loss: 3.974569, mae: 3.672962, mean_q: 16.062667, mean_eps: 0.876947\n",
      " 2750/10000: episode: 275, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4593.200 [792.000, 9192.000],  loss: 3.278517, mae: 3.578168, mean_q: 15.847605, mean_eps: 0.876497\n",
      " 2760/10000: episode: 276, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5959.200 [3620.000, 8706.000],  loss: 6.624037, mae: 3.831091, mean_q: 17.159768, mean_eps: 0.876047\n",
      " 2770/10000: episode: 277, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5549.700 [2170.000, 8786.000],  loss: 5.682643, mae: 3.796841, mean_q: 17.018321, mean_eps: 0.875597\n",
      " 2780/10000: episode: 278, duration: 0.290s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4068.100 [1834.000, 7815.000],  loss: 3.352224, mae: 3.970476, mean_q: 17.743769, mean_eps: 0.875148\n",
      " 2790/10000: episode: 279, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4371.100 [418.000, 9736.000],  loss: 4.344802, mae: 3.998053, mean_q: 17.415714, mean_eps: 0.874697\n",
      " 2800/10000: episode: 280, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5050.600 [127.000, 9488.000],  loss: 3.278719, mae: 4.031435, mean_q: 17.642154, mean_eps: 0.874247\n",
      " 2810/10000: episode: 281, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6616.600 [1873.000, 9472.000],  loss: 4.337656, mae: 3.999814, mean_q: 17.783393, mean_eps: 0.873798\n",
      " 2820/10000: episode: 282, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5459.000 [88.000, 8538.000],  loss: 2.690819, mae: 3.802241, mean_q: 17.015701, mean_eps: 0.873348\n",
      " 2830/10000: episode: 283, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4530.500 [1342.000, 8098.000],  loss: 5.332002, mae: 3.921000, mean_q: 17.549672, mean_eps: 0.872897\n",
      " 2840/10000: episode: 284, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6876.100 [2428.000, 9418.000],  loss: 4.940248, mae: 3.967110, mean_q: 17.688284, mean_eps: 0.872447\n",
      " 2850/10000: episode: 285, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6124.200 [418.000, 9706.000],  loss: 5.414787, mae: 3.940501, mean_q: 17.491796, mean_eps: 0.871997\n",
      " 2860/10000: episode: 286, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5577.000 [757.000, 9417.000],  loss: 5.091896, mae: 4.354353, mean_q: 19.369065, mean_eps: 0.871547\n",
      " 2870/10000: episode: 287, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 7320.300 [2307.000, 9014.000],  loss: 9.590270, mae: 4.448644, mean_q: 19.860881, mean_eps: 0.871097\n",
      " 2880/10000: episode: 288, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3983.800 [984.000, 8690.000],  loss: 5.782257, mae: 4.517226, mean_q: 20.259762, mean_eps: 0.870648\n",
      " 2890/10000: episode: 289, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4432.900 [813.000, 9664.000],  loss: 8.395907, mae: 4.518370, mean_q: 20.279720, mean_eps: 0.870197\n",
      " 2900/10000: episode: 290, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3334.000 [279.000, 6799.000],  loss: 4.413717, mae: 4.946332, mean_q: 22.156418, mean_eps: 0.869747\n",
      " 2910/10000: episode: 291, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5434.500 [2532.000, 9444.000],  loss: 5.473626, mae: 5.075325, mean_q: 22.296431, mean_eps: 0.869297\n",
      " 2920/10000: episode: 292, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2775.200 [1611.000, 6885.000],  loss: 7.487584, mae: 4.814093, mean_q: 21.300961, mean_eps: 0.868847\n",
      " 2930/10000: episode: 293, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3699.200 [93.000, 8369.000],  loss: 5.639063, mae: 4.787859, mean_q: 21.315866, mean_eps: 0.868398\n",
      " 2940/10000: episode: 294, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5176.500 [823.000, 9368.000],  loss: 8.639527, mae: 4.952012, mean_q: 21.809608, mean_eps: 0.867947\n",
      " 2950/10000: episode: 295, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3311.300 [193.000, 9606.000],  loss: 9.262795, mae: 4.683858, mean_q: 20.283211, mean_eps: 0.867498\n",
      " 2960/10000: episode: 296, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4347.600 [422.000, 8355.000],  loss: 6.913314, mae: 5.325878, mean_q: 23.208216, mean_eps: 0.867047\n",
      " 2970/10000: episode: 297, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4369.500 [186.000, 9586.000],  loss: 7.146145, mae: 5.376605, mean_q: 23.418243, mean_eps: 0.866597\n",
      " 2980/10000: episode: 298, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5227.600 [842.000, 9628.000],  loss: 9.801054, mae: 5.231909, mean_q: 22.744346, mean_eps: 0.866147\n",
      " 2990/10000: episode: 299, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3721.500 [618.000, 9318.000],  loss: 9.290199, mae: 5.485922, mean_q: 23.802729, mean_eps: 0.865698\n",
      " 3000/10000: episode: 300, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2832.400 [352.000, 7740.000],  loss: 7.258882, mae: 5.674524, mean_q: 24.213099, mean_eps: 0.865248\n",
      " 3010/10000: episode: 301, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5397.700 [199.000, 9116.000],  loss: 9.666547, mae: 5.606252, mean_q: 23.857088, mean_eps: 0.864797\n",
      " 3020/10000: episode: 302, duration: 0.363s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4677.200 [481.000, 9706.000],  loss: 11.601739, mae: 5.464828, mean_q: 23.323762, mean_eps: 0.864347\n",
      " 3030/10000: episode: 303, duration: 0.317s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6061.000 [2274.000, 9685.000],  loss: 10.462889, mae: 5.468762, mean_q: 23.135732, mean_eps: 0.863897\n",
      " 3040/10000: episode: 304, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5667.500 [910.000, 9500.000],  loss: 14.115643, mae: 5.405834, mean_q: 22.806235, mean_eps: 0.863448\n",
      " 3050/10000: episode: 305, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4897.600 [2317.000, 8434.000],  loss: 12.200301, mae: 5.280815, mean_q: 23.128052, mean_eps: 0.862997\n",
      " 3060/10000: episode: 306, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4893.400 [444.000, 8829.000],  loss: 5.645255, mae: 5.178802, mean_q: 23.288303, mean_eps: 0.862548\n",
      " 3070/10000: episode: 307, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6368.600 [418.000, 9374.000],  loss: 5.047007, mae: 5.129150, mean_q: 23.053942, mean_eps: 0.862097\n",
      " 3080/10000: episode: 308, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3463.800 [314.000, 9389.000],  loss: 10.126989, mae: 5.166154, mean_q: 21.965325, mean_eps: 0.861647\n",
      " 3090/10000: episode: 309, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3767.900 [314.000, 9118.000],  loss: 12.839283, mae: 6.080594, mean_q: 26.111857, mean_eps: 0.861197\n",
      " 3100/10000: episode: 310, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6272.700 [355.000, 9613.000],  loss: 12.575575, mae: 5.860279, mean_q: 25.245756, mean_eps: 0.860748\n",
      " 3110/10000: episode: 311, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5117.500 [355.000, 8974.000],  loss: 7.959471, mae: 6.111780, mean_q: 26.478330, mean_eps: 0.860298\n",
      " 3120/10000: episode: 312, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6505.400 [157.000, 9712.000],  loss: 8.434176, mae: 5.945052, mean_q: 24.964809, mean_eps: 0.859847\n",
      " 3130/10000: episode: 313, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3721.300 [157.000, 7155.000],  loss: 8.103691, mae: 6.040104, mean_q: 25.443476, mean_eps: 0.859398\n",
      " 3140/10000: episode: 314, duration: 0.374s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4742.000 [157.000, 8659.000],  loss: 11.561848, mae: 5.804212, mean_q: 24.415048, mean_eps: 0.858947\n",
      " 3150/10000: episode: 315, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4218.400 [9.000, 9299.000],  loss: 13.121637, mae: 6.167454, mean_q: 25.878377, mean_eps: 0.858498\n",
      " 3160/10000: episode: 316, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4500.600 [452.000, 9516.000],  loss: 8.935785, mae: 6.144317, mean_q: 25.366905, mean_eps: 0.858047\n",
      " 3170/10000: episode: 317, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5477.900 [418.000, 8831.000],  loss: 7.606039, mae: 6.400274, mean_q: 26.289733, mean_eps: 0.857598\n",
      " 3180/10000: episode: 318, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4146.300 [418.000, 9563.000],  loss: 11.238225, mae: 6.004833, mean_q: 24.702750, mean_eps: 0.857147\n",
      " 3190/10000: episode: 319, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3933.600 [207.000, 9137.000],  loss: 10.829885, mae: 6.399302, mean_q: 26.294429, mean_eps: 0.856697\n",
      " 3200/10000: episode: 320, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5909.800 [89.000, 9440.000],  loss: 11.214225, mae: 6.228476, mean_q: 25.461653, mean_eps: 0.856247\n",
      " 3210/10000: episode: 321, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4375.700 [55.000, 7978.000],  loss: 9.160205, mae: 6.356330, mean_q: 26.381890, mean_eps: 0.855798\n",
      " 3220/10000: episode: 322, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4398.600 [120.000, 8776.000],  loss: 12.818700, mae: 6.388129, mean_q: 27.585959, mean_eps: 0.855348\n",
      " 3230/10000: episode: 323, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2560.100 [197.000, 8968.000],  loss: 11.741606, mae: 6.363693, mean_q: 27.865409, mean_eps: 0.854897\n",
      " 3240/10000: episode: 324, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5756.800 [197.000, 9036.000],  loss: 7.215682, mae: 6.555273, mean_q: 28.742300, mean_eps: 0.854448\n",
      " 3250/10000: episode: 325, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2384.500 [197.000, 7440.000],  loss: 13.711012, mae: 6.738162, mean_q: 28.343180, mean_eps: 0.853997\n",
      " 3260/10000: episode: 326, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4068.400 [117.000, 8888.000],  loss: 9.752102, mae: 6.876020, mean_q: 27.966046, mean_eps: 0.853548\n",
      " 3270/10000: episode: 327, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5923.200 [418.000, 9129.000],  loss: 12.902708, mae: 7.115687, mean_q: 28.970105, mean_eps: 0.853097\n",
      " 3280/10000: episode: 328, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5042.900 [418.000, 9608.000],  loss: 17.474293, mae: 7.102388, mean_q: 28.966627, mean_eps: 0.852648\n",
      " 3290/10000: episode: 329, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4678.300 [40.000, 7835.000],  loss: 10.493234, mae: 7.352254, mean_q: 29.823111, mean_eps: 0.852197\n",
      " 3300/10000: episode: 330, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3988.900 [729.000, 8826.000],  loss: 12.310254, mae: 7.121479, mean_q: 29.290731, mean_eps: 0.851747\n",
      " 3310/10000: episode: 331, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3538.000 [678.000, 7992.000],  loss: 20.486728, mae: 7.066383, mean_q: 29.202331, mean_eps: 0.851297\n",
      " 3320/10000: episode: 332, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6360.000 [2328.000, 9616.000],  loss: 19.110934, mae: 7.276519, mean_q: 30.060994, mean_eps: 0.850848\n",
      " 3330/10000: episode: 333, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3741.000 [348.000, 9448.000],  loss: 10.580466, mae: 7.379485, mean_q: 29.934983, mean_eps: 0.850398\n",
      " 3340/10000: episode: 334, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6189.000 [461.000, 9400.000],  loss: 16.343911, mae: 7.192571, mean_q: 29.194153, mean_eps: 0.849948\n",
      " 3350/10000: episode: 335, duration: 0.260s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4541.100 [366.000, 7360.000],  loss: 17.917909, mae: 7.712806, mean_q: 31.358074, mean_eps: 0.849498\n",
      " 3360/10000: episode: 336, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5691.200 [900.000, 9182.000],  loss: 19.899753, mae: 7.675335, mean_q: 31.516365, mean_eps: 0.849047\n",
      " 3370/10000: episode: 337, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1581.900 [418.000, 4656.000],  loss: 12.674615, mae: 7.586650, mean_q: 31.900088, mean_eps: 0.848598\n",
      " 3380/10000: episode: 338, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4708.200 [418.000, 9070.000],  loss: 21.825147, mae: 7.885832, mean_q: 33.189428, mean_eps: 0.848147\n",
      " 3390/10000: episode: 339, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4208.300 [314.000, 9503.000],  loss: 21.650625, mae: 8.126523, mean_q: 33.501498, mean_eps: 0.847698\n",
      " 3400/10000: episode: 340, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4748.800 [157.000, 9233.000],  loss: 14.103855, mae: 7.989392, mean_q: 33.164601, mean_eps: 0.847247\n",
      " 3410/10000: episode: 341, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5117.000 [1017.000, 9362.000],  loss: 21.102951, mae: 8.619912, mean_q: 36.054260, mean_eps: 0.846797\n",
      " 3420/10000: episode: 342, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4217.500 [1263.000, 9367.000],  loss: 15.673452, mae: 8.042121, mean_q: 32.367619, mean_eps: 0.846347\n",
      " 3430/10000: episode: 343, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4590.000 [1730.000, 9359.000],  loss: 18.809555, mae: 8.301709, mean_q: 33.953304, mean_eps: 0.845898\n",
      " 3440/10000: episode: 344, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3967.900 [227.000, 8585.000],  loss: 25.780798, mae: 7.660243, mean_q: 31.814757, mean_eps: 0.845448\n",
      " 3450/10000: episode: 345, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3272.700 [707.000, 7031.000],  loss: 15.966529, mae: 7.660882, mean_q: 31.108521, mean_eps: 0.844997\n",
      " 3460/10000: episode: 346, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 5189.300 [902.000, 8724.000],  loss: 24.245579, mae: 7.295871, mean_q: 29.422119, mean_eps: 0.844548\n",
      " 3470/10000: episode: 347, duration: 0.329s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4020.700 [69.000, 8724.000],  loss: 21.415518, mae: 7.226334, mean_q: 29.047232, mean_eps: 0.844097\n",
      " 3480/10000: episode: 348, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3609.400 [60.000, 7685.000],  loss: 13.506618, mae: 7.935470, mean_q: 32.590301, mean_eps: 0.843647\n",
      " 3490/10000: episode: 349, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5128.500 [321.000, 9000.000],  loss: 19.953196, mae: 7.806723, mean_q: 32.456005, mean_eps: 0.843197\n",
      " 3500/10000: episode: 350, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4947.400 [1578.000, 8900.000],  loss: 18.111802, mae: 8.434419, mean_q: 35.138913, mean_eps: 0.842747\n",
      " 3510/10000: episode: 351, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4993.300 [2304.000, 9240.000],  loss: 21.228566, mae: 8.591824, mean_q: 35.777191, mean_eps: 0.842297\n",
      " 3520/10000: episode: 352, duration: 0.242s, episode steps:  10, steps per second:  41, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4287.100 [505.000, 9327.000],  loss: 30.127603, mae: 9.304589, mean_q: 39.331725, mean_eps: 0.841847\n",
      " 3530/10000: episode: 353, duration: 0.237s, episode steps:  10, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4348.200 [505.000, 7406.000],  loss: 21.535019, mae: 9.079862, mean_q: 38.587097, mean_eps: 0.841398\n",
      " 3540/10000: episode: 354, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5280.600 [505.000, 9683.000],  loss: 33.130456, mae: 9.031023, mean_q: 38.347284, mean_eps: 0.840948\n",
      " 3550/10000: episode: 355, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3985.100 [107.000, 7441.000],  loss: 21.182618, mae: 9.445535, mean_q: 39.966116, mean_eps: 0.840498\n",
      " 3560/10000: episode: 356, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4887.200 [505.000, 9736.000],  loss: 19.239756, mae: 9.846334, mean_q: 40.186681, mean_eps: 0.840048\n",
      " 3570/10000: episode: 357, duration: 0.253s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5134.600 [66.000, 9585.000],  loss: 21.884653, mae: 10.057776, mean_q: 40.865233, mean_eps: 0.839597\n",
      " 3580/10000: episode: 358, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5391.700 [381.000, 9736.000],  loss: 30.741696, mae: 9.735273, mean_q: 39.563688, mean_eps: 0.839147\n",
      " 3590/10000: episode: 359, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6684.400 [935.000, 9265.000],  loss: 20.966190, mae: 9.628603, mean_q: 39.083810, mean_eps: 0.838697\n",
      " 3600/10000: episode: 360, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5616.200 [1900.000, 9736.000],  loss: 24.214220, mae: 10.155943, mean_q: 41.118866, mean_eps: 0.838247\n",
      " 3610/10000: episode: 361, duration: 0.241s, episode steps:  10, steps per second:  42, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 6021.600 [314.000, 9736.000],  loss: 40.844429, mae: 9.723261, mean_q: 39.269678, mean_eps: 0.837798\n",
      " 3620/10000: episode: 362, duration: 0.247s, episode steps:  10, steps per second:  41, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2728.400 [0.000, 8465.000],  loss: 44.253408, mae: 10.326706, mean_q: 42.718498, mean_eps: 0.837347\n",
      " 3630/10000: episode: 363, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6392.000 [505.000, 9711.000],  loss: 28.917942, mae: 10.844008, mean_q: 45.525579, mean_eps: 0.836897\n",
      " 3640/10000: episode: 364, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3830.100 [1690.000, 9683.000],  loss: 29.531974, mae: 11.609305, mean_q: 48.780533, mean_eps: 0.836448\n",
      " 3650/10000: episode: 365, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2684.500 [314.000, 5831.000],  loss: 32.275720, mae: 11.842357, mean_q: 49.661033, mean_eps: 0.835998\n",
      " 3660/10000: episode: 366, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2829.400 [134.000, 8090.000],  loss: 33.227719, mae: 11.849413, mean_q: 49.530806, mean_eps: 0.835547\n",
      " 3670/10000: episode: 367, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4731.700 [456.000, 8934.000],  loss: 23.197007, mae: 11.578471, mean_q: 48.062403, mean_eps: 0.835097\n",
      " 3680/10000: episode: 368, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3949.100 [505.000, 7458.000],  loss: 45.072510, mae: 12.243106, mean_q: 49.357630, mean_eps: 0.834647\n",
      " 3690/10000: episode: 369, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4417.300 [505.000, 8597.000],  loss: 43.761946, mae: 12.777311, mean_q: 53.242350, mean_eps: 0.834197\n",
      " 3700/10000: episode: 370, duration: 0.407s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3023.700 [225.000, 9479.000],  loss: 48.597965, mae: 11.942068, mean_q: 51.064438, mean_eps: 0.833747\n",
      " 3710/10000: episode: 371, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4356.000 [423.000, 9630.000],  loss: 37.349895, mae: 13.711653, mean_q: 55.205566, mean_eps: 0.833297\n",
      " 3720/10000: episode: 372, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3082.600 [128.000, 8438.000],  loss: 23.601644, mae: 12.151505, mean_q: 49.588732, mean_eps: 0.832847\n",
      " 3730/10000: episode: 373, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 2393.300 [618.000, 5761.000],  loss: 42.834647, mae: 12.012491, mean_q: 49.287383, mean_eps: 0.832397\n",
      " 3740/10000: episode: 374, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3295.000 [471.000, 9373.000],  loss: 40.701440, mae: 11.604425, mean_q: 47.563000, mean_eps: 0.831947\n",
      " 3750/10000: episode: 375, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4289.200 [164.000, 8220.000],  loss: 34.517462, mae: 11.851733, mean_q: 47.389215, mean_eps: 0.831498\n",
      " 3760/10000: episode: 376, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4147.800 [1273.000, 8897.000],  loss: 53.920999, mae: 11.624697, mean_q: 45.966393, mean_eps: 0.831048\n",
      " 3770/10000: episode: 377, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3028.400 [734.000, 8969.000],  loss: 44.664542, mae: 13.295772, mean_q: 52.486261, mean_eps: 0.830597\n",
      " 3780/10000: episode: 378, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3838.200 [445.000, 9238.000],  loss: 36.302503, mae: 13.160748, mean_q: 52.020042, mean_eps: 0.830148\n",
      " 3790/10000: episode: 379, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2736.100 [238.000, 7469.000],  loss: 40.542523, mae: 12.664232, mean_q: 50.286381, mean_eps: 0.829697\n",
      " 3800/10000: episode: 380, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3309.800 [278.000, 8520.000],  loss: 22.322292, mae: 13.116896, mean_q: 52.710625, mean_eps: 0.829247\n",
      " 3810/10000: episode: 381, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5368.600 [478.000, 9737.000],  loss: 50.827663, mae: 13.624523, mean_q: 54.448960, mean_eps: 0.828797\n",
      " 3820/10000: episode: 382, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3799.100 [645.000, 9354.000],  loss: 61.847980, mae: 13.924055, mean_q: 55.947976, mean_eps: 0.828348\n",
      " 3830/10000: episode: 383, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 5233.800 [2644.000, 9650.000],  loss: 50.384634, mae: 13.076385, mean_q: 52.897913, mean_eps: 0.827898\n",
      " 3840/10000: episode: 384, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3838.900 [47.000, 7637.000],  loss: 38.817501, mae: 13.187461, mean_q: 53.065923, mean_eps: 0.827447\n",
      " 3850/10000: episode: 385, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4058.100 [1533.000, 6826.000],  loss: 40.409431, mae: 13.100777, mean_q: 52.586556, mean_eps: 0.826997\n",
      " 3860/10000: episode: 386, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4517.200 [988.000, 8606.000],  loss: 58.183535, mae: 13.761987, mean_q: 55.394394, mean_eps: 0.826547\n",
      " 3870/10000: episode: 387, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4481.000 [821.000, 8903.000],  loss: 57.895008, mae: 14.678032, mean_q: 58.963273, mean_eps: 0.826098\n",
      " 3880/10000: episode: 388, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4426.900 [795.000, 9226.000],  loss: 44.699206, mae: 15.649168, mean_q: 62.640104, mean_eps: 0.825647\n",
      " 3890/10000: episode: 389, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4431.400 [215.000, 9098.000],  loss: 83.658841, mae: 15.650057, mean_q: 62.451315, mean_eps: 0.825198\n",
      " 3900/10000: episode: 390, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5057.200 [271.000, 9712.000],  loss: 42.827473, mae: 15.670300, mean_q: 62.358459, mean_eps: 0.824747\n",
      " 3910/10000: episode: 391, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4460.700 [576.000, 9382.000],  loss: 64.239799, mae: 16.115783, mean_q: 63.973179, mean_eps: 0.824297\n",
      " 3920/10000: episode: 392, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5837.800 [671.000, 9063.000],  loss: 66.800148, mae: 16.840310, mean_q: 66.343937, mean_eps: 0.823847\n",
      " 3930/10000: episode: 393, duration: 0.394s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4386.400 [700.000, 8491.000],  loss: 64.069507, mae: 16.636266, mean_q: 65.052627, mean_eps: 0.823398\n",
      " 3940/10000: episode: 394, duration: 0.324s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2463.500 [510.000, 9236.000],  loss: 77.243139, mae: 16.095401, mean_q: 63.359837, mean_eps: 0.822948\n",
      " 3950/10000: episode: 395, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5789.900 [1296.000, 9468.000],  loss: 45.421579, mae: 15.771398, mean_q: 62.174503, mean_eps: 0.822497\n",
      " 3960/10000: episode: 396, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5251.300 [938.000, 9626.000],  loss: 71.956620, mae: 16.214468, mean_q: 63.689069, mean_eps: 0.822047\n",
      " 3970/10000: episode: 397, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3397.000 [418.000, 7704.000],  loss: 84.604905, mae: 16.315243, mean_q: 65.080755, mean_eps: 0.821597\n",
      " 3980/10000: episode: 398, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4637.700 [995.000, 9603.000],  loss: 41.872977, mae: 17.123828, mean_q: 70.498531, mean_eps: 0.821148\n",
      " 3990/10000: episode: 399, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 3018.600 [418.000, 8934.000],  loss: 70.197308, mae: 17.408246, mean_q: 72.554542, mean_eps: 0.820697\n",
      " 4000/10000: episode: 400, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6187.600 [418.000, 9567.000],  loss: 65.296947, mae: 16.191355, mean_q: 66.984396, mean_eps: 0.820248\n",
      " 4010/10000: episode: 401, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3748.000 [383.000, 8206.000],  loss: 76.557270, mae: 18.012304, mean_q: 73.841827, mean_eps: 0.819797\n",
      " 4020/10000: episode: 402, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5010.300 [418.000, 9232.000],  loss: 90.420581, mae: 17.194418, mean_q: 69.465257, mean_eps: 0.819347\n",
      " 4030/10000: episode: 403, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 5504.500 [1503.000, 9521.000],  loss: 80.460894, mae: 18.213315, mean_q: 71.813973, mean_eps: 0.818897\n",
      " 4040/10000: episode: 404, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4309.200 [969.000, 7212.000],  loss: 75.017855, mae: 19.198914, mean_q: 75.689780, mean_eps: 0.818448\n",
      " 4050/10000: episode: 405, duration: 0.839s, episode steps:  10, steps per second:  12, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6667.700 [3573.000, 9485.000],  loss: 101.938186, mae: 18.975746, mean_q: 75.153768, mean_eps: 0.817998\n",
      " 4060/10000: episode: 406, duration: 1.164s, episode steps:  10, steps per second:   9, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 6482.600 [2295.000, 9737.000],  loss: 84.154352, mae: 18.649766, mean_q: 73.822817, mean_eps: 0.817547\n",
      " 4070/10000: episode: 407, duration: 0.391s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2739.000 [755.000, 8686.000],  loss: 47.528948, mae: 19.566692, mean_q: 79.077917, mean_eps: 0.817098\n",
      " 4080/10000: episode: 408, duration: 0.391s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4862.700 [2928.000, 9428.000],  loss: 69.912855, mae: 18.540525, mean_q: 73.382487, mean_eps: 0.816647\n",
      " 4090/10000: episode: 409, duration: 0.422s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4196.100 [657.000, 8888.000],  loss: 117.660512, mae: 19.973783, mean_q: 78.324186, mean_eps: 0.816198\n",
      " 4100/10000: episode: 410, duration: 0.499s, episode steps:  10, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5192.600 [1607.000, 9172.000],  loss: 117.045391, mae: 19.696089, mean_q: 77.909785, mean_eps: 0.815747\n",
      " 4110/10000: episode: 411, duration: 0.522s, episode steps:  10, steps per second:  19, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3905.900 [804.000, 8811.000],  loss: 98.179287, mae: 20.753705, mean_q: 82.289919, mean_eps: 0.815298\n",
      " 4120/10000: episode: 412, duration: 0.380s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3072.800 [418.000, 9650.000],  loss: 110.438547, mae: 21.562030, mean_q: 86.802741, mean_eps: 0.814847\n",
      " 4130/10000: episode: 413, duration: 0.358s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4925.600 [1264.000, 7428.000],  loss: 130.571620, mae: 21.983014, mean_q: 87.986018, mean_eps: 0.814397\n",
      " 4140/10000: episode: 414, duration: 0.330s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6767.600 [3975.000, 8272.000],  loss: 98.168390, mae: 22.217736, mean_q: 87.605386, mean_eps: 0.813947\n",
      " 4150/10000: episode: 415, duration: 0.420s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5636.300 [418.000, 9588.000],  loss: 112.275797, mae: 23.165719, mean_q: 91.544653, mean_eps: 0.813498\n",
      " 4160/10000: episode: 416, duration: 0.407s, episode steps:  10, steps per second:  25, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 2852.600 [418.000, 7280.000],  loss: 93.826084, mae: 24.334239, mean_q: 96.505588, mean_eps: 0.813048\n",
      " 4170/10000: episode: 417, duration: 0.639s, episode steps:  10, steps per second:  16, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5777.300 [3297.000, 9708.000],  loss: 154.268380, mae: 24.628571, mean_q: 96.753205, mean_eps: 0.812598\n",
      " 4180/10000: episode: 418, duration: 0.519s, episode steps:  10, steps per second:  19, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 4421.500 [418.000, 9237.000],  loss: 204.823193, mae: 23.665807, mean_q: 92.294449, mean_eps: 0.812148\n",
      " 4190/10000: episode: 419, duration: 0.420s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3584.100 [127.000, 9560.000],  loss: 126.303558, mae: 23.730380, mean_q: 92.705468, mean_eps: 0.811697\n",
      " 4200/10000: episode: 420, duration: 0.327s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2478.300 [24.000, 6559.000],  loss: 139.555398, mae: 25.230487, mean_q: 99.656160, mean_eps: 0.811248\n",
      " 4210/10000: episode: 421, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4672.800 [119.000, 8617.000],  loss: 149.039723, mae: 23.282087, mean_q: 94.903299, mean_eps: 0.810797\n",
      " 4220/10000: episode: 422, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4949.300 [1688.000, 9692.000],  loss: 89.343518, mae: 23.162530, mean_q: 95.491142, mean_eps: 0.810348\n",
      " 4230/10000: episode: 423, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3415.500 [255.000, 8481.000],  loss: 151.703471, mae: 22.909412, mean_q: 93.557920, mean_eps: 0.809897\n",
      " 4240/10000: episode: 424, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4546.800 [418.000, 8191.000],  loss: 145.922386, mae: 24.038869, mean_q: 99.261640, mean_eps: 0.809447\n",
      " 4250/10000: episode: 425, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3441.300 [287.000, 9677.000],  loss: 193.390722, mae: 25.779661, mean_q: 108.468231, mean_eps: 0.808997\n",
      " 4260/10000: episode: 426, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4572.800 [63.000, 9059.000],  loss: 138.557967, mae: 26.813771, mean_q: 112.478815, mean_eps: 0.808548\n",
      " 4270/10000: episode: 427, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2353.800 [418.000, 7460.000],  loss: 120.131072, mae: 24.989343, mean_q: 103.302383, mean_eps: 0.808098\n",
      " 4280/10000: episode: 428, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3704.700 [203.000, 9451.000],  loss: 255.976347, mae: 28.498224, mean_q: 114.531221, mean_eps: 0.807647\n",
      " 4290/10000: episode: 429, duration: 0.335s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4028.500 [459.000, 7886.000],  loss: 267.028898, mae: 28.332813, mean_q: 112.140789, mean_eps: 0.807198\n",
      " 4300/10000: episode: 430, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4047.900 [156.000, 9461.000],  loss: 183.980993, mae: 26.695269, mean_q: 106.361304, mean_eps: 0.806747\n",
      " 4310/10000: episode: 431, duration: 0.300s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3795.100 [201.000, 8745.000],  loss: 216.820410, mae: 27.590740, mean_q: 110.283382, mean_eps: 0.806298\n",
      " 4320/10000: episode: 432, duration: 0.297s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3744.700 [59.000, 8788.000],  loss: 139.076056, mae: 26.666224, mean_q: 106.504034, mean_eps: 0.805847\n",
      " 4330/10000: episode: 433, duration: 0.430s, episode steps:  10, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4813.600 [108.000, 9415.000],  loss: 150.657356, mae: 28.072218, mean_q: 112.693431, mean_eps: 0.805397\n",
      " 4340/10000: episode: 434, duration: 0.351s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2745.900 [913.000, 7023.000],  loss: 193.689439, mae: 29.516121, mean_q: 121.855494, mean_eps: 0.804947\n",
      " 4350/10000: episode: 435, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  6.000, mean reward:  0.600 [ 0.000,  1.000], mean action: 2901.800 [418.000, 9616.000],  loss: 136.054520, mae: 29.027729, mean_q: 118.236839, mean_eps: 0.804497\n",
      " 4360/10000: episode: 436, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 3977.400 [418.000, 9711.000],  loss: 283.316149, mae: 28.395704, mean_q: 113.140929, mean_eps: 0.804048\n",
      " 4370/10000: episode: 437, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4878.100 [970.000, 8514.000],  loss: 201.183852, mae: 30.036325, mean_q: 116.307192, mean_eps: 0.803598\n",
      " 4380/10000: episode: 438, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4742.100 [534.000, 8959.000],  loss: 149.698267, mae: 31.039911, mean_q: 120.352749, mean_eps: 0.803148\n",
      " 4390/10000: episode: 439, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5894.000 [604.000, 9515.000],  loss: 196.538041, mae: 32.078574, mean_q: 125.041017, mean_eps: 0.802698\n",
      " 4400/10000: episode: 440, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5586.100 [598.000, 8703.000],  loss: 189.301603, mae: 30.965923, mean_q: 124.667473, mean_eps: 0.802248\n",
      " 4410/10000: episode: 441, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 7568.400 [5340.000, 9164.000],  loss: 295.365129, mae: 32.790887, mean_q: 134.095566, mean_eps: 0.801797\n",
      " 4420/10000: episode: 442, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5777.100 [3754.000, 9329.000],  loss: 284.666941, mae: 33.879252, mean_q: 138.853362, mean_eps: 0.801347\n",
      " 4430/10000: episode: 443, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5689.300 [418.000, 8602.000],  loss: 387.236871, mae: 33.701861, mean_q: 139.122739, mean_eps: 0.800897\n",
      " 4440/10000: episode: 444, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3390.900 [418.000, 9334.000],  loss: 405.019168, mae: 35.491098, mean_q: 147.609296, mean_eps: 0.800448\n",
      " 4450/10000: episode: 445, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4093.200 [418.000, 7185.000],  loss: 238.816564, mae: 35.336744, mean_q: 146.053516, mean_eps: 0.799997\n",
      " 4460/10000: episode: 446, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3686.400 [116.000, 9391.000],  loss: 518.544827, mae: 35.543312, mean_q: 145.929891, mean_eps: 0.799547\n",
      " 4470/10000: episode: 447, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5910.900 [418.000, 9033.000],  loss: 482.776453, mae: 36.271679, mean_q: 147.993805, mean_eps: 0.799098\n",
      " 4480/10000: episode: 448, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4172.600 [145.000, 8828.000],  loss: 314.430446, mae: 40.998915, mean_q: 170.446001, mean_eps: 0.798648\n",
      " 4490/10000: episode: 449, duration: 0.390s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3672.700 [256.000, 9648.000],  loss: 685.665887, mae: 41.339769, mean_q: 170.736050, mean_eps: 0.798198\n",
      " 4500/10000: episode: 450, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3364.400 [132.000, 7991.000],  loss: 313.451774, mae: 38.186845, mean_q: 154.371353, mean_eps: 0.797747\n",
      " 4510/10000: episode: 451, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4934.800 [1141.000, 9433.000],  loss: 316.505567, mae: 41.890070, mean_q: 169.466451, mean_eps: 0.797297\n",
      " 4520/10000: episode: 452, duration: 0.309s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2446.400 [480.000, 8297.000],  loss: 627.952974, mae: 39.987817, mean_q: 161.820078, mean_eps: 0.796847\n",
      " 4530/10000: episode: 453, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3986.100 [1030.000, 8258.000],  loss: 532.915651, mae: 43.081310, mean_q: 173.989186, mean_eps: 0.796397\n",
      " 4540/10000: episode: 454, duration: 0.446s, episode steps:  10, steps per second:  22, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3451.100 [1826.000, 8464.000],  loss: 410.327666, mae: 42.727527, mean_q: 171.960051, mean_eps: 0.795948\n",
      " 4550/10000: episode: 455, duration: 0.372s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5397.200 [1024.000, 8891.000],  loss: 405.428665, mae: 43.005500, mean_q: 171.687926, mean_eps: 0.795497\n",
      " 4560/10000: episode: 456, duration: 0.297s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5029.200 [92.000, 9181.000],  loss: 312.568134, mae: 42.335638, mean_q: 162.679411, mean_eps: 0.795047\n",
      " 4570/10000: episode: 457, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5822.300 [1129.000, 9202.000],  loss: 317.961533, mae: 44.111870, mean_q: 170.207449, mean_eps: 0.794597\n",
      " 4580/10000: episode: 458, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7770.400 [4187.000, 9181.000],  loss: 439.847023, mae: 43.562140, mean_q: 166.475090, mean_eps: 0.794148\n",
      " 4590/10000: episode: 459, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 6123.000 [0.000, 9342.000],  loss: 328.100205, mae: 43.611975, mean_q: 162.022743, mean_eps: 0.793698\n",
      " 4600/10000: episode: 460, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2314.100 [264.000, 7534.000],  loss: 401.768805, mae: 42.084805, mean_q: 159.931012, mean_eps: 0.793247\n",
      " 4610/10000: episode: 461, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3142.500 [448.000, 7660.000],  loss: 447.283070, mae: 43.286232, mean_q: 168.406847, mean_eps: 0.792798\n",
      " 4620/10000: episode: 462, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4637.000 [1664.000, 7793.000],  loss: 606.671747, mae: 47.202169, mean_q: 182.186559, mean_eps: 0.792347\n",
      " 4630/10000: episode: 463, duration: 0.340s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3923.000 [1548.000, 7228.000],  loss: 587.609521, mae: 46.802348, mean_q: 179.281201, mean_eps: 0.791897\n",
      " 4640/10000: episode: 464, duration: 0.343s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5697.000 [2498.000, 9106.000],  loss: 353.884979, mae: 47.621357, mean_q: 182.225302, mean_eps: 0.791447\n",
      " 4650/10000: episode: 465, duration: 0.290s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5136.500 [2337.000, 8394.000],  loss: 511.958215, mae: 47.032032, mean_q: 181.605791, mean_eps: 0.790998\n",
      " 4660/10000: episode: 466, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3272.500 [1124.000, 8313.000],  loss: 515.948834, mae: 45.523722, mean_q: 184.629126, mean_eps: 0.790548\n",
      " 4670/10000: episode: 467, duration: 0.534s, episode steps:  10, steps per second:  19, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3342.500 [442.000, 7134.000],  loss: 352.964523, mae: 45.986769, mean_q: 197.658943, mean_eps: 0.790097\n",
      " 4680/10000: episode: 468, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5037.900 [1293.000, 9539.000],  loss: 746.698196, mae: 46.066043, mean_q: 185.515489, mean_eps: 0.789647\n",
      " 4690/10000: episode: 469, duration: 0.405s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4435.300 [26.000, 9418.000],  loss: 362.025608, mae: 45.084144, mean_q: 171.788484, mean_eps: 0.789197\n",
      " 4700/10000: episode: 470, duration: 0.342s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4145.200 [183.000, 9545.000],  loss: 496.134780, mae: 51.232030, mean_q: 196.830972, mean_eps: 0.788748\n",
      " 4710/10000: episode: 471, duration: 0.360s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3595.500 [0.000, 9515.000],  loss: 805.463589, mae: 48.386189, mean_q: 187.813734, mean_eps: 0.788297\n",
      " 4720/10000: episode: 472, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3946.600 [0.000, 8035.000],  loss: 834.279564, mae: 53.040697, mean_q: 208.313524, mean_eps: 0.787848\n",
      " 4730/10000: episode: 473, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2903.000 [0.000, 5780.000],  loss: 665.642131, mae: 53.534311, mean_q: 212.648691, mean_eps: 0.787397\n",
      " 4740/10000: episode: 474, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5940.600 [418.000, 9533.000],  loss: 461.781502, mae: 52.876772, mean_q: 207.888019, mean_eps: 0.786947\n",
      " 4750/10000: episode: 475, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5069.000 [418.000, 9301.000],  loss: 728.139503, mae: 50.545560, mean_q: 199.799443, mean_eps: 0.786497\n",
      " 4760/10000: episode: 476, duration: 0.297s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4795.300 [418.000, 9198.000],  loss: 1418.438153, mae: 55.509005, mean_q: 219.498279, mean_eps: 0.786048\n",
      " 4770/10000: episode: 477, duration: 0.253s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5058.500 [134.000, 9207.000],  loss: 874.513074, mae: 59.440631, mean_q: 232.201094, mean_eps: 0.785598\n",
      " 4780/10000: episode: 478, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3025.200 [0.000, 8787.000],  loss: 948.406154, mae: 55.693294, mean_q: 218.441740, mean_eps: 0.785147\n",
      " 4790/10000: episode: 479, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4020.900 [0.000, 8290.000],  loss: 1054.143851, mae: 59.932891, mean_q: 229.719444, mean_eps: 0.784697\n",
      " 4800/10000: episode: 480, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5093.600 [156.000, 9086.000],  loss: 395.387277, mae: 58.571621, mean_q: 225.265869, mean_eps: 0.784247\n",
      " 4810/10000: episode: 481, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3423.700 [156.000, 8794.000],  loss: 601.671469, mae: 58.152331, mean_q: 224.590514, mean_eps: 0.783798\n",
      " 4820/10000: episode: 482, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3890.100 [122.000, 8118.000],  loss: 733.618114, mae: 59.469348, mean_q: 229.802585, mean_eps: 0.783347\n",
      " 4830/10000: episode: 483, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3931.900 [0.000, 9646.000],  loss: 1176.751785, mae: 60.138054, mean_q: 232.725002, mean_eps: 0.782898\n",
      " 4840/10000: episode: 484, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5377.800 [504.000, 9314.000],  loss: 1815.346201, mae: 60.764127, mean_q: 236.198831, mean_eps: 0.782447\n",
      " 4850/10000: episode: 485, duration: 0.302s, episode steps:  10, steps per second:  33, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4377.800 [0.000, 9660.000],  loss: 1211.797374, mae: 59.998097, mean_q: 233.261406, mean_eps: 0.781997\n",
      " 4860/10000: episode: 486, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5955.400 [484.000, 9185.000],  loss: 1135.409508, mae: 67.410414, mean_q: 264.766470, mean_eps: 0.781547\n",
      " 4870/10000: episode: 487, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4223.000 [418.000, 9320.000],  loss: 716.065189, mae: 64.197573, mean_q: 252.293248, mean_eps: 0.781098\n",
      " 4880/10000: episode: 488, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3425.200 [418.000, 8603.000],  loss: 1155.801511, mae: 64.161863, mean_q: 250.310960, mean_eps: 0.780648\n",
      " 4890/10000: episode: 489, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3004.900 [272.000, 5494.000],  loss: 1193.631039, mae: 66.489531, mean_q: 255.686301, mean_eps: 0.780197\n",
      " 4900/10000: episode: 490, duration: 0.326s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5173.400 [1058.000, 9287.000],  loss: 1380.845432, mae: 71.833589, mean_q: 276.696208, mean_eps: 0.779747\n",
      " 4910/10000: episode: 491, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5462.900 [98.000, 9310.000],  loss: 990.831624, mae: 70.934466, mean_q: 273.835912, mean_eps: 0.779297\n",
      " 4920/10000: episode: 492, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4784.300 [40.000, 9446.000],  loss: 1541.184225, mae: 75.639491, mean_q: 292.112411, mean_eps: 0.778848\n",
      " 4930/10000: episode: 493, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3095.900 [501.000, 5131.000],  loss: 1408.342224, mae: 72.230185, mean_q: 270.354008, mean_eps: 0.778397\n",
      " 4940/10000: episode: 494, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3608.700 [1268.000, 8565.000],  loss: 1336.059952, mae: 72.644160, mean_q: 274.179413, mean_eps: 0.777948\n",
      " 4950/10000: episode: 495, duration: 0.325s, episode steps:  10, steps per second:  31, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4255.200 [168.000, 9070.000],  loss: 1798.573138, mae: 72.306337, mean_q: 275.013934, mean_eps: 0.777497\n",
      " 4960/10000: episode: 496, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3691.700 [418.000, 7919.000],  loss: 1500.353583, mae: 73.090228, mean_q: 275.389278, mean_eps: 0.777047\n",
      " 4970/10000: episode: 497, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4153.500 [418.000, 7907.000],  loss: 1809.497064, mae: 76.947726, mean_q: 293.243152, mean_eps: 0.776597\n",
      " 4980/10000: episode: 498, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3167.700 [418.000, 7810.000],  loss: 1497.447052, mae: 76.936049, mean_q: 293.412227, mean_eps: 0.776148\n",
      " 4990/10000: episode: 499, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4315.300 [403.000, 9649.000],  loss: 2250.273395, mae: 82.137650, mean_q: 313.602542, mean_eps: 0.775698\n",
      " 5000/10000: episode: 500, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 4199.900 [418.000, 9337.000],  loss: 3106.612286, mae: 87.324030, mean_q: 333.908655, mean_eps: 0.775247\n",
      " 5010/10000: episode: 501, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4471.600 [418.000, 8582.000],  loss: 1882.723029, mae: 87.946028, mean_q: 331.964862, mean_eps: 0.774797\n",
      " 5020/10000: episode: 502, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 3248.300 [405.000, 9599.000],  loss: 1465.906146, mae: 84.901824, mean_q: 319.690927, mean_eps: 0.774347\n",
      " 5030/10000: episode: 503, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5906.600 [1031.000, 8629.000],  loss: 1601.001526, mae: 84.783485, mean_q: 319.900439, mean_eps: 0.773898\n",
      " 5040/10000: episode: 504, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4305.400 [106.000, 9506.000],  loss: 2240.438300, mae: 87.638782, mean_q: 326.388208, mean_eps: 0.773447\n",
      " 5050/10000: episode: 505, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4089.100 [418.000, 8985.000],  loss: 3506.007086, mae: 83.745068, mean_q: 315.768204, mean_eps: 0.772998\n",
      " 5060/10000: episode: 506, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4099.700 [118.000, 9734.000],  loss: 2342.489465, mae: 92.106638, mean_q: 352.729926, mean_eps: 0.772547\n",
      " 5070/10000: episode: 507, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4332.000 [418.000, 8268.000],  loss: 1783.634930, mae: 93.794576, mean_q: 362.413437, mean_eps: 0.772097\n",
      " 5080/10000: episode: 508, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6573.300 [418.000, 8909.000],  loss: 2541.180243, mae: 95.809867, mean_q: 367.482675, mean_eps: 0.771647\n",
      " 5090/10000: episode: 509, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4916.600 [244.000, 8089.000],  loss: 1867.679437, mae: 94.769970, mean_q: 373.174036, mean_eps: 0.771198\n",
      " 5100/10000: episode: 510, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5011.300 [359.000, 9434.000],  loss: 2046.654977, mae: 91.982458, mean_q: 355.401089, mean_eps: 0.770748\n",
      " 5110/10000: episode: 511, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3402.500 [314.000, 8943.000],  loss: 2508.637805, mae: 92.430094, mean_q: 340.073477, mean_eps: 0.770297\n",
      " 5120/10000: episode: 512, duration: 0.250s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4543.600 [1829.000, 8478.000],  loss: 2412.613116, mae: 96.580514, mean_q: 351.171240, mean_eps: 0.769847\n",
      " 5130/10000: episode: 513, duration: 0.236s, episode steps:  10, steps per second:  42, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4698.000 [418.000, 8027.000],  loss: 2157.749628, mae: 88.986444, mean_q: 324.856454, mean_eps: 0.769397\n",
      " 5140/10000: episode: 514, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4547.900 [418.000, 9481.000],  loss: 2537.678027, mae: 97.325249, mean_q: 363.572833, mean_eps: 0.768948\n",
      " 5150/10000: episode: 515, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4776.600 [418.000, 9637.000],  loss: 2738.326147, mae: 96.079574, mean_q: 367.391605, mean_eps: 0.768497\n",
      " 5160/10000: episode: 516, duration: 0.242s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3315.600 [331.000, 9435.000],  loss: 3592.366516, mae: 105.188354, mean_q: 399.171451, mean_eps: 0.768047\n",
      " 5170/10000: episode: 517, duration: 0.248s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4748.000 [486.000, 9415.000],  loss: 2553.877278, mae: 105.093990, mean_q: 385.002936, mean_eps: 0.767597\n",
      " 5180/10000: episode: 518, duration: 0.260s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4817.100 [76.000, 9286.000],  loss: 2666.118216, mae: 105.369891, mean_q: 386.076529, mean_eps: 0.767147\n",
      " 5190/10000: episode: 519, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5563.900 [1607.000, 9624.000],  loss: 4402.402966, mae: 111.342499, mean_q: 407.107303, mean_eps: 0.766698\n",
      " 5200/10000: episode: 520, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5191.400 [385.000, 8946.000],  loss: 3899.844971, mae: 109.571318, mean_q: 394.499619, mean_eps: 0.766248\n",
      " 5210/10000: episode: 521, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3989.700 [385.000, 9190.000],  loss: 3271.899304, mae: 115.016744, mean_q: 414.641962, mean_eps: 0.765798\n",
      " 5220/10000: episode: 522, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4587.600 [418.000, 8056.000],  loss: 4420.483508, mae: 108.773531, mean_q: 400.662576, mean_eps: 0.765348\n",
      " 5230/10000: episode: 523, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4000.000 [418.000, 9011.000],  loss: 5059.869751, mae: 114.360085, mean_q: 429.301566, mean_eps: 0.764898\n",
      " 5240/10000: episode: 524, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3864.500 [418.000, 8828.000],  loss: 3914.798450, mae: 119.674236, mean_q: 443.750302, mean_eps: 0.764447\n",
      " 5250/10000: episode: 525, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2738.000 [56.000, 7136.000],  loss: 3110.107526, mae: 112.288565, mean_q: 408.388181, mean_eps: 0.763997\n",
      " 5260/10000: episode: 526, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4947.400 [418.000, 9365.000],  loss: 3414.651392, mae: 131.150497, mean_q: 464.820914, mean_eps: 0.763547\n",
      " 5270/10000: episode: 527, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4816.400 [418.000, 9631.000],  loss: 3137.965326, mae: 124.863126, mean_q: 455.907751, mean_eps: 0.763098\n",
      " 5280/10000: episode: 528, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3068.900 [49.000, 9693.000],  loss: 5882.474799, mae: 125.448354, mean_q: 467.990149, mean_eps: 0.762647\n",
      " 5290/10000: episode: 529, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5916.800 [2014.000, 9583.000],  loss: 5060.489575, mae: 132.028356, mean_q: 494.954846, mean_eps: 0.762197\n",
      " 5300/10000: episode: 530, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2967.100 [418.000, 6812.000],  loss: 5398.605664, mae: 132.937703, mean_q: 500.045612, mean_eps: 0.761748\n",
      " 5310/10000: episode: 531, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3471.000 [339.000, 9094.000],  loss: 5191.723022, mae: 129.199223, mean_q: 469.071722, mean_eps: 0.761298\n",
      " 5320/10000: episode: 532, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6120.300 [1074.000, 9556.000],  loss: 4830.299878, mae: 140.133131, mean_q: 512.936816, mean_eps: 0.760848\n",
      " 5330/10000: episode: 533, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4242.500 [265.000, 6918.000],  loss: 5598.256128, mae: 137.181478, mean_q: 510.645959, mean_eps: 0.760397\n",
      " 5340/10000: episode: 534, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5287.300 [195.000, 9639.000],  loss: 4029.233783, mae: 133.538419, mean_q: 498.305704, mean_eps: 0.759947\n",
      " 5350/10000: episode: 535, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3233.400 [195.000, 7749.000],  loss: 4261.320685, mae: 137.318623, mean_q: 515.318524, mean_eps: 0.759497\n",
      " 5360/10000: episode: 536, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4030.800 [195.000, 9334.000],  loss: 5629.882129, mae: 145.393994, mean_q: 547.000385, mean_eps: 0.759047\n",
      " 5370/10000: episode: 537, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4006.900 [195.000, 9406.000],  loss: 5469.733215, mae: 146.315785, mean_q: 549.965100, mean_eps: 0.758597\n",
      " 5380/10000: episode: 538, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3684.100 [195.000, 9332.000],  loss: 5946.256335, mae: 141.266875, mean_q: 509.441910, mean_eps: 0.758147\n",
      " 5390/10000: episode: 539, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3512.200 [418.000, 8591.000],  loss: 4107.353229, mae: 147.617299, mean_q: 537.584912, mean_eps: 0.757697\n",
      " 5400/10000: episode: 540, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6818.400 [418.000, 9651.000],  loss: 5331.508667, mae: 144.039763, mean_q: 538.248221, mean_eps: 0.757247\n",
      " 5410/10000: episode: 541, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4675.900 [418.000, 8624.000],  loss: 6436.376990, mae: 159.249315, mean_q: 600.494672, mean_eps: 0.756798\n",
      " 5420/10000: episode: 542, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  6.000, mean reward:  0.600 [ 0.000,  1.000], mean action: 2594.100 [418.000, 9596.000],  loss: 6485.443585, mae: 159.628430, mean_q: 599.078601, mean_eps: 0.756348\n",
      " 5430/10000: episode: 543, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 3222.800 [418.000, 8345.000],  loss: 11440.669153, mae: 165.679160, mean_q: 611.577551, mean_eps: 0.755897\n",
      " 5440/10000: episode: 544, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2794.100 [276.000, 8110.000],  loss: 9739.844067, mae: 170.098244, mean_q: 596.811279, mean_eps: 0.755448\n",
      " 5450/10000: episode: 545, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4606.600 [414.000, 9176.000],  loss: 5466.477563, mae: 165.790491, mean_q: 574.636859, mean_eps: 0.754997\n",
      " 5460/10000: episode: 546, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4396.200 [414.000, 7825.000],  loss: 8765.198853, mae: 179.092168, mean_q: 623.964783, mean_eps: 0.754547\n",
      " 5470/10000: episode: 547, duration: 0.342s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5593.800 [324.000, 8964.000],  loss: 8257.103085, mae: 170.803632, mean_q: 615.621722, mean_eps: 0.754097\n",
      " 5480/10000: episode: 548, duration: 0.388s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3953.700 [55.000, 7442.000],  loss: 3928.449243, mae: 164.908279, mean_q: 603.318964, mean_eps: 0.753648\n",
      " 5490/10000: episode: 549, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5922.200 [505.000, 8939.000],  loss: 9790.980786, mae: 172.865764, mean_q: 634.499817, mean_eps: 0.753198\n",
      " 5500/10000: episode: 550, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5808.000 [2542.000, 9267.000],  loss: 10938.678687, mae: 170.610226, mean_q: 630.686853, mean_eps: 0.752747\n",
      " 5510/10000: episode: 551, duration: 0.361s, episode steps:  10, steps per second:  28, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 6503.200 [1707.000, 9408.000],  loss: 8641.105078, mae: 172.161494, mean_q: 681.889490, mean_eps: 0.752297\n",
      " 5520/10000: episode: 552, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5869.900 [1154.000, 9598.000],  loss: 9484.539502, mae: 172.524716, mean_q: 652.742493, mean_eps: 0.751848\n",
      " 5530/10000: episode: 553, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5135.900 [314.000, 9378.000],  loss: 9731.333984, mae: 181.515244, mean_q: 649.096613, mean_eps: 0.751398\n",
      " 5540/10000: episode: 554, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4552.200 [153.000, 8735.000],  loss: 4314.146411, mae: 177.450784, mean_q: 625.189008, mean_eps: 0.750947\n",
      " 5550/10000: episode: 555, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3820.900 [314.000, 7385.000],  loss: 5556.807703, mae: 183.456003, mean_q: 649.429156, mean_eps: 0.750497\n",
      " 5560/10000: episode: 556, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5964.300 [2302.000, 8988.000],  loss: 9593.297510, mae: 174.285463, mean_q: 619.540533, mean_eps: 0.750047\n",
      " 5570/10000: episode: 557, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3907.200 [1767.000, 9685.000],  loss: 12493.402710, mae: 195.345399, mean_q: 694.450311, mean_eps: 0.749597\n",
      " 5580/10000: episode: 558, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4304.200 [2428.000, 8246.000],  loss: 9030.329663, mae: 196.149304, mean_q: 695.848389, mean_eps: 0.749147\n",
      " 5590/10000: episode: 559, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4038.200 [836.000, 7344.000],  loss: 12311.722900, mae: 197.652208, mean_q: 698.878949, mean_eps: 0.748698\n",
      " 5600/10000: episode: 560, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3558.700 [247.000, 7520.000],  loss: 9893.852344, mae: 193.005080, mean_q: 670.495404, mean_eps: 0.748247\n",
      " 5610/10000: episode: 561, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4088.400 [1224.000, 9297.000],  loss: 8840.319604, mae: 194.494101, mean_q: 670.351550, mean_eps: 0.747797\n",
      " 5620/10000: episode: 562, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5168.600 [520.000, 8483.000],  loss: 7128.286768, mae: 200.507935, mean_q: 686.676190, mean_eps: 0.747347\n",
      " 5630/10000: episode: 563, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5378.800 [520.000, 8619.000],  loss: 5517.290869, mae: 206.946284, mean_q: 725.650244, mean_eps: 0.746897\n",
      " 5640/10000: episode: 564, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2569.300 [520.000, 7238.000],  loss: 6433.516785, mae: 203.651164, mean_q: 719.990961, mean_eps: 0.746448\n",
      " 5650/10000: episode: 565, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5477.700 [520.000, 9640.000],  loss: 10822.955127, mae: 214.737065, mean_q: 759.167139, mean_eps: 0.745997\n",
      " 5660/10000: episode: 566, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3681.900 [306.000, 8899.000],  loss: 8991.998901, mae: 208.061720, mean_q: 733.722711, mean_eps: 0.745548\n",
      " 5670/10000: episode: 567, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5635.600 [2398.000, 9703.000],  loss: 12264.436353, mae: 214.715103, mean_q: 756.696252, mean_eps: 0.745097\n",
      " 5680/10000: episode: 568, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3420.800 [520.000, 8351.000],  loss: 6806.724023, mae: 214.396745, mean_q: 756.472412, mean_eps: 0.744647\n",
      " 5690/10000: episode: 569, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4433.800 [520.000, 8824.000],  loss: 11084.655688, mae: 218.756250, mean_q: 756.119147, mean_eps: 0.744197\n",
      " 5700/10000: episode: 570, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4382.800 [418.000, 9696.000],  loss: 14294.982275, mae: 232.672559, mean_q: 794.328412, mean_eps: 0.743748\n",
      " 5710/10000: episode: 571, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3185.200 [418.000, 8585.000],  loss: 18376.290332, mae: 228.251274, mean_q: 783.880884, mean_eps: 0.743298\n",
      " 5720/10000: episode: 572, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6015.200 [418.000, 9489.000],  loss: 8031.116455, mae: 228.644873, mean_q: 792.313470, mean_eps: 0.742847\n",
      " 5730/10000: episode: 573, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4005.300 [35.000, 9728.000],  loss: 10765.842969, mae: 227.923334, mean_q: 798.243774, mean_eps: 0.742397\n",
      " 5740/10000: episode: 574, duration: 0.457s, episode steps:  10, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4492.900 [418.000, 9617.000],  loss: 19438.445117, mae: 246.126297, mean_q: 862.642896, mean_eps: 0.741947\n",
      " 5750/10000: episode: 575, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3067.500 [104.000, 9421.000],  loss: 15830.351562, mae: 245.231279, mean_q: 857.417822, mean_eps: 0.741498\n",
      " 5760/10000: episode: 576, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2783.800 [59.000, 8905.000],  loss: 9763.042712, mae: 239.547098, mean_q: 823.071924, mean_eps: 0.741047\n",
      " 5770/10000: episode: 577, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5351.500 [2377.000, 7365.000],  loss: 13101.741821, mae: 248.839752, mean_q: 871.062781, mean_eps: 0.740598\n",
      " 5780/10000: episode: 578, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5118.600 [931.000, 8045.000],  loss: 15026.660022, mae: 258.924031, mean_q: 916.692242, mean_eps: 0.740147\n",
      " 5790/10000: episode: 579, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 6597.200 [1678.000, 9665.000],  loss: 13250.766406, mae: 259.931595, mean_q: 921.212012, mean_eps: 0.739697\n",
      " 5800/10000: episode: 580, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5896.400 [30.000, 9354.000],  loss: 26847.105298, mae: 262.094472, mean_q: 909.849231, mean_eps: 0.739247\n",
      " 5810/10000: episode: 581, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3075.200 [418.000, 7842.000],  loss: 14613.018384, mae: 269.193802, mean_q: 929.398022, mean_eps: 0.738798\n",
      " 5820/10000: episode: 582, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2504.200 [418.000, 8404.000],  loss: 17087.806543, mae: 277.641989, mean_q: 979.514807, mean_eps: 0.738347\n",
      " 5830/10000: episode: 583, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4287.800 [418.000, 8198.000],  loss: 16372.427539, mae: 282.252441, mean_q: 988.456824, mean_eps: 0.737898\n",
      " 5840/10000: episode: 584, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4151.200 [418.000, 9675.000],  loss: 26432.121094, mae: 286.034853, mean_q: 977.398199, mean_eps: 0.737447\n",
      " 5850/10000: episode: 585, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4463.500 [418.000, 8469.000],  loss: 22439.715039, mae: 292.748788, mean_q: 994.275348, mean_eps: 0.736997\n",
      " 5860/10000: episode: 586, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 3632.400 [418.000, 9650.000],  loss: 26119.962793, mae: 308.260367, mean_q: 1049.563452, mean_eps: 0.736548\n",
      " 5870/10000: episode: 587, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5494.800 [418.000, 9541.000],  loss: 20354.312256, mae: 310.556177, mean_q: 1078.596149, mean_eps: 0.736097\n",
      " 5880/10000: episode: 588, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3639.900 [418.000, 7618.000],  loss: 16976.090601, mae: 300.935504, mean_q: 1043.762268, mean_eps: 0.735648\n",
      " 5890/10000: episode: 589, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4064.900 [418.000, 8583.000],  loss: 23985.340698, mae: 312.468741, mean_q: 1075.539502, mean_eps: 0.735197\n",
      " 5900/10000: episode: 590, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2840.400 [60.000, 9577.000],  loss: 24630.840918, mae: 295.571527, mean_q: 1007.464142, mean_eps: 0.734747\n",
      " 5910/10000: episode: 591, duration: 0.374s, episode steps:  10, steps per second:  27, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3461.900 [520.000, 8325.000],  loss: 27039.359375, mae: 313.588210, mean_q: 1049.183752, mean_eps: 0.734297\n",
      " 5920/10000: episode: 592, duration: 0.315s, episode steps:  10, steps per second:  32, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4896.700 [279.000, 8086.000],  loss: 18296.312109, mae: 324.740881, mean_q: 1069.787909, mean_eps: 0.733848\n",
      " 5930/10000: episode: 593, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3219.000 [279.000, 7412.000],  loss: 31944.506592, mae: 331.573596, mean_q: 1088.845264, mean_eps: 0.733398\n",
      " 5940/10000: episode: 594, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3406.500 [279.000, 7857.000],  loss: 31931.548438, mae: 332.292911, mean_q: 1072.954675, mean_eps: 0.732947\n",
      " 5950/10000: episode: 595, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4140.700 [365.000, 7018.000],  loss: 32678.444141, mae: 337.466910, mean_q: 1086.072650, mean_eps: 0.732497\n",
      " 5960/10000: episode: 596, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4023.600 [365.000, 9147.000],  loss: 27547.090723, mae: 311.814166, mean_q: 1010.194263, mean_eps: 0.732047\n",
      " 5970/10000: episode: 597, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2838.500 [365.000, 8871.000],  loss: 31339.072168, mae: 312.940802, mean_q: 1020.594641, mean_eps: 0.731598\n",
      " 5980/10000: episode: 598, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1754.900 [20.000, 8271.000],  loss: 24160.424756, mae: 332.632990, mean_q: 1088.721436, mean_eps: 0.731147\n",
      " 5990/10000: episode: 599, duration: 0.326s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4509.700 [245.000, 9224.000],  loss: 30278.324707, mae: 332.820810, mean_q: 1084.963708, mean_eps: 0.730697\n",
      " 6000/10000: episode: 600, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4749.400 [811.000, 8570.000],  loss: 29962.977637, mae: 332.244400, mean_q: 1066.510895, mean_eps: 0.730247\n",
      " 6010/10000: episode: 601, duration: 0.336s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5361.300 [626.000, 9682.000],  loss: 25558.870117, mae: 326.094583, mean_q: 1063.911401, mean_eps: 0.729797\n",
      " 6020/10000: episode: 602, duration: 0.422s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4941.600 [970.000, 8282.000],  loss: 18342.587793, mae: 310.462366, mean_q: 1030.419287, mean_eps: 0.729347\n",
      " 6030/10000: episode: 603, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4829.400 [1054.000, 8889.000],  loss: 23007.076416, mae: 324.420682, mean_q: 1081.559552, mean_eps: 0.728898\n",
      " 6040/10000: episode: 604, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3939.100 [264.000, 9114.000],  loss: 25136.803174, mae: 316.601514, mean_q: 1038.932880, mean_eps: 0.728448\n",
      " 6050/10000: episode: 605, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5451.500 [2463.000, 9246.000],  loss: 34438.945508, mae: 328.424454, mean_q: 1098.429932, mean_eps: 0.727998\n",
      " 6060/10000: episode: 606, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6029.400 [865.000, 9739.000],  loss: 25557.684863, mae: 330.966754, mean_q: 1090.432666, mean_eps: 0.727547\n",
      " 6070/10000: episode: 607, duration: 0.400s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2404.300 [248.000, 5638.000],  loss: 26566.004053, mae: 349.342566, mean_q: 1165.307251, mean_eps: 0.727097\n",
      " 6080/10000: episode: 608, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3257.300 [1371.000, 9451.000],  loss: 44158.107910, mae: 358.543793, mean_q: 1220.806458, mean_eps: 0.726648\n",
      " 6090/10000: episode: 609, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 3149.400 [542.000, 7790.000],  loss: 38277.201611, mae: 363.017404, mean_q: 1234.753137, mean_eps: 0.726197\n",
      " 6100/10000: episode: 610, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5832.500 [698.000, 9715.000],  loss: 41736.457422, mae: 360.687085, mean_q: 1255.526099, mean_eps: 0.725748\n",
      " 6110/10000: episode: 611, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5559.800 [477.000, 7546.000],  loss: 22710.549561, mae: 361.572101, mean_q: 1298.654041, mean_eps: 0.725297\n",
      " 6120/10000: episode: 612, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6199.000 [2697.000, 8706.000],  loss: 25959.277539, mae: 346.446997, mean_q: 1255.234216, mean_eps: 0.724847\n",
      " 6130/10000: episode: 613, duration: 0.267s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5992.900 [1297.000, 9351.000],  loss: 35607.417578, mae: 369.066794, mean_q: 1292.468323, mean_eps: 0.724398\n",
      " 6140/10000: episode: 614, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5880.400 [766.000, 9004.000],  loss: 26196.297119, mae: 377.100735, mean_q: 1255.043835, mean_eps: 0.723948\n",
      " 6150/10000: episode: 615, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2506.700 [140.000, 7991.000],  loss: 30834.349219, mae: 364.447876, mean_q: 1199.016907, mean_eps: 0.723498\n",
      " 6160/10000: episode: 616, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2741.300 [140.000, 6121.000],  loss: 37843.644629, mae: 369.568817, mean_q: 1214.076135, mean_eps: 0.723047\n",
      " 6170/10000: episode: 617, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5093.600 [1044.000, 9169.000],  loss: 46365.094336, mae: 381.186765, mean_q: 1262.170581, mean_eps: 0.722598\n",
      " 6180/10000: episode: 618, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4034.500 [140.000, 9153.000],  loss: 31143.768213, mae: 396.969608, mean_q: 1300.013818, mean_eps: 0.722147\n",
      " 6190/10000: episode: 619, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4948.700 [185.000, 9508.000],  loss: 30934.413916, mae: 374.679333, mean_q: 1219.923993, mean_eps: 0.721697\n",
      " 6200/10000: episode: 620, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 1139.800 [285.000, 3614.000],  loss: 36286.157227, mae: 408.029761, mean_q: 1320.290198, mean_eps: 0.721247\n",
      " 6210/10000: episode: 621, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2551.300 [10.000, 9164.000],  loss: 29790.159717, mae: 420.721167, mean_q: 1375.519604, mean_eps: 0.720797\n",
      " 6220/10000: episode: 622, duration: 0.327s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2354.900 [508.000, 8025.000],  loss: 35023.865918, mae: 392.657758, mean_q: 1295.376416, mean_eps: 0.720347\n",
      " 6230/10000: episode: 623, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3908.800 [689.000, 9470.000],  loss: 45388.778125, mae: 386.345538, mean_q: 1276.320947, mean_eps: 0.719897\n",
      " 6240/10000: episode: 624, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3009.000 [1333.000, 5644.000],  loss: 32032.146191, mae: 421.867502, mean_q: 1383.373657, mean_eps: 0.719448\n",
      " 6250/10000: episode: 625, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3145.400 [170.000, 7526.000],  loss: 49685.168311, mae: 421.326758, mean_q: 1373.374622, mean_eps: 0.718998\n",
      " 6260/10000: episode: 626, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2812.800 [70.000, 9652.000],  loss: 28886.020166, mae: 436.981265, mean_q: 1411.976038, mean_eps: 0.718548\n",
      " 6270/10000: episode: 627, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5245.300 [418.000, 8789.000],  loss: 37186.586914, mae: 450.564163, mean_q: 1461.714026, mean_eps: 0.718098\n",
      " 6280/10000: episode: 628, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4220.500 [171.000, 9568.000],  loss: 31476.359473, mae: 424.403979, mean_q: 1372.697632, mean_eps: 0.717647\n",
      " 6290/10000: episode: 629, duration: 0.245s, episode steps:  10, steps per second:  41, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3992.400 [418.000, 9131.000],  loss: 57972.376172, mae: 437.025104, mean_q: 1417.506055, mean_eps: 0.717197\n",
      " 6300/10000: episode: 630, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 2963.100 [416.000, 7734.000],  loss: 51024.662207, mae: 434.166211, mean_q: 1417.776587, mean_eps: 0.716747\n",
      " 6310/10000: episode: 631, duration: 0.290s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4389.900 [737.000, 8915.000],  loss: 48134.131738, mae: 424.717520, mean_q: 1388.878162, mean_eps: 0.716298\n",
      " 6320/10000: episode: 632, duration: 0.490s, episode steps:  10, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5251.900 [896.000, 8663.000],  loss: 56176.063281, mae: 470.959610, mean_q: 1546.890051, mean_eps: 0.715848\n",
      " 6330/10000: episode: 633, duration: 0.318s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5068.000 [24.000, 9719.000],  loss: 31471.318945, mae: 466.764313, mean_q: 1496.999792, mean_eps: 0.715397\n",
      " 6340/10000: episode: 634, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3638.900 [24.000, 8796.000],  loss: 41682.466406, mae: 446.456934, mean_q: 1422.799902, mean_eps: 0.714947\n",
      " 6350/10000: episode: 635, duration: 0.324s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4472.300 [14.000, 9161.000],  loss: 44522.796680, mae: 456.698874, mean_q: 1462.236438, mean_eps: 0.714498\n",
      " 6360/10000: episode: 636, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5353.900 [1979.000, 9510.000],  loss: 30494.596289, mae: 465.245605, mean_q: 1493.316870, mean_eps: 0.714048\n",
      " 6370/10000: episode: 637, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4705.200 [903.000, 9619.000],  loss: 55449.519141, mae: 456.572589, mean_q: 1465.889258, mean_eps: 0.713597\n",
      " 6380/10000: episode: 638, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4543.300 [485.000, 9708.000],  loss: 58082.080859, mae: 485.652570, mean_q: 1556.851611, mean_eps: 0.713147\n",
      " 6390/10000: episode: 639, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4373.300 [157.000, 8355.000],  loss: 37344.399707, mae: 467.130417, mean_q: 1463.388733, mean_eps: 0.712697\n",
      " 6400/10000: episode: 640, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2371.700 [157.000, 8574.000],  loss: 45805.202344, mae: 480.130829, mean_q: 1504.918103, mean_eps: 0.712247\n",
      " 6410/10000: episode: 641, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4059.300 [157.000, 9021.000],  loss: 54384.460547, mae: 489.584024, mean_q: 1529.186414, mean_eps: 0.711797\n",
      " 6420/10000: episode: 642, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3567.700 [157.000, 9251.000],  loss: 39222.553516, mae: 521.825031, mean_q: 1632.115515, mean_eps: 0.711348\n",
      " 6430/10000: episode: 643, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4607.500 [147.000, 8650.000],  loss: 57276.237012, mae: 491.361835, mean_q: 1552.818970, mean_eps: 0.710897\n",
      " 6440/10000: episode: 644, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1948.300 [0.000, 7635.000],  loss: 42221.612500, mae: 501.968439, mean_q: 1556.509314, mean_eps: 0.710447\n",
      " 6450/10000: episode: 645, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2184.200 [0.000, 6641.000],  loss: 43636.954883, mae: 494.343817, mean_q: 1537.554724, mean_eps: 0.709997\n",
      " 6460/10000: episode: 646, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4631.700 [0.000, 8763.000],  loss: 48549.530664, mae: 526.211536, mean_q: 1641.999902, mean_eps: 0.709547\n",
      " 6470/10000: episode: 647, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3238.100 [0.000, 9404.000],  loss: 71954.811133, mae: 478.864304, mean_q: 1511.117444, mean_eps: 0.709098\n",
      " 6480/10000: episode: 648, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4296.200 [0.000, 9607.000],  loss: 90181.344922, mae: 511.073358, mean_q: 1613.584045, mean_eps: 0.708647\n",
      " 6490/10000: episode: 649, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4183.800 [0.000, 9547.000],  loss: 49590.543945, mae: 525.820117, mean_q: 1656.852429, mean_eps: 0.708198\n",
      " 6500/10000: episode: 650, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4502.300 [66.000, 9719.000],  loss: 51291.624609, mae: 521.439731, mean_q: 1651.394556, mean_eps: 0.707747\n",
      " 6510/10000: episode: 651, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2399.100 [92.000, 5858.000],  loss: 40530.348242, mae: 512.177847, mean_q: 1584.589978, mean_eps: 0.707297\n",
      " 6520/10000: episode: 652, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5283.100 [92.000, 8626.000],  loss: 52220.687988, mae: 513.226672, mean_q: 1585.680151, mean_eps: 0.706847\n",
      " 6530/10000: episode: 653, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2310.800 [13.000, 6112.000],  loss: 93740.707812, mae: 542.947043, mean_q: 1704.256921, mean_eps: 0.706398\n",
      " 6540/10000: episode: 654, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1597.600 [156.000, 8939.000],  loss: 56473.328516, mae: 512.006363, mean_q: 1620.553381, mean_eps: 0.705948\n",
      " 6550/10000: episode: 655, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4213.300 [633.000, 8223.000],  loss: 64975.720313, mae: 529.444193, mean_q: 1644.181982, mean_eps: 0.705497\n",
      " 6560/10000: episode: 656, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2653.800 [157.000, 8758.000],  loss: 39439.753320, mae: 553.110095, mean_q: 1726.496814, mean_eps: 0.705047\n",
      " 6570/10000: episode: 657, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3220.600 [157.000, 9553.000],  loss: 70420.720898, mae: 579.443076, mean_q: 1798.359045, mean_eps: 0.704597\n",
      " 6580/10000: episode: 658, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2600.700 [0.000, 9717.000],  loss: 48684.544336, mae: 563.065671, mean_q: 1725.829236, mean_eps: 0.704148\n",
      " 6590/10000: episode: 659, duration: 0.342s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4140.000 [418.000, 8182.000],  loss: 86433.372266, mae: 580.005450, mean_q: 1800.145349, mean_eps: 0.703697\n",
      " 6600/10000: episode: 660, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4000.300 [224.000, 8542.000],  loss: 77001.756445, mae: 572.854645, mean_q: 1822.816626, mean_eps: 0.703248\n",
      " 6610/10000: episode: 661, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4084.400 [797.000, 9081.000],  loss: 50744.795410, mae: 557.328152, mean_q: 1784.864319, mean_eps: 0.702797\n",
      " 6620/10000: episode: 662, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 3705.100 [387.000, 7984.000],  loss: 72771.447656, mae: 575.545105, mean_q: 1844.497852, mean_eps: 0.702347\n",
      " 6630/10000: episode: 663, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4808.200 [587.000, 9105.000],  loss: 89028.138770, mae: 598.842926, mean_q: 1955.747559, mean_eps: 0.701897\n",
      " 6640/10000: episode: 664, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4211.900 [1711.000, 7997.000],  loss: 117442.039453, mae: 608.600470, mean_q: 1971.129919, mean_eps: 0.701448\n",
      " 6650/10000: episode: 665, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6089.600 [262.000, 9612.000],  loss: 62380.112305, mae: 604.751593, mean_q: 1885.935266, mean_eps: 0.700997\n",
      " 6660/10000: episode: 666, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5662.000 [593.000, 8394.000],  loss: 99480.357520, mae: 624.772119, mean_q: 1961.015527, mean_eps: 0.700547\n",
      " 6670/10000: episode: 667, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5567.500 [760.000, 9260.000],  loss: 81270.660937, mae: 639.673517, mean_q: 2009.405176, mean_eps: 0.700097\n",
      " 6680/10000: episode: 668, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6248.300 [1806.000, 9166.000],  loss: 74513.959375, mae: 612.570319, mean_q: 1921.718872, mean_eps: 0.699647\n",
      " 6690/10000: episode: 669, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4746.600 [1586.000, 8433.000],  loss: 102180.665430, mae: 637.766638, mean_q: 1986.343091, mean_eps: 0.699198\n",
      " 6700/10000: episode: 670, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5858.100 [1215.000, 9692.000],  loss: 87900.141992, mae: 640.644476, mean_q: 2032.399866, mean_eps: 0.698747\n",
      " 6710/10000: episode: 671, duration: 0.332s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3920.500 [1269.000, 8769.000],  loss: 73476.556836, mae: 624.928110, mean_q: 2034.579626, mean_eps: 0.698298\n",
      " 6720/10000: episode: 672, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3859.200 [1043.000, 8235.000],  loss: 81788.490234, mae: 587.155670, mean_q: 1905.512415, mean_eps: 0.697847\n",
      " 6730/10000: episode: 673, duration: 0.317s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3626.200 [2114.000, 5491.000],  loss: 99004.177539, mae: 616.359814, mean_q: 1894.493616, mean_eps: 0.697397\n",
      " 6740/10000: episode: 674, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3628.000 [710.000, 6873.000],  loss: 111925.114844, mae: 619.270093, mean_q: 1897.586841, mean_eps: 0.696947\n",
      " 6750/10000: episode: 675, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4885.300 [282.000, 9447.000],  loss: 101400.515234, mae: 642.055420, mean_q: 1996.905042, mean_eps: 0.696498\n",
      " 6760/10000: episode: 676, duration: 0.305s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4881.000 [773.000, 9272.000],  loss: 85080.720996, mae: 639.572168, mean_q: 2005.611877, mean_eps: 0.696048\n",
      " 6770/10000: episode: 677, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4901.300 [531.000, 8169.000],  loss: 64232.077930, mae: 657.611194, mean_q: 2064.802258, mean_eps: 0.695597\n",
      " 6780/10000: episode: 678, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3666.500 [235.000, 5860.000],  loss: 72109.020312, mae: 664.409723, mean_q: 2083.681250, mean_eps: 0.695147\n",
      " 6790/10000: episode: 679, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5894.900 [4137.000, 9285.000],  loss: 103643.981055, mae: 657.217456, mean_q: 2056.245776, mean_eps: 0.694697\n",
      " 6800/10000: episode: 680, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4131.300 [356.000, 8548.000],  loss: 128572.659570, mae: 692.482581, mean_q: 2160.338330, mean_eps: 0.694248\n",
      " 6810/10000: episode: 681, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 5763.000 [2274.000, 9551.000],  loss: 78670.561328, mae: 714.935626, mean_q: 2282.742139, mean_eps: 0.693797\n",
      " 6820/10000: episode: 682, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4170.600 [273.000, 8906.000],  loss: 113241.633594, mae: 695.501062, mean_q: 2270.108459, mean_eps: 0.693348\n",
      " 6830/10000: episode: 683, duration: 0.300s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4972.400 [1682.000, 8428.000],  loss: 88482.802734, mae: 706.548853, mean_q: 2316.485083, mean_eps: 0.692897\n",
      " 6840/10000: episode: 684, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5124.400 [502.000, 9714.000],  loss: 126795.757617, mae: 718.502802, mean_q: 2353.580029, mean_eps: 0.692447\n",
      " 6850/10000: episode: 685, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 4671.300 [2308.000, 8349.000],  loss: 134801.067383, mae: 701.554938, mean_q: 2296.474951, mean_eps: 0.691997\n",
      " 6860/10000: episode: 686, duration: 0.297s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5949.700 [1455.000, 9420.000],  loss: 81969.852148, mae: 742.539899, mean_q: 2462.330835, mean_eps: 0.691548\n",
      " 6870/10000: episode: 687, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4571.100 [3037.000, 9522.000],  loss: 137391.843359, mae: 757.208948, mean_q: 2383.108984, mean_eps: 0.691098\n",
      " 6880/10000: episode: 688, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4478.000 [156.000, 7979.000],  loss: 112556.118359, mae: 779.394897, mean_q: 2458.977612, mean_eps: 0.690648\n",
      " 6890/10000: episode: 689, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3084.100 [156.000, 9657.000],  loss: 128380.767969, mae: 808.711774, mean_q: 2556.746094, mean_eps: 0.690197\n",
      " 6900/10000: episode: 690, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4995.800 [565.000, 8950.000],  loss: 113718.926172, mae: 777.503717, mean_q: 2416.828271, mean_eps: 0.689747\n",
      " 6910/10000: episode: 691, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5469.500 [793.000, 9127.000],  loss: 113818.560742, mae: 714.374066, mean_q: 2222.969849, mean_eps: 0.689298\n",
      " 6920/10000: episode: 692, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3866.900 [3183.000, 6093.000],  loss: 143583.897656, mae: 789.227740, mean_q: 2446.096655, mean_eps: 0.688847\n",
      " 6930/10000: episode: 693, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4981.600 [1251.000, 8409.000],  loss: 94261.022070, mae: 789.033899, mean_q: 2421.622192, mean_eps: 0.688398\n",
      " 6940/10000: episode: 694, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3332.000 [199.000, 8483.000],  loss: 120017.740918, mae: 783.063574, mean_q: 2402.332532, mean_eps: 0.687947\n",
      " 6950/10000: episode: 695, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 6076.000 [594.000, 7128.000],  loss: 126585.885742, mae: 757.512683, mean_q: 2334.780042, mean_eps: 0.687497\n",
      " 6960/10000: episode: 696, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4095.400 [1422.000, 7128.000],  loss: 151997.855078, mae: 762.777423, mean_q: 2355.407788, mean_eps: 0.687047\n",
      " 6970/10000: episode: 697, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4854.000 [525.000, 9698.000],  loss: 128817.018555, mae: 807.910590, mean_q: 2457.432300, mean_eps: 0.686598\n",
      " 6980/10000: episode: 698, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3402.900 [510.000, 8620.000],  loss: 95942.441797, mae: 841.245764, mean_q: 2550.847070, mean_eps: 0.686148\n",
      " 6990/10000: episode: 699, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3062.400 [510.000, 9364.000],  loss: 97778.163867, mae: 817.963007, mean_q: 2505.167920, mean_eps: 0.685697\n",
      " 7000/10000: episode: 700, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4453.800 [510.000, 7703.000],  loss: 216120.776172, mae: 848.096844, mean_q: 2601.986084, mean_eps: 0.685247\n",
      " 7010/10000: episode: 701, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4940.500 [510.000, 9549.000],  loss: 109094.903906, mae: 832.263141, mean_q: 2549.926904, mean_eps: 0.684797\n",
      " 7020/10000: episode: 702, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4801.700 [510.000, 8410.000],  loss: 120520.582812, mae: 841.211481, mean_q: 2583.529053, mean_eps: 0.684347\n",
      " 7030/10000: episode: 703, duration: 0.300s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4814.200 [8.000, 9646.000],  loss: 200096.954492, mae: 889.776141, mean_q: 2652.697339, mean_eps: 0.683897\n",
      " 7040/10000: episode: 704, duration: 0.322s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2410.700 [418.000, 9669.000],  loss: 108145.719141, mae: 832.955994, mean_q: 2495.111548, mean_eps: 0.683447\n",
      " 7050/10000: episode: 705, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 1839.000 [63.000, 6943.000],  loss: 218711.486719, mae: 859.279633, mean_q: 2570.988184, mean_eps: 0.682997\n",
      " 7060/10000: episode: 706, duration: 0.310s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4510.200 [337.000, 9020.000],  loss: 121768.873828, mae: 910.065063, mean_q: 2716.405811, mean_eps: 0.682547\n",
      " 7070/10000: episode: 707, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4286.100 [337.000, 9036.000],  loss: 128378.303125, mae: 892.945105, mean_q: 2658.428906, mean_eps: 0.682098\n",
      " 7080/10000: episode: 708, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6897.900 [1274.000, 9488.000],  loss: 138324.587500, mae: 895.855090, mean_q: 2664.125073, mean_eps: 0.681648\n",
      " 7090/10000: episode: 709, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5345.200 [64.000, 8876.000],  loss: 133097.516797, mae: 896.721600, mean_q: 2662.862329, mean_eps: 0.681198\n",
      " 7100/10000: episode: 710, duration: 0.375s, episode steps:  10, steps per second:  27, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 3939.500 [444.000, 8030.000],  loss: 82534.258203, mae: 920.113135, mean_q: 2762.217896, mean_eps: 0.680748\n",
      " 7110/10000: episode: 711, duration: 0.337s, episode steps:  10, steps per second:  30, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2985.800 [178.000, 9391.000],  loss: 218144.643750, mae: 900.476758, mean_q: 2773.551294, mean_eps: 0.680297\n",
      " 7120/10000: episode: 712, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3834.700 [330.000, 8277.000],  loss: 172516.765625, mae: 941.076648, mean_q: 2923.300952, mean_eps: 0.679847\n",
      " 7130/10000: episode: 713, duration: 0.361s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2201.000 [444.000, 8198.000],  loss: 95264.969922, mae: 965.224475, mean_q: 3110.372412, mean_eps: 0.679397\n",
      " 7140/10000: episode: 714, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2575.800 [300.000, 7429.000],  loss: 190626.253906, mae: 985.171832, mean_q: 3013.996191, mean_eps: 0.678947\n",
      " 7150/10000: episode: 715, duration: 0.308s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2429.100 [620.000, 5623.000],  loss: 151544.800781, mae: 964.334277, mean_q: 2938.543823, mean_eps: 0.678498\n",
      " 7160/10000: episode: 716, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3674.500 [355.000, 8846.000],  loss: 114801.111523, mae: 928.853595, mean_q: 2823.591846, mean_eps: 0.678047\n",
      " 7170/10000: episode: 717, duration: 0.351s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3352.400 [355.000, 9688.000],  loss: 184415.258984, mae: 986.532190, mean_q: 2935.151196, mean_eps: 0.677597\n",
      " 7180/10000: episode: 718, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2503.300 [355.000, 9367.000],  loss: 258384.979102, mae: 960.469360, mean_q: 2865.840771, mean_eps: 0.677148\n",
      " 7190/10000: episode: 719, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2198.500 [337.000, 5796.000],  loss: 195726.811914, mae: 987.197375, mean_q: 2956.999219, mean_eps: 0.676698\n",
      " 7200/10000: episode: 720, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4152.800 [337.000, 9390.000],  loss: 233151.303906, mae: 1024.609637, mean_q: 3034.160767, mean_eps: 0.676248\n",
      " 7210/10000: episode: 721, duration: 0.297s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4547.500 [424.000, 9528.000],  loss: 180633.542969, mae: 1021.090930, mean_q: 2987.171362, mean_eps: 0.675797\n",
      " 7220/10000: episode: 722, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2824.100 [355.000, 8289.000],  loss: 227084.892969, mae: 1011.914862, mean_q: 2981.546240, mean_eps: 0.675347\n",
      " 7230/10000: episode: 723, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3551.800 [355.000, 8608.000],  loss: 162923.032031, mae: 969.541711, mean_q: 2889.168262, mean_eps: 0.674897\n",
      " 7240/10000: episode: 724, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3557.500 [355.000, 8870.000],  loss: 235006.332422, mae: 1007.695764, mean_q: 3009.759521, mean_eps: 0.674447\n",
      " 7250/10000: episode: 725, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2832.300 [280.000, 9664.000],  loss: 173807.508594, mae: 1040.989117, mean_q: 3102.062598, mean_eps: 0.673998\n",
      " 7260/10000: episode: 726, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5271.800 [1071.000, 9179.000],  loss: 143617.517187, mae: 1054.193597, mean_q: 3132.563135, mean_eps: 0.673547\n",
      " 7270/10000: episode: 727, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4594.200 [932.000, 8856.000],  loss: 200702.030469, mae: 1081.999249, mean_q: 3247.302563, mean_eps: 0.673097\n",
      " 7280/10000: episode: 728, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2906.700 [65.000, 8307.000],  loss: 198922.435938, mae: 1035.919934, mean_q: 3103.550903, mean_eps: 0.672647\n",
      " 7290/10000: episode: 729, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5194.400 [768.000, 8455.000],  loss: 230044.691992, mae: 1086.421460, mean_q: 3171.499438, mean_eps: 0.672198\n",
      " 7300/10000: episode: 730, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5536.500 [725.000, 9301.000],  loss: 199840.895313, mae: 1045.946063, mean_q: 3050.159717, mean_eps: 0.671748\n",
      " 7310/10000: episode: 731, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3300.500 [273.000, 9733.000],  loss: 134212.170703, mae: 1011.532379, mean_q: 2946.713770, mean_eps: 0.671297\n",
      " 7320/10000: episode: 732, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4697.700 [337.000, 8704.000],  loss: 226756.018750, mae: 1058.237402, mean_q: 3103.150488, mean_eps: 0.670848\n",
      " 7330/10000: episode: 733, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5484.000 [2676.000, 8213.000],  loss: 302790.426172, mae: 1154.944275, mean_q: 3401.040869, mean_eps: 0.670397\n",
      " 7340/10000: episode: 734, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5265.200 [543.000, 8981.000],  loss: 239627.095312, mae: 1091.103949, mean_q: 3211.826147, mean_eps: 0.669947\n",
      " 7350/10000: episode: 735, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5080.300 [1081.000, 9189.000],  loss: 255609.471094, mae: 1073.744385, mean_q: 3158.298413, mean_eps: 0.669497\n",
      " 7360/10000: episode: 736, duration: 0.479s, episode steps:  10, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6556.900 [1081.000, 9519.000],  loss: 210029.664844, mae: 1029.328857, mean_q: 3071.846436, mean_eps: 0.669048\n",
      " 7370/10000: episode: 737, duration: 0.320s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5210.700 [2789.000, 6769.000],  loss: 392811.706250, mae: 1156.350342, mean_q: 3521.867041, mean_eps: 0.668598\n",
      " 7380/10000: episode: 738, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5580.100 [2311.000, 9078.000],  loss: 224681.038281, mae: 1162.269983, mean_q: 3607.876880, mean_eps: 0.668147\n",
      " 7390/10000: episode: 739, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 5352.400 [115.000, 8764.000],  loss: 230679.322656, mae: 1152.842297, mean_q: 3625.263672, mean_eps: 0.667697\n",
      " 7400/10000: episode: 740, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6267.500 [1670.000, 9508.000],  loss: 384817.260156, mae: 1174.669806, mean_q: 3686.592310, mean_eps: 0.667247\n",
      " 7410/10000: episode: 741, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5600.100 [159.000, 9672.000],  loss: 302761.165625, mae: 1218.977344, mean_q: 3785.490088, mean_eps: 0.666798\n",
      " 7420/10000: episode: 742, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6552.300 [4218.000, 8814.000],  loss: 342197.423828, mae: 1220.110657, mean_q: 3915.426440, mean_eps: 0.666347\n",
      " 7430/10000: episode: 743, duration: 0.308s, episode steps:  10, steps per second:  33, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 5252.200 [954.000, 7033.000],  loss: 300870.138281, mae: 1218.199854, mean_q: 3957.184180, mean_eps: 0.665897\n",
      " 7440/10000: episode: 744, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5975.800 [2955.000, 6835.000],  loss: 340194.081250, mae: 1273.127905, mean_q: 4044.802271, mean_eps: 0.665447\n",
      " 7450/10000: episode: 745, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6009.900 [2661.000, 9518.000],  loss: 253024.324219, mae: 1294.524792, mean_q: 3891.160864, mean_eps: 0.664997\n",
      " 7460/10000: episode: 746, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5808.100 [1750.000, 9669.000],  loss: 321334.067188, mae: 1289.372095, mean_q: 3835.244971, mean_eps: 0.664547\n",
      " 7470/10000: episode: 747, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3019.100 [16.000, 8328.000],  loss: 247837.232422, mae: 1261.433020, mean_q: 3785.878125, mean_eps: 0.664098\n",
      " 7480/10000: episode: 748, duration: 0.282s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5791.500 [1081.000, 7609.000],  loss: 494802.544531, mae: 1262.098682, mean_q: 3862.902124, mean_eps: 0.663647\n",
      " 7490/10000: episode: 749, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2670.700 [1081.000, 8174.000],  loss: 290098.095313, mae: 1273.779358, mean_q: 3904.390942, mean_eps: 0.663197\n",
      " 7500/10000: episode: 750, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4218.000 [469.000, 9735.000],  loss: 288923.164062, mae: 1288.771106, mean_q: 3802.522607, mean_eps: 0.662747\n",
      " 7510/10000: episode: 751, duration: 0.300s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5364.100 [1818.000, 8801.000],  loss: 299478.642969, mae: 1276.653333, mean_q: 3809.312061, mean_eps: 0.662297\n",
      " 7520/10000: episode: 752, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4446.800 [242.000, 8737.000],  loss: 208822.429688, mae: 1345.360535, mean_q: 3915.592944, mean_eps: 0.661848\n",
      " 7530/10000: episode: 753, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3637.700 [443.000, 9001.000],  loss: 354221.053906, mae: 1371.642285, mean_q: 3948.007056, mean_eps: 0.661397\n",
      " 7540/10000: episode: 754, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4069.600 [384.000, 7640.000],  loss: 247240.437891, mae: 1307.677209, mean_q: 3757.902563, mean_eps: 0.660948\n",
      " 7550/10000: episode: 755, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3468.100 [486.000, 7127.000],  loss: 266625.178125, mae: 1348.185022, mean_q: 3880.738281, mean_eps: 0.660497\n",
      " 7560/10000: episode: 756, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3668.600 [191.000, 9681.000],  loss: 311133.703906, mae: 1385.323608, mean_q: 4009.755054, mean_eps: 0.660047\n",
      " 7570/10000: episode: 757, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1968.400 [460.000, 4422.000],  loss: 325627.010937, mae: 1411.766516, mean_q: 4095.116479, mean_eps: 0.659597\n",
      " 7580/10000: episode: 758, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2718.100 [1047.000, 6236.000],  loss: 303541.990625, mae: 1287.750171, mean_q: 3736.364014, mean_eps: 0.659148\n",
      " 7590/10000: episode: 759, duration: 0.331s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4255.100 [2014.000, 8825.000],  loss: 273892.715625, mae: 1331.811292, mean_q: 3861.140552, mean_eps: 0.658698\n",
      " 7600/10000: episode: 760, duration: 0.330s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1903.200 [307.000, 8597.000],  loss: 361159.975000, mae: 1410.758911, mean_q: 4075.787939, mean_eps: 0.658247\n",
      " 7610/10000: episode: 761, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2405.500 [307.000, 7858.000],  loss: 301406.113281, mae: 1344.136646, mean_q: 3932.229443, mean_eps: 0.657797\n",
      " 7620/10000: episode: 762, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2743.300 [307.000, 8878.000],  loss: 577896.328125, mae: 1398.632874, mean_q: 4118.530371, mean_eps: 0.657347\n",
      " 7630/10000: episode: 763, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3291.700 [307.000, 9564.000],  loss: 285467.382812, mae: 1400.858704, mean_q: 4033.085425, mean_eps: 0.656898\n",
      " 7640/10000: episode: 764, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1297.800 [445.000, 8225.000],  loss: 274765.814063, mae: 1500.983142, mean_q: 4279.739941, mean_eps: 0.656447\n",
      " 7650/10000: episode: 765, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5925.400 [445.000, 9294.000],  loss: 290528.366797, mae: 1458.632117, mean_q: 4166.227661, mean_eps: 0.655998\n",
      " 7660/10000: episode: 766, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4090.200 [839.000, 9015.000],  loss: 259706.559375, mae: 1408.745703, mean_q: 4032.637573, mean_eps: 0.655547\n",
      " 7670/10000: episode: 767, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3280.900 [274.000, 9317.000],  loss: 274334.531250, mae: 1422.966382, mean_q: 4073.480151, mean_eps: 0.655097\n",
      " 7680/10000: episode: 768, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3665.600 [762.000, 8556.000],  loss: 430228.462500, mae: 1336.100342, mean_q: 3820.750293, mean_eps: 0.654647\n",
      " 7690/10000: episode: 769, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3680.800 [350.000, 7994.000],  loss: 367962.726562, mae: 1454.557593, mean_q: 4167.643652, mean_eps: 0.654198\n",
      " 7700/10000: episode: 770, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 4594.800 [306.000, 9226.000],  loss: 293610.253125, mae: 1479.250403, mean_q: 4272.285352, mean_eps: 0.653748\n",
      " 7710/10000: episode: 771, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 5190.800 [1126.000, 8433.000],  loss: 480007.110938, mae: 1501.931567, mean_q: 4363.676611, mean_eps: 0.653298\n",
      " 7720/10000: episode: 772, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6216.800 [2550.000, 9602.000],  loss: 660620.785937, mae: 1571.162012, mean_q: 4733.173145, mean_eps: 0.652847\n",
      " 7730/10000: episode: 773, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4831.400 [562.000, 8122.000],  loss: 496642.048438, mae: 1621.746863, mean_q: 4966.322266, mean_eps: 0.652397\n",
      " 7740/10000: episode: 774, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5081.600 [1377.000, 8399.000],  loss: 463073.777344, mae: 1617.966711, mean_q: 5012.826807, mean_eps: 0.651948\n",
      " 7750/10000: episode: 775, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4998.200 [395.000, 9393.000],  loss: 455649.423438, mae: 1570.178101, mean_q: 4748.721924, mean_eps: 0.651497\n",
      " 7760/10000: episode: 776, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 3831.000 [143.000, 9072.000],  loss: 509136.828125, mae: 1590.779797, mean_q: 4732.763281, mean_eps: 0.651048\n",
      " 7770/10000: episode: 777, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5377.000 [3524.000, 8194.000],  loss: 361181.745312, mae: 1617.651807, mean_q: 4777.999512, mean_eps: 0.650597\n",
      " 7780/10000: episode: 778, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4741.700 [124.000, 8270.000],  loss: 488438.308594, mae: 1663.897205, mean_q: 4965.072998, mean_eps: 0.650147\n",
      " 7790/10000: episode: 779, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1730.100 [124.000, 4980.000],  loss: 438763.401953, mae: 1602.032751, mean_q: 4932.944287, mean_eps: 0.649697\n",
      " 7800/10000: episode: 780, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2690.900 [124.000, 8131.000],  loss: 348253.935937, mae: 1561.259851, mean_q: 4859.415234, mean_eps: 0.649248\n",
      " 7810/10000: episode: 781, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5235.000 [124.000, 9711.000],  loss: 281078.327344, mae: 1605.528516, mean_q: 5010.489551, mean_eps: 0.648798\n",
      " 7820/10000: episode: 782, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3859.000 [124.000, 9113.000],  loss: 368588.132812, mae: 1645.231421, mean_q: 5131.516455, mean_eps: 0.648347\n",
      " 7830/10000: episode: 783, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3501.900 [124.000, 8248.000],  loss: 592505.603125, mae: 1721.940125, mean_q: 5140.482471, mean_eps: 0.647897\n",
      " 7840/10000: episode: 784, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5079.500 [181.000, 8904.000],  loss: 372596.678906, mae: 1626.495544, mean_q: 4738.436572, mean_eps: 0.647447\n",
      " 7850/10000: episode: 785, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2567.400 [24.000, 8407.000],  loss: 512601.128125, mae: 1701.335913, mean_q: 4973.922266, mean_eps: 0.646998\n",
      " 7860/10000: episode: 786, duration: 0.345s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2017.300 [24.000, 8920.000],  loss: 371942.414062, mae: 1704.681567, mean_q: 5007.207617, mean_eps: 0.646547\n",
      " 7870/10000: episode: 787, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 1754.200 [24.000, 5019.000],  loss: 664709.910937, mae: 1713.500220, mean_q: 5040.932520, mean_eps: 0.646097\n",
      " 7880/10000: episode: 788, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1252.600 [24.000, 6650.000],  loss: 488923.162500, mae: 1681.426782, mean_q: 4958.875537, mean_eps: 0.645647\n",
      " 7890/10000: episode: 789, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2472.800 [24.000, 8999.000],  loss: 446683.410938, mae: 1845.606360, mean_q: 5480.765137, mean_eps: 0.645197\n",
      " 7900/10000: episode: 790, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4167.000 [24.000, 9462.000],  loss: 736498.010938, mae: 1668.462573, mean_q: 4857.769482, mean_eps: 0.644747\n",
      " 7910/10000: episode: 791, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4574.900 [183.000, 8644.000],  loss: 592839.915625, mae: 1750.896240, mean_q: 5003.136865, mean_eps: 0.644298\n",
      " 7920/10000: episode: 792, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3913.500 [55.000, 9432.000],  loss: 646361.437500, mae: 1801.541296, mean_q: 5166.464014, mean_eps: 0.643848\n",
      " 7930/10000: episode: 793, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2571.600 [55.000, 7335.000],  loss: 507743.289844, mae: 1734.517834, mean_q: 5016.494775, mean_eps: 0.643398\n",
      " 7940/10000: episode: 794, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3946.300 [55.000, 7551.000],  loss: 585305.553125, mae: 1772.429346, mean_q: 5070.112549, mean_eps: 0.642947\n",
      " 7950/10000: episode: 795, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4954.100 [990.000, 8628.000],  loss: 550216.215625, mae: 1909.118542, mean_q: 5416.513770, mean_eps: 0.642497\n",
      " 7960/10000: episode: 796, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4870.100 [1497.000, 7551.000],  loss: 484504.036719, mae: 1816.568506, mean_q: 5150.265918, mean_eps: 0.642047\n",
      " 7970/10000: episode: 797, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5709.700 [182.000, 8876.000],  loss: 482733.950781, mae: 1782.998633, mean_q: 5049.526416, mean_eps: 0.641597\n",
      " 7980/10000: episode: 798, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5215.800 [1721.000, 9623.000],  loss: 553042.190625, mae: 1775.295203, mean_q: 5027.828955, mean_eps: 0.641148\n",
      " 7990/10000: episode: 799, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4394.800 [658.000, 8492.000],  loss: 513591.439844, mae: 1888.479675, mean_q: 5315.095020, mean_eps: 0.640697\n",
      " 8000/10000: episode: 800, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4554.900 [939.000, 8710.000],  loss: 821667.284375, mae: 1884.748340, mean_q: 5203.876123, mean_eps: 0.640247\n",
      " 8010/10000: episode: 801, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4457.900 [1152.000, 9261.000],  loss: 601390.571875, mae: 1913.134338, mean_q: 5342.185498, mean_eps: 0.639798\n",
      " 8020/10000: episode: 802, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5293.000 [1602.000, 9169.000],  loss: 677171.814844, mae: 1847.077673, mean_q: 5144.436719, mean_eps: 0.639348\n",
      " 8030/10000: episode: 803, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5083.300 [166.000, 9205.000],  loss: 657647.007812, mae: 1882.216577, mean_q: 5210.197510, mean_eps: 0.638898\n",
      " 8040/10000: episode: 804, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4946.000 [166.000, 9623.000],  loss: 699469.485938, mae: 1814.548901, mean_q: 5020.691431, mean_eps: 0.638447\n",
      " 8050/10000: episode: 805, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5137.600 [845.000, 9419.000],  loss: 528501.412500, mae: 1904.332690, mean_q: 5304.661230, mean_eps: 0.637997\n",
      " 8060/10000: episode: 806, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5402.500 [48.000, 8491.000],  loss: 434771.989063, mae: 1885.791455, mean_q: 5284.992529, mean_eps: 0.637547\n",
      " 8070/10000: episode: 807, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5098.000 [3635.000, 7873.000],  loss: 419358.860938, mae: 1993.088403, mean_q: 5520.983789, mean_eps: 0.637097\n",
      " 8080/10000: episode: 808, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3450.400 [616.000, 4896.000],  loss: 490303.067188, mae: 2057.381824, mean_q: 5667.963721, mean_eps: 0.636647\n",
      " 8090/10000: episode: 809, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4808.400 [859.000, 8370.000],  loss: 724895.515625, mae: 2000.808911, mean_q: 5512.588721, mean_eps: 0.636197\n",
      " 8100/10000: episode: 810, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5533.800 [3379.000, 9577.000],  loss: 752707.746875, mae: 1966.361963, mean_q: 5414.513477, mean_eps: 0.635747\n",
      " 8110/10000: episode: 811, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4533.200 [510.000, 9280.000],  loss: 623621.110547, mae: 2084.138159, mean_q: 5733.855469, mean_eps: 0.635297\n",
      " 8120/10000: episode: 812, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4930.500 [510.000, 8472.000],  loss: 621338.293750, mae: 2045.550806, mean_q: 5627.369287, mean_eps: 0.634848\n",
      " 8130/10000: episode: 813, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3630.600 [156.000, 9001.000],  loss: 394325.929688, mae: 2027.305396, mean_q: 5564.228320, mean_eps: 0.634398\n",
      " 8140/10000: episode: 814, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2934.000 [156.000, 9511.000],  loss: 548041.061719, mae: 1986.677930, mean_q: 5489.715283, mean_eps: 0.633947\n",
      " 8150/10000: episode: 815, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2448.500 [20.000, 9354.000],  loss: 593991.412891, mae: 2009.023010, mean_q: 5690.673389, mean_eps: 0.633498\n",
      " 8160/10000: episode: 816, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3414.200 [513.000, 9492.000],  loss: 495619.883203, mae: 2057.245947, mean_q: 5879.988428, mean_eps: 0.633047\n",
      " 8170/10000: episode: 817, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5934.400 [820.000, 9674.000],  loss: 789377.075781, mae: 2010.483008, mean_q: 5758.193701, mean_eps: 0.632597\n",
      " 8180/10000: episode: 818, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3184.200 [820.000, 9259.000],  loss: 708270.593750, mae: 2005.084546, mean_q: 5741.645312, mean_eps: 0.632147\n",
      " 8190/10000: episode: 819, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3176.500 [166.000, 7888.000],  loss: 783753.239062, mae: 2094.000562, mean_q: 5819.498291, mean_eps: 0.631698\n",
      " 8200/10000: episode: 820, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3971.800 [10.000, 9233.000],  loss: 629300.900000, mae: 2081.195312, mean_q: 5797.406982, mean_eps: 0.631248\n",
      " 8210/10000: episode: 821, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3499.800 [113.000, 9541.000],  loss: 1204059.640625, mae: 2083.686511, mean_q: 5844.503906, mean_eps: 0.630797\n",
      " 8220/10000: episode: 822, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3400.600 [166.000, 9450.000],  loss: 967584.353125, mae: 2033.326917, mean_q: 5997.821240, mean_eps: 0.630347\n",
      " 8230/10000: episode: 823, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4134.500 [166.000, 9631.000],  loss: 859588.050000, mae: 2118.087561, mean_q: 6466.797412, mean_eps: 0.629897\n",
      " 8240/10000: episode: 824, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3221.600 [166.000, 8511.000],  loss: 1023008.496875, mae: 2126.489465, mean_q: 6553.656689, mean_eps: 0.629448\n",
      " 8250/10000: episode: 825, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2317.200 [166.000, 9176.000],  loss: 753608.653125, mae: 2108.741370, mean_q: 6372.695361, mean_eps: 0.628997\n",
      " 8260/10000: episode: 826, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2547.500 [166.000, 7088.000],  loss: 987935.200000, mae: 2216.426440, mean_q: 6492.704102, mean_eps: 0.628547\n",
      " 8270/10000: episode: 827, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2604.600 [166.000, 8235.000],  loss: 666630.056250, mae: 2274.043213, mean_q: 6480.111475, mean_eps: 0.628097\n",
      " 8280/10000: episode: 828, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3581.700 [166.000, 7220.000],  loss: 1164626.653125, mae: 2248.799097, mean_q: 6320.712061, mean_eps: 0.627647\n",
      " 8290/10000: episode: 829, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2987.500 [156.000, 8068.000],  loss: 581226.713672, mae: 2135.222742, mean_q: 5981.032080, mean_eps: 0.627197\n",
      " 8300/10000: episode: 830, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4140.500 [894.000, 9741.000],  loss: 930548.609375, mae: 2308.882263, mean_q: 6414.551123, mean_eps: 0.626748\n",
      " 8310/10000: episode: 831, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4487.500 [1905.000, 8878.000],  loss: 759169.062500, mae: 2331.783154, mean_q: 6471.151709, mean_eps: 0.626297\n",
      " 8320/10000: episode: 832, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 5432.500 [3035.000, 9193.000],  loss: 1005902.118750, mae: 2288.069727, mean_q: 6342.052637, mean_eps: 0.625847\n",
      " 8330/10000: episode: 833, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4759.700 [900.000, 9163.000],  loss: 656611.812109, mae: 2239.425073, mean_q: 6196.454443, mean_eps: 0.625397\n",
      " 8340/10000: episode: 834, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 3694.500 [578.000, 7879.000],  loss: 881712.400000, mae: 2469.578101, mean_q: 6837.776904, mean_eps: 0.624947\n",
      " 8350/10000: episode: 835, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5293.800 [109.000, 9390.000],  loss: 1164630.135938, mae: 2329.796973, mean_q: 6482.147754, mean_eps: 0.624498\n",
      " 8360/10000: episode: 836, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4098.200 [1374.000, 7028.000],  loss: 1213550.314062, mae: 2293.031250, mean_q: 6368.508154, mean_eps: 0.624047\n",
      " 8370/10000: episode: 837, duration: 0.493s, episode steps:  10, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4753.400 [2312.000, 9510.000],  loss: 891383.550000, mae: 2478.153564, mean_q: 6874.621045, mean_eps: 0.623598\n",
      " 8380/10000: episode: 838, duration: 0.367s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3622.000 [197.000, 6712.000],  loss: 1186132.896875, mae: 2417.458862, mean_q: 6757.199561, mean_eps: 0.623147\n",
      " 8390/10000: episode: 839, duration: 0.322s, episode steps:  10, steps per second:  31, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 4896.100 [2765.000, 8183.000],  loss: 1216750.467188, mae: 2543.044897, mean_q: 7356.141797, mean_eps: 0.622697\n",
      " 8400/10000: episode: 840, duration: 0.316s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5728.200 [3313.000, 8256.000],  loss: 945886.684375, mae: 2424.980127, mean_q: 6752.628760, mean_eps: 0.622247\n",
      " 8410/10000: episode: 841, duration: 0.333s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3628.500 [854.000, 9477.000],  loss: 1293169.434375, mae: 2519.254028, mean_q: 6916.973437, mean_eps: 0.621798\n",
      " 8420/10000: episode: 842, duration: 0.340s, episode steps:  10, steps per second:  29, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 4572.900 [563.000, 9522.000],  loss: 1305060.675000, mae: 2483.540942, mean_q: 6834.942529, mean_eps: 0.621348\n",
      " 8430/10000: episode: 843, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5021.800 [2036.000, 9458.000],  loss: 1104182.968750, mae: 2408.114502, mean_q: 6747.806396, mean_eps: 0.620897\n",
      " 8440/10000: episode: 844, duration: 0.342s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3930.800 [1902.000, 6547.000],  loss: 873146.793750, mae: 2624.241821, mean_q: 7585.427930, mean_eps: 0.620447\n",
      " 8450/10000: episode: 845, duration: 0.322s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4647.200 [3007.000, 9614.000],  loss: 1582080.831250, mae: 2620.522461, mean_q: 7768.746289, mean_eps: 0.619997\n",
      " 8460/10000: episode: 846, duration: 0.339s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3326.300 [1397.000, 4381.000],  loss: 1978481.475000, mae: 2756.698975, mean_q: 8273.059912, mean_eps: 0.619548\n",
      " 8470/10000: episode: 847, duration: 0.334s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3912.100 [1796.000, 8887.000],  loss: 1516559.043750, mae: 2535.706030, mean_q: 7611.964844, mean_eps: 0.619097\n",
      " 8480/10000: episode: 848, duration: 0.448s, episode steps:  10, steps per second:  22, episode reward:  4.000, mean reward:  0.400 [ 0.000,  1.000], mean action: 5146.500 [2839.000, 9551.000],  loss: 1860378.778125, mae: 2559.353735, mean_q: 7604.852979, mean_eps: 0.618648\n",
      " 8490/10000: episode: 849, duration: 0.316s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3073.300 [510.000, 9016.000],  loss: 1276740.875000, mae: 2650.891992, mean_q: 7410.171826, mean_eps: 0.618197\n",
      " 8500/10000: episode: 850, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2623.500 [214.000, 9506.000],  loss: 960420.340625, mae: 2644.099512, mean_q: 7437.309717, mean_eps: 0.617747\n",
      " 8510/10000: episode: 851, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 1953.000 [510.000, 5445.000],  loss: 1220147.234375, mae: 2702.957080, mean_q: 7694.875586, mean_eps: 0.617297\n",
      " 8520/10000: episode: 852, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3480.600 [204.000, 9031.000],  loss: 1626089.262500, mae: 2680.857373, mean_q: 7695.144385, mean_eps: 0.616848\n",
      " 8530/10000: episode: 853, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3101.000 [510.000, 7944.000],  loss: 1317878.021875, mae: 2822.076611, mean_q: 7817.756787, mean_eps: 0.616397\n",
      " 8540/10000: episode: 854, duration: 0.329s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3730.600 [134.000, 9270.000],  loss: 1024807.909375, mae: 2746.694849, mean_q: 7645.899756, mean_eps: 0.615947\n",
      " 8550/10000: episode: 855, duration: 0.310s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3613.100 [1273.000, 9105.000],  loss: 1342886.020312, mae: 2774.919678, mean_q: 7760.086768, mean_eps: 0.615497\n",
      " 8560/10000: episode: 856, duration: 0.334s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4617.300 [1521.000, 9688.000],  loss: 1300476.340625, mae: 2678.809131, mean_q: 7500.432617, mean_eps: 0.615047\n",
      " 8570/10000: episode: 857, duration: 0.346s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4753.600 [1521.000, 9655.000],  loss: 960561.593750, mae: 2779.289526, mean_q: 7780.163379, mean_eps: 0.614598\n",
      " 8580/10000: episode: 858, duration: 0.332s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4270.700 [1386.000, 9012.000],  loss: 1381298.454687, mae: 2848.774048, mean_q: 7964.647217, mean_eps: 0.614147\n",
      " 8590/10000: episode: 859, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2599.700 [407.000, 8065.000],  loss: 1585535.110937, mae: 2762.989404, mean_q: 7712.168604, mean_eps: 0.613698\n",
      " 8600/10000: episode: 860, duration: 0.282s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2913.800 [1521.000, 5497.000],  loss: 1807102.175000, mae: 2790.488208, mean_q: 7773.549854, mean_eps: 0.613247\n",
      " 8610/10000: episode: 861, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7200.700 [4197.000, 9544.000],  loss: 1300960.743750, mae: 2896.399219, mean_q: 7981.882471, mean_eps: 0.612797\n",
      " 8620/10000: episode: 862, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1694.200 [1193.000, 4076.000],  loss: 941302.881250, mae: 2871.114160, mean_q: 7933.216211, mean_eps: 0.612347\n",
      " 8630/10000: episode: 863, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 4456.900 [508.000, 8741.000],  loss: 1737927.687500, mae: 3026.694482, mean_q: 8298.858594, mean_eps: 0.611898\n",
      " 8640/10000: episode: 864, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3235.800 [84.000, 9540.000],  loss: 1960094.631250, mae: 3082.156934, mean_q: 8391.414062, mean_eps: 0.611448\n",
      " 8650/10000: episode: 865, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2515.900 [508.000, 7510.000],  loss: 1768468.706250, mae: 3079.469238, mean_q: 8376.567139, mean_eps: 0.610997\n",
      " 8660/10000: episode: 866, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1544.200 [352.000, 4513.000],  loss: 1773214.178125, mae: 3007.230396, mean_q: 8281.506201, mean_eps: 0.610547\n",
      " 8670/10000: episode: 867, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4277.600 [510.000, 9113.000],  loss: 1907016.700000, mae: 3016.021265, mean_q: 8357.739648, mean_eps: 0.610097\n",
      " 8680/10000: episode: 868, duration: 0.317s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1634.900 [28.000, 6150.000],  loss: 1727587.587500, mae: 3000.463892, mean_q: 8361.930127, mean_eps: 0.609648\n",
      " 8690/10000: episode: 869, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2580.800 [510.000, 6457.000],  loss: 1797918.425000, mae: 3164.984839, mean_q: 8834.790381, mean_eps: 0.609197\n",
      " 8700/10000: episode: 870, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4808.900 [510.000, 9736.000],  loss: 1717089.068750, mae: 2989.666064, mean_q: 8238.316992, mean_eps: 0.608747\n",
      " 8710/10000: episode: 871, duration: 0.340s, episode steps:  10, steps per second:  29, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3464.000 [508.000, 9689.000],  loss: 2256494.025000, mae: 3062.749194, mean_q: 8552.722461, mean_eps: 0.608297\n",
      " 8720/10000: episode: 872, duration: 0.463s, episode steps:  10, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2812.700 [508.000, 6115.000],  loss: 2649991.662500, mae: 3160.417822, mean_q: 8971.646582, mean_eps: 0.607847\n",
      " 8730/10000: episode: 873, duration: 0.368s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2762.800 [508.000, 5021.000],  loss: 2194613.543750, mae: 3140.330029, mean_q: 9018.704102, mean_eps: 0.607397\n",
      " 8740/10000: episode: 874, duration: 0.374s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4173.700 [508.000, 8535.000],  loss: 1634860.343750, mae: 3060.884229, mean_q: 8802.277783, mean_eps: 0.606948\n",
      " 8750/10000: episode: 875, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3444.800 [508.000, 8198.000],  loss: 2153087.081250, mae: 3398.818286, mean_q: 9974.739453, mean_eps: 0.606498\n",
      " 8760/10000: episode: 876, duration: 0.302s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3875.400 [508.000, 7592.000],  loss: 1932778.393750, mae: 3230.472095, mean_q: 9125.457520, mean_eps: 0.606048\n",
      " 8770/10000: episode: 877, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3211.700 [317.000, 8101.000],  loss: 2583123.025000, mae: 3426.895264, mean_q: 9493.735840, mean_eps: 0.605597\n",
      " 8780/10000: episode: 878, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4972.600 [1224.000, 9740.000],  loss: 2177138.359375, mae: 3319.349097, mean_q: 9166.988672, mean_eps: 0.605147\n",
      " 8790/10000: episode: 879, duration: 0.335s, episode steps:  10, steps per second:  30, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4926.400 [940.000, 9576.000],  loss: 1410523.265625, mae: 3282.015430, mean_q: 9104.361523, mean_eps: 0.604697\n",
      " 8800/10000: episode: 880, duration: 0.351s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4420.300 [1657.000, 9539.000],  loss: 1996978.609375, mae: 3337.430469, mean_q: 9471.640430, mean_eps: 0.604247\n",
      " 8810/10000: episode: 881, duration: 0.390s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2189.500 [508.000, 5773.000],  loss: 2214569.725000, mae: 3528.324414, mean_q: 9886.080469, mean_eps: 0.603798\n",
      " 8820/10000: episode: 882, duration: 0.386s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3195.400 [121.000, 8666.000],  loss: 1815217.525000, mae: 3276.237720, mean_q: 9209.576563, mean_eps: 0.603347\n",
      " 8830/10000: episode: 883, duration: 0.375s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4188.500 [183.000, 9627.000],  loss: 1467923.865625, mae: 3442.210083, mean_q: 9774.049707, mean_eps: 0.602897\n",
      " 8840/10000: episode: 884, duration: 0.361s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5584.400 [3916.000, 9695.000],  loss: 2070040.375000, mae: 3312.751904, mean_q: 9483.830273, mean_eps: 0.602448\n",
      " 8850/10000: episode: 885, duration: 0.323s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4663.300 [1136.000, 6273.000],  loss: 2314230.437500, mae: 3535.888818, mean_q: 10137.418945, mean_eps: 0.601998\n",
      " 8860/10000: episode: 886, duration: 0.374s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4222.200 [706.000, 8471.000],  loss: 2245791.050000, mae: 3484.792480, mean_q: 9534.041406, mean_eps: 0.601548\n",
      " 8870/10000: episode: 887, duration: 0.342s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3763.100 [510.000, 8817.000],  loss: 1940850.325000, mae: 3734.468652, mean_q: 10169.159668, mean_eps: 0.601097\n",
      " 8880/10000: episode: 888, duration: 0.372s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2548.600 [510.000, 7908.000],  loss: 2225564.800000, mae: 3649.348315, mean_q: 9992.205664, mean_eps: 0.600647\n",
      " 8890/10000: episode: 889, duration: 0.382s, episode steps:  10, steps per second:  26, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 3540.900 [510.000, 9329.000],  loss: 2207883.450000, mae: 3451.567554, mean_q: 9450.143945, mean_eps: 0.600197\n",
      " 8900/10000: episode: 890, duration: 0.395s, episode steps:  10, steps per second:  25, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5113.800 [510.000, 8913.000],  loss: 2420209.287500, mae: 3508.582935, mean_q: 9673.594531, mean_eps: 0.599747\n",
      " 8910/10000: episode: 891, duration: 0.383s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4567.800 [191.000, 8429.000],  loss: 2315307.643750, mae: 3707.600342, mean_q: 10326.723047, mean_eps: 0.599297\n",
      " 8920/10000: episode: 892, duration: 0.373s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4077.300 [1118.000, 7386.000],  loss: 2441567.312500, mae: 3746.835645, mean_q: 10458.570898, mean_eps: 0.598847\n",
      " 8930/10000: episode: 893, duration: 0.404s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4944.200 [2495.000, 5984.000],  loss: 2181132.278125, mae: 3595.975244, mean_q: 10029.211523, mean_eps: 0.598397\n",
      " 8940/10000: episode: 894, duration: 0.386s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6065.100 [4055.000, 9726.000],  loss: 1994627.350000, mae: 3702.937158, mean_q: 10305.818750, mean_eps: 0.597947\n",
      " 8950/10000: episode: 895, duration: 0.386s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7066.800 [3962.000, 9080.000],  loss: 2469654.593750, mae: 3555.452246, mean_q: 9872.279980, mean_eps: 0.597498\n",
      " 8960/10000: episode: 896, duration: 0.457s, episode steps:  10, steps per second:  22, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5208.400 [394.000, 8689.000],  loss: 2418742.453125, mae: 3583.640845, mean_q: 9925.298730, mean_eps: 0.597048\n",
      " 8970/10000: episode: 897, duration: 0.488s, episode steps:  10, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3212.700 [1932.000, 5699.000],  loss: 2829690.212500, mae: 3509.295728, mean_q: 9529.884766, mean_eps: 0.596598\n",
      " 8980/10000: episode: 898, duration: 0.356s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4328.000 [853.000, 9048.000],  loss: 3284302.725000, mae: 3773.947778, mean_q: 10225.013281, mean_eps: 0.596148\n",
      " 8990/10000: episode: 899, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3744.600 [252.000, 7477.000],  loss: 2404696.515625, mae: 3903.328442, mean_q: 10562.726953, mean_eps: 0.595697\n",
      " 9000/10000: episode: 900, duration: 0.392s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4629.100 [209.000, 9430.000],  loss: 2340063.525000, mae: 3895.194312, mean_q: 10526.745215, mean_eps: 0.595247\n",
      " 9010/10000: episode: 901, duration: 0.323s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2894.800 [563.000, 5825.000],  loss: 3663017.156250, mae: 3884.365186, mean_q: 10321.731885, mean_eps: 0.594797\n",
      " 9020/10000: episode: 902, duration: 0.314s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4493.500 [1664.000, 9671.000],  loss: 4106440.050000, mae: 3950.794385, mean_q: 10481.296191, mean_eps: 0.594347\n",
      " 9030/10000: episode: 903, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5403.400 [900.000, 9355.000],  loss: 2083738.537500, mae: 3976.078369, mean_q: 10562.549805, mean_eps: 0.593898\n",
      " 9040/10000: episode: 904, duration: 0.314s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2224.700 [900.000, 7838.000],  loss: 2372923.506250, mae: 3698.575562, mean_q: 9850.111133, mean_eps: 0.593447\n",
      " 9050/10000: episode: 905, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 5826.600 [531.000, 9296.000],  loss: 2666573.818750, mae: 4053.339990, mean_q: 10790.690527, mean_eps: 0.592997\n",
      " 9060/10000: episode: 906, duration: 0.330s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4750.100 [357.000, 9506.000],  loss: 3448950.012500, mae: 4067.719531, mean_q: 10827.357324, mean_eps: 0.592548\n",
      " 9070/10000: episode: 907, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4701.500 [90.000, 7466.000],  loss: 3302591.487500, mae: 4018.209155, mean_q: 10650.324121, mean_eps: 0.592098\n",
      " 9080/10000: episode: 908, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3874.000 [1333.000, 7466.000],  loss: 1918927.637500, mae: 3860.780322, mean_q: 10286.185352, mean_eps: 0.591647\n",
      " 9090/10000: episode: 909, duration: 0.419s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6065.800 [2151.000, 8456.000],  loss: 3568954.450000, mae: 4176.888770, mean_q: 11317.379395, mean_eps: 0.591197\n",
      " 9100/10000: episode: 910, duration: 0.396s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5767.900 [2274.000, 7829.000],  loss: 2354928.656250, mae: 4041.836938, mean_q: 11051.118164, mean_eps: 0.590747\n",
      " 9110/10000: episode: 911, duration: 0.349s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7100.400 [3195.000, 9434.000],  loss: 2697764.293750, mae: 4170.119312, mean_q: 11293.237305, mean_eps: 0.590297\n",
      " 9120/10000: episode: 912, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4854.200 [0.000, 8636.000],  loss: 3371230.037500, mae: 4236.276660, mean_q: 11411.604102, mean_eps: 0.589847\n",
      " 9130/10000: episode: 913, duration: 0.416s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5437.400 [1307.000, 8708.000],  loss: 2472098.912500, mae: 4282.707886, mean_q: 11610.808887, mean_eps: 0.589398\n",
      " 9140/10000: episode: 914, duration: 0.309s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5388.100 [91.000, 8552.000],  loss: 2770305.521875, mae: 4103.079419, mean_q: 11509.055078, mean_eps: 0.588947\n",
      " 9150/10000: episode: 915, duration: 0.326s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7174.000 [4622.000, 9499.000],  loss: 3919608.537500, mae: 4188.019775, mean_q: 11889.473926, mean_eps: 0.588497\n",
      " 9160/10000: episode: 916, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4867.700 [75.000, 8427.000],  loss: 3155396.625000, mae: 4252.535791, mean_q: 11805.308398, mean_eps: 0.588047\n",
      " 9170/10000: episode: 917, duration: 0.314s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5742.700 [3358.000, 8603.000],  loss: 4211815.025000, mae: 4402.338477, mean_q: 11922.533496, mean_eps: 0.587597\n",
      " 9180/10000: episode: 918, duration: 0.345s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5785.500 [3621.000, 6488.000],  loss: 2489141.937500, mae: 4395.144946, mean_q: 11912.172949, mean_eps: 0.587148\n",
      " 9190/10000: episode: 919, duration: 0.308s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5586.100 [375.000, 8856.000],  loss: 4241209.478125, mae: 4466.256787, mean_q: 11950.679590, mean_eps: 0.586697\n",
      " 9200/10000: episode: 920, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5038.100 [2223.000, 6835.000],  loss: 2829184.800000, mae: 4435.997925, mean_q: 11855.462305, mean_eps: 0.586248\n",
      " 9210/10000: episode: 921, duration: 0.310s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4650.100 [552.000, 9002.000],  loss: 3669987.687500, mae: 4276.938354, mean_q: 11418.547266, mean_eps: 0.585797\n",
      " 9220/10000: episode: 922, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5103.500 [706.000, 9515.000],  loss: 2929623.656250, mae: 4344.207300, mean_q: 11619.000586, mean_eps: 0.585347\n",
      " 9230/10000: episode: 923, duration: 0.305s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2537.900 [392.000, 9072.000],  loss: 2588225.462500, mae: 4312.357471, mean_q: 11677.145605, mean_eps: 0.584897\n",
      " 9240/10000: episode: 924, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3984.300 [706.000, 9227.000],  loss: 3443985.918750, mae: 4726.114551, mean_q: 12858.019824, mean_eps: 0.584448\n",
      " 9250/10000: episode: 925, duration: 0.321s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3495.000 [472.000, 9660.000],  loss: 2933072.200000, mae: 4605.826050, mean_q: 12545.855078, mean_eps: 0.583998\n",
      " 9260/10000: episode: 926, duration: 0.390s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3358.100 [706.000, 9239.000],  loss: 3389886.912500, mae: 4332.057031, mean_q: 11798.141992, mean_eps: 0.583547\n",
      " 9270/10000: episode: 927, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3308.200 [433.000, 9432.000],  loss: 3549669.137500, mae: 4535.266699, mean_q: 12355.587793, mean_eps: 0.583097\n",
      " 9280/10000: episode: 928, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1466.800 [283.000, 8737.000],  loss: 3768990.881250, mae: 4465.146533, mean_q: 12289.339551, mean_eps: 0.582647\n",
      " 9290/10000: episode: 929, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3889.700 [706.000, 8753.000],  loss: 2547010.025000, mae: 4480.365356, mean_q: 12219.724512, mean_eps: 0.582198\n",
      " 9300/10000: episode: 930, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2786.500 [908.000, 8524.000],  loss: 4510176.562500, mae: 4637.902295, mean_q: 12628.574512, mean_eps: 0.581747\n",
      " 9310/10000: episode: 931, duration: 0.399s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1711.600 [780.000, 5012.000],  loss: 3637600.112500, mae: 4538.970166, mean_q: 12335.606543, mean_eps: 0.581298\n",
      " 9320/10000: episode: 932, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4657.200 [545.000, 9574.000],  loss: 3611803.325000, mae: 4570.331543, mean_q: 12393.550098, mean_eps: 0.580847\n",
      " 9330/10000: episode: 933, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2740.400 [407.000, 8086.000],  loss: 4728011.962500, mae: 4639.998389, mean_q: 12552.737109, mean_eps: 0.580397\n",
      " 9340/10000: episode: 934, duration: 0.267s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3081.600 [490.000, 5627.000],  loss: 4350735.812500, mae: 4544.442456, mean_q: 12126.207129, mean_eps: 0.579947\n",
      " 9350/10000: episode: 935, duration: 0.360s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3925.300 [1977.000, 9188.000],  loss: 3559055.675000, mae: 4617.202588, mean_q: 12217.246191, mean_eps: 0.579498\n",
      " 9360/10000: episode: 936, duration: 0.323s, episode steps:  10, steps per second:  31, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3780.600 [484.000, 7951.000],  loss: 4208048.675000, mae: 4924.504785, mean_q: 13001.582129, mean_eps: 0.579047\n",
      " 9370/10000: episode: 937, duration: 0.353s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3462.300 [484.000, 9542.000],  loss: 4739953.850000, mae: 4891.117725, mean_q: 12884.960449, mean_eps: 0.578597\n",
      " 9380/10000: episode: 938, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3121.400 [484.000, 8344.000],  loss: 3071963.400000, mae: 4921.636963, mean_q: 12943.372559, mean_eps: 0.578147\n",
      " 9390/10000: episode: 939, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2882.900 [484.000, 8683.000],  loss: 3170393.812500, mae: 4975.265234, mean_q: 13062.532422, mean_eps: 0.577697\n",
      " 9400/10000: episode: 940, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7377.000 [1952.000, 8811.000],  loss: 2627299.950000, mae: 4737.376733, mean_q: 12436.132227, mean_eps: 0.577248\n",
      " 9410/10000: episode: 941, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6593.000 [1522.000, 9729.000],  loss: 2622940.518750, mae: 4402.044971, mean_q: 11556.051758, mean_eps: 0.576797\n",
      " 9420/10000: episode: 942, duration: 0.323s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5712.200 [484.000, 9689.000],  loss: 3366076.425000, mae: 5068.979028, mean_q: 13325.720020, mean_eps: 0.576348\n",
      " 9430/10000: episode: 943, duration: 0.340s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2436.300 [484.000, 8559.000],  loss: 3652282.575000, mae: 5075.793384, mean_q: 13526.709961, mean_eps: 0.575897\n",
      " 9440/10000: episode: 944, duration: 0.318s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3605.300 [484.000, 9296.000],  loss: 4044659.590625, mae: 4881.219116, mean_q: 13098.912012, mean_eps: 0.575447\n",
      " 9450/10000: episode: 945, duration: 0.418s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2607.500 [484.000, 7545.000],  loss: 3571333.500000, mae: 5293.267041, mean_q: 14521.167383, mean_eps: 0.574997\n",
      " 9460/10000: episode: 946, duration: 0.436s, episode steps:  10, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2269.800 [484.000, 8968.000],  loss: 4861372.462500, mae: 5293.393896, mean_q: 14097.231934, mean_eps: 0.574548\n",
      " 9470/10000: episode: 947, duration: 0.324s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3613.900 [141.000, 7241.000],  loss: 3930462.768750, mae: 5264.898340, mean_q: 13728.369238, mean_eps: 0.574098\n",
      " 9480/10000: episode: 948, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3926.200 [995.000, 7119.000],  loss: 4098044.087500, mae: 5167.193652, mean_q: 13472.250781, mean_eps: 0.573647\n",
      " 9490/10000: episode: 949, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4674.100 [1528.000, 7287.000],  loss: 3752886.112500, mae: 5328.483105, mean_q: 13880.282617, mean_eps: 0.573197\n",
      " 9500/10000: episode: 950, duration: 0.375s, episode steps:  10, steps per second:  27, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4465.800 [29.000, 7832.000],  loss: 4837359.543750, mae: 5300.719531, mean_q: 13824.224707, mean_eps: 0.572747\n",
      " 9510/10000: episode: 951, duration: 0.402s, episode steps:  10, steps per second:  25, episode reward:  6.000, mean reward:  0.600 [ 0.000,  1.000], mean action: 5664.600 [4909.000, 7793.000],  loss: 3394976.693750, mae: 5086.858643, mean_q: 13345.262598, mean_eps: 0.572298\n",
      " 9520/10000: episode: 952, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5409.100 [1122.000, 8973.000],  loss: 3952662.325000, mae: 5216.233740, mean_q: 13718.358105, mean_eps: 0.571847\n",
      " 9530/10000: episode: 953, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4166.400 [164.000, 8145.000],  loss: 3232313.825000, mae: 5289.671387, mean_q: 13750.135937, mean_eps: 0.571398\n",
      " 9540/10000: episode: 954, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1084.700 [23.000, 3627.000],  loss: 6363322.287500, mae: 5361.240771, mean_q: 13873.258301, mean_eps: 0.570947\n",
      " 9550/10000: episode: 955, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4375.800 [1710.000, 7995.000],  loss: 5410574.187500, mae: 5528.597900, mean_q: 14322.505664, mean_eps: 0.570497\n",
      " 9560/10000: episode: 956, duration: 0.409s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5153.600 [2157.000, 8919.000],  loss: 5577471.800000, mae: 5554.566650, mean_q: 14460.298242, mean_eps: 0.570047\n",
      " 9570/10000: episode: 957, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4995.600 [1212.000, 9115.000],  loss: 3935890.100000, mae: 5379.223242, mean_q: 13813.838477, mean_eps: 0.569598\n",
      " 9580/10000: episode: 958, duration: 0.362s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3882.100 [1479.000, 9149.000],  loss: 6609219.700000, mae: 5557.870996, mean_q: 14235.640527, mean_eps: 0.569148\n",
      " 9590/10000: episode: 959, duration: 0.306s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5050.600 [333.000, 8817.000],  loss: 4173900.395313, mae: 5296.920508, mean_q: 13561.619824, mean_eps: 0.568698\n",
      " 9600/10000: episode: 960, duration: 0.345s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4757.400 [517.000, 8423.000],  loss: 6814345.875000, mae: 5548.761865, mean_q: 14219.407520, mean_eps: 0.568247\n",
      " 9610/10000: episode: 961, duration: 0.361s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6300.000 [499.000, 9441.000],  loss: 3748570.962500, mae: 5655.068994, mean_q: 14487.954980, mean_eps: 0.567797\n",
      " 9620/10000: episode: 962, duration: 0.346s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5430.500 [314.000, 9375.000],  loss: 3475285.750000, mae: 5912.238770, mean_q: 15134.660547, mean_eps: 0.567348\n",
      " 9630/10000: episode: 963, duration: 0.356s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3607.100 [314.000, 9702.000],  loss: 5364771.246875, mae: 5702.517920, mean_q: 14606.911914, mean_eps: 0.566897\n",
      " 9640/10000: episode: 964, duration: 0.348s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3217.900 [122.000, 6531.000],  loss: 3644239.612500, mae: 5505.988184, mean_q: 14113.126074, mean_eps: 0.566448\n",
      " 9650/10000: episode: 965, duration: 0.338s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3593.000 [314.000, 9558.000],  loss: 6049406.212500, mae: 5616.971240, mean_q: 14646.925195, mean_eps: 0.565997\n",
      " 9660/10000: episode: 966, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  5.000, mean reward:  0.500 [ 0.000,  1.000], mean action: 2256.000 [314.000, 9367.000],  loss: 4034246.971875, mae: 5548.714258, mean_q: 14611.498047, mean_eps: 0.565547\n",
      " 9670/10000: episode: 967, duration: 0.326s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1600.700 [72.000, 7971.000],  loss: 5842121.475000, mae: 5667.122217, mean_q: 15002.117480, mean_eps: 0.565097\n",
      " 9680/10000: episode: 968, duration: 0.336s, episode steps:  10, steps per second:  30, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2519.700 [314.000, 8268.000],  loss: 4267307.068750, mae: 5623.052881, mean_q: 14479.697949, mean_eps: 0.564648\n",
      " 9690/10000: episode: 969, duration: 0.320s, episode steps:  10, steps per second:  31, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4559.000 [719.000, 8741.000],  loss: 6191060.200000, mae: 5717.593701, mean_q: 14748.990137, mean_eps: 0.564198\n",
      " 9700/10000: episode: 970, duration: 0.336s, episode steps:  10, steps per second:  30, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3652.200 [150.000, 9534.000],  loss: 5105561.425000, mae: 5748.517236, mean_q: 14851.688770, mean_eps: 0.563747\n",
      " 9710/10000: episode: 971, duration: 0.363s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3652.200 [703.000, 9535.000],  loss: 5258248.650000, mae: 5616.990234, mean_q: 14468.371973, mean_eps: 0.563297\n",
      " 9720/10000: episode: 972, duration: 0.340s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7835.000 [5577.000, 8158.000],  loss: 6055926.600000, mae: 5854.921240, mean_q: 14975.126270, mean_eps: 0.562847\n",
      " 9730/10000: episode: 973, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6566.200 [1496.000, 9284.000],  loss: 5883718.987500, mae: 5612.640039, mean_q: 14378.014844, mean_eps: 0.562397\n",
      " 9740/10000: episode: 974, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5593.400 [904.000, 8453.000],  loss: 6576804.275000, mae: 5956.283350, mean_q: 15145.119922, mean_eps: 0.561947\n",
      " 9750/10000: episode: 975, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6001.200 [1166.000, 9710.000],  loss: 5250910.512500, mae: 5887.164404, mean_q: 14927.742480, mean_eps: 0.561497\n",
      " 9760/10000: episode: 976, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3522.600 [418.000, 7490.000],  loss: 5768047.650000, mae: 5992.999463, mean_q: 15172.213477, mean_eps: 0.561047\n",
      " 9770/10000: episode: 977, duration: 0.362s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3589.000 [445.000, 9735.000],  loss: 5643668.318750, mae: 6013.927979, mean_q: 15418.643457, mean_eps: 0.560597\n",
      " 9780/10000: episode: 978, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2752.200 [1496.000, 9136.000],  loss: 7044880.425000, mae: 5866.136133, mean_q: 15373.535938, mean_eps: 0.560148\n",
      " 9790/10000: episode: 979, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4916.200 [1137.000, 9173.000],  loss: 5739719.900000, mae: 5672.744824, mean_q: 14838.055078, mean_eps: 0.559698\n",
      " 9800/10000: episode: 980, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5626.500 [1123.000, 9380.000],  loss: 5117931.943750, mae: 5980.094385, mean_q: 15555.906738, mean_eps: 0.559248\n",
      " 9810/10000: episode: 981, duration: 0.309s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6111.600 [15.000, 8158.000],  loss: 6147991.575000, mae: 6126.347510, mean_q: 16101.222656, mean_eps: 0.558798\n",
      " 9820/10000: episode: 982, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4584.100 [280.000, 9045.000],  loss: 4917636.131250, mae: 5991.269922, mean_q: 15788.196875, mean_eps: 0.558347\n",
      " 9830/10000: episode: 983, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7222.700 [1961.000, 8346.000],  loss: 6243206.537500, mae: 5895.724072, mean_q: 15506.875488, mean_eps: 0.557897\n",
      " 9840/10000: episode: 984, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7207.800 [927.000, 9541.000],  loss: 7820049.550000, mae: 6460.613770, mean_q: 16754.195215, mean_eps: 0.557447\n",
      " 9850/10000: episode: 985, duration: 0.470s, episode steps:  10, steps per second:  21, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3343.300 [140.000, 9541.000],  loss: 6439043.687500, mae: 6315.126221, mean_q: 16235.601758, mean_eps: 0.556997\n",
      " 9860/10000: episode: 986, duration: 0.436s, episode steps:  10, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6065.500 [987.000, 8184.000],  loss: 6027157.950000, mae: 6153.227344, mean_q: 15855.422266, mean_eps: 0.556548\n",
      " 9870/10000: episode: 987, duration: 0.353s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6304.100 [2432.000, 8568.000],  loss: 6104956.237500, mae: 6230.246484, mean_q: 16090.565625, mean_eps: 0.556097\n",
      " 9880/10000: episode: 988, duration: 0.364s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6642.500 [2556.000, 9541.000],  loss: 5452066.850000, mae: 6426.733691, mean_q: 16483.566602, mean_eps: 0.555647\n",
      " 9890/10000: episode: 989, duration: 0.474s, episode steps:  10, steps per second:  21, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6192.100 [168.000, 9541.000],  loss: 8127127.500000, mae: 6212.421924, mean_q: 15962.582910, mean_eps: 0.555198\n",
      " 9900/10000: episode: 990, duration: 0.414s, episode steps:  10, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 8612.600 [2422.000, 9541.000],  loss: 5527404.700000, mae: 6520.432471, mean_q: 17327.079102, mean_eps: 0.554748\n",
      " 9910/10000: episode: 991, duration: 0.358s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 8876.300 [6818.000, 9541.000],  loss: 5674216.575000, mae: 6292.318359, mean_q: 16936.046582, mean_eps: 0.554297\n",
      " 9920/10000: episode: 992, duration: 0.341s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6968.100 [1173.000, 9541.000],  loss: 6195604.225000, mae: 6664.931738, mean_q: 17837.482617, mean_eps: 0.553847\n",
      " 9930/10000: episode: 993, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5868.400 [140.000, 9541.000],  loss: 5851744.337500, mae: 6474.884570, mean_q: 16735.708008, mean_eps: 0.553397\n",
      " 9940/10000: episode: 994, duration: 0.372s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1983.800 [140.000, 6531.000],  loss: 5040096.181250, mae: 6528.288477, mean_q: 16823.087305, mean_eps: 0.552947\n",
      " 9950/10000: episode: 995, duration: 0.310s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2947.600 [140.000, 8770.000],  loss: 5434119.643750, mae: 6518.478711, mean_q: 16838.043848, mean_eps: 0.552497\n",
      " 9960/10000: episode: 996, duration: 0.320s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2930.400 [140.000, 8298.000],  loss: 6411070.250000, mae: 6890.588428, mean_q: 17754.284961, mean_eps: 0.552047\n",
      " 9970/10000: episode: 997, duration: 0.325s, episode steps:  10, steps per second:  31, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4388.900 [360.000, 9080.000],  loss: 4258935.775000, mae: 6437.741357, mean_q: 16457.981445, mean_eps: 0.551597\n",
      " 9980/10000: episode: 998, duration: 0.341s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2641.500 [235.000, 7490.000],  loss: 4142455.200000, mae: 6526.511230, mean_q: 16676.107910, mean_eps: 0.551147\n",
      " 9990/10000: episode: 999, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4136.700 [20.000, 9723.000],  loss: 6422616.237500, mae: 6510.932129, mean_q: 16619.631738, mean_eps: 0.550697\n",
      " 10000/10000: episode: 1000, duration: 0.306s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3159.800 [20.000, 8166.000],  loss: 6553323.550000, mae: 6019.218311, mean_q: 15340.713477, mean_eps: 0.550248\n",
      "done, took 279.285 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146e4dee0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_dqn(dqn, env, top_k=10, num_users=100):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    ndcg_list = []\n",
    "    map_list = []\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    for _ in range(num_users):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_recommended = []\n",
    "        total_liked = list(env.liked_movies)\n",
    "\n",
    "        recommended_set = set()\n",
    "\n",
    "        while not done:\n",
    "            # Get Q-values\n",
    "            q_values = dqn.compute_q_values(np.expand_dims(obs, axis=0))\n",
    "            q_values = q_values[0]  # shape (num_actions,)\n",
    "\n",
    "            # Pick top_k movies (avoid recommending same movie twice)\n",
    "            ranked_movie_indices = np.argsort(q_values)[::-1]\n",
    "            recommended = []\n",
    "            for idx in ranked_movie_indices:\n",
    "                movie_id = env.movie_ids[idx]\n",
    "                if movie_id not in recommended_set:\n",
    "                    recommended.append(movie_id)\n",
    "                    recommended_set.add(movie_id)\n",
    "                if len(recommended) == top_k:\n",
    "                    break\n",
    "\n",
    "            total_recommended.extend(recommended)\n",
    "\n",
    "            # We only need one step to evaluate, no full rollout\n",
    "            done = True\n",
    "\n",
    "        # --- Metrics ---\n",
    "        # True positives\n",
    "        hits = [1 if r in total_liked else 0 for r in total_recommended]\n",
    "\n",
    "        # Precision@k\n",
    "        precision = np.sum(hits) / top_k\n",
    "        precision_list.append(precision)\n",
    "\n",
    "        # Recall@k\n",
    "        recall = np.sum(hits) / max(len(total_liked), 1)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "        # NDCG@k\n",
    "        dcg = np.sum([\n",
    "            hit / np.log2(idx + 2) for idx, hit in enumerate(hits)\n",
    "        ])\n",
    "        ideal_hits = sorted([1] * min(len(total_liked), top_k) + [0] * (top_k - min(len(total_liked), top_k)), reverse=True)\n",
    "        idcg = np.sum([\n",
    "            hit / np.log2(idx + 2) for idx, hit in enumerate(ideal_hits)\n",
    "        ])\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_list.append(ndcg)\n",
    "\n",
    "        # MAP@k\n",
    "        num_hits = 0\n",
    "        ap_sum = 0.0\n",
    "        for idx, hit in enumerate(hits):\n",
    "            if hit == 1:\n",
    "                num_hits += 1\n",
    "                ap_sum += num_hits / (idx + 1)\n",
    "        map_at_k = ap_sum / max(np.sum(hits), 1)\n",
    "        map_list.append(map_at_k)\n",
    "\n",
    "    # Aggregate results safely\n",
    "    precision_final = np.nanmean(precision_list)\n",
    "    recall_final = np.nanmean(recall_list)\n",
    "    ndcg_final = np.nanmean(ndcg_list)\n",
    "    map_final = np.nanmean(map_list)\n",
    "\n",
    "    print(f\"Precision@{top_k}: {precision_final:.4f}\")\n",
    "    print(f\"Recall@{top_k}: {recall_final:.4f}\")\n",
    "    print(f\"NDCG@{top_k}: {ndcg_final:.4f}\")\n",
    "    print(f\"MAP@{top_k}: {map_final:.4f}\")\n",
    "\n",
    "    return precision_final, recall_final, ndcg_final, map_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0290\n",
      "Recall@10: 0.0045\n",
      "NDCG@10: 0.0638\n",
      "MAP@10: 0.2900\n",
      "Final DQN Performance:\n",
      " Precision@10 : 2.90%\n",
      " Recall@10    : 0.45%\n",
      " NDCG@10      : 6.38%\n",
      " MAP@10       : 29.00%\n"
     ]
    }
   ],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')\n",
    "precision, recall, ndcg, map_k = evaluate_dqn(dqn, env, top_k=10, num_users=100)\n",
    "\n",
    "print(f\"Final DQN Performance:\")\n",
    "print(f\" Precision@10 : {precision*100:.2f}%\")\n",
    "print(f\" Recall@10    : {recall*100:.2f}%\")\n",
    "print(f\" NDCG@10      : {ndcg*100:.2f}%\")\n",
    "print(f\" MAP@10       : {map_k*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Stronger DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "\n",
    "def build_stronger_dqn(state_size, nb_actions):\n",
    "    inputs = Input(shape=(1, state_size))  # Keras-RL expects window_length=1\n",
    "    x = Flatten()(inputs)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(nb_actions, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:02:39.219635: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_90_1/bias/Assign' id:15615 op device:{requested: '', assigned: ''} def:{{{node dense_90_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_90_1/bias, dense_90_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model2 = build_stronger_dqn(\n",
    "    state_size=env.state_size,\n",
    "    nb_actions=len(env.movie_ids)\n",
    ")\n",
    "\n",
    "# Configure agent\n",
    "memory2 = SequentialMemory(limit=50000, window_length=1)\n",
    "policy2 = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.05,\n",
    "                              value_test=0.01,\n",
    "                              nb_steps=50000)\n",
    "\n",
    "dqn2 = DQNAgent(\n",
    "    model=model2,\n",
    "    nb_actions=len(env.movie_ids),\n",
    "    memory=memory2,\n",
    "    nb_steps_warmup=500,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy2,\n",
    "    enable_double_dqn=True\n",
    ")\n",
    "\n",
    "optimizer2 = Adam(learning_rate=5e-4, clipnorm=1.0)\n",
    "dqn2.compile(optimizer=optimizer2, metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ddpg/lib/python3.9/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2025-04-27 19:02:42.382834: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_91/BiasAdd' id:15546 op device:{requested: '', assigned: ''} def:{{{node dense_91/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_91/MatMul, dense_91/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-04-27 19:02:42.876378: W tensorflow/c/c_api.cc:291] Operation '{name:'count_93/Assign' id:15814 op device:{requested: '', assigned: ''} def:{{{node count_93/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_93, count_93/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10/5000: episode: 1, duration: 1.129s, episode steps:  10, steps per second:   9, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5668.600 [1656.000, 8960.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20/5000: episode: 2, duration: 0.015s, episode steps:  10, steps per second: 653, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6819.200 [1858.000, 9004.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30/5000: episode: 3, duration: 0.016s, episode steps:  10, steps per second: 642, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5125.300 [864.000, 8621.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40/5000: episode: 4, duration: 0.015s, episode steps:  10, steps per second: 657, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4339.500 [1068.000, 8311.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   50/5000: episode: 5, duration: 0.015s, episode steps:  10, steps per second: 673, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3353.300 [287.000, 9718.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   60/5000: episode: 6, duration: 0.015s, episode steps:  10, steps per second: 663, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6153.100 [512.000, 9559.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   70/5000: episode: 7, duration: 0.016s, episode steps:  10, steps per second: 639, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3763.600 [231.000, 7261.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   80/5000: episode: 8, duration: 0.016s, episode steps:  10, steps per second: 644, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4832.900 [1017.000, 8919.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   90/5000: episode: 9, duration: 0.016s, episode steps:  10, steps per second: 611, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4974.700 [325.000, 8292.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  100/5000: episode: 10, duration: 0.016s, episode steps:  10, steps per second: 614, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3924.500 [910.000, 6177.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  110/5000: episode: 11, duration: 0.016s, episode steps:  10, steps per second: 617, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5890.800 [546.000, 9654.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  120/5000: episode: 12, duration: 0.016s, episode steps:  10, steps per second: 631, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6315.800 [2351.000, 9527.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  130/5000: episode: 13, duration: 0.016s, episode steps:  10, steps per second: 630, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5043.900 [2501.000, 8458.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  140/5000: episode: 14, duration: 0.017s, episode steps:  10, steps per second: 592, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4627.800 [493.000, 7449.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  150/5000: episode: 15, duration: 0.023s, episode steps:  10, steps per second: 442, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4846.800 [319.000, 9562.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  160/5000: episode: 16, duration: 0.020s, episode steps:  10, steps per second: 500, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5906.200 [179.000, 8795.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  170/5000: episode: 17, duration: 0.017s, episode steps:  10, steps per second: 575, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4112.500 [183.000, 9155.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  180/5000: episode: 18, duration: 0.017s, episode steps:  10, steps per second: 573, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5375.000 [2130.000, 9697.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  190/5000: episode: 19, duration: 0.016s, episode steps:  10, steps per second: 616, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3373.200 [368.000, 6488.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  200/5000: episode: 20, duration: 0.016s, episode steps:  10, steps per second: 630, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4007.700 [256.000, 9492.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  210/5000: episode: 21, duration: 0.016s, episode steps:  10, steps per second: 612, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4854.200 [815.000, 8354.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  220/5000: episode: 22, duration: 0.016s, episode steps:  10, steps per second: 633, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5901.600 [2457.000, 8803.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  230/5000: episode: 23, duration: 0.016s, episode steps:  10, steps per second: 640, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3865.400 [2.000, 7075.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  240/5000: episode: 24, duration: 0.015s, episode steps:  10, steps per second: 662, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3877.400 [39.000, 9577.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  250/5000: episode: 25, duration: 0.016s, episode steps:  10, steps per second: 638, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4607.900 [55.000, 9376.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  260/5000: episode: 26, duration: 0.016s, episode steps:  10, steps per second: 642, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4627.100 [70.000, 8450.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  270/5000: episode: 27, duration: 0.020s, episode steps:  10, steps per second: 491, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5028.800 [2744.000, 8556.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  280/5000: episode: 28, duration: 0.020s, episode steps:  10, steps per second: 494, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4338.200 [1093.000, 7834.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  290/5000: episode: 29, duration: 0.017s, episode steps:  10, steps per second: 589, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5916.700 [319.000, 9564.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  300/5000: episode: 30, duration: 0.016s, episode steps:  10, steps per second: 610, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5174.900 [860.000, 8251.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  310/5000: episode: 31, duration: 0.016s, episode steps:  10, steps per second: 640, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4928.300 [126.000, 9482.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  320/5000: episode: 32, duration: 0.016s, episode steps:  10, steps per second: 617, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5250.900 [819.000, 9465.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  330/5000: episode: 33, duration: 0.017s, episode steps:  10, steps per second: 598, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4433.600 [205.000, 9170.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  340/5000: episode: 34, duration: 0.016s, episode steps:  10, steps per second: 618, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4862.000 [921.000, 9195.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  350/5000: episode: 35, duration: 0.016s, episode steps:  10, steps per second: 624, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4895.100 [341.000, 7742.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  360/5000: episode: 36, duration: 0.016s, episode steps:  10, steps per second: 641, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4561.500 [626.000, 9334.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  370/5000: episode: 37, duration: 0.016s, episode steps:  10, steps per second: 629, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3927.600 [1368.000, 7658.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  380/5000: episode: 38, duration: 0.016s, episode steps:  10, steps per second: 639, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5870.600 [3139.000, 9669.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  390/5000: episode: 39, duration: 0.020s, episode steps:  10, steps per second: 505, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4250.000 [254.000, 8746.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  400/5000: episode: 40, duration: 0.022s, episode steps:  10, steps per second: 462, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4271.400 [1810.000, 7065.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  410/5000: episode: 41, duration: 0.018s, episode steps:  10, steps per second: 549, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5063.000 [887.000, 9027.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  420/5000: episode: 42, duration: 0.017s, episode steps:  10, steps per second: 597, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5975.500 [1599.000, 8932.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  430/5000: episode: 43, duration: 0.015s, episode steps:  10, steps per second: 652, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3920.400 [340.000, 9395.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  440/5000: episode: 44, duration: 0.016s, episode steps:  10, steps per second: 637, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4901.100 [195.000, 9356.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  450/5000: episode: 45, duration: 0.017s, episode steps:  10, steps per second: 601, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5383.900 [1575.000, 9736.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  460/5000: episode: 46, duration: 0.015s, episode steps:  10, steps per second: 651, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5865.800 [413.000, 9553.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  470/5000: episode: 47, duration: 0.015s, episode steps:  10, steps per second: 655, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5070.900 [787.000, 9642.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  480/5000: episode: 48, duration: 0.015s, episode steps:  10, steps per second: 647, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5713.500 [592.000, 9384.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  490/5000: episode: 49, duration: 0.015s, episode steps:  10, steps per second: 667, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6247.000 [805.000, 8154.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  500/5000: episode: 50, duration: 0.015s, episode steps:  10, steps per second: 656, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4367.300 [305.000, 9518.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ddpg/lib/python3.9/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2025-04-27 19:02:44.339536: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_91_1/BiasAdd' id:15646 op device:{requested: '', assigned: ''} def:{{{node dense_91_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_91_1/MatMul, dense_91_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-04-27 19:02:45.057869: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_103/AddN' id:15930 op device:{requested: '', assigned: ''} def:{{{node loss_103/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_103/mul, loss_103/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-04-27 19:02:45.558458: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/dense_90/bias/v/Assign' id:16175 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/dense_90/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_22/Adam/dense_90/bias/v, training_22/Adam/dense_90/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  510/5000: episode: 51, duration: 4.294s, episode steps:  10, steps per second:   2, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4659.000 [1101.000, 9730.000],  loss: 0.004177, mae: 0.007409, mean_q: 0.033689, mean_eps: 0.990405\n",
      "  520/5000: episode: 52, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6735.300 [4437.000, 9078.000],  loss: 0.008017, mae: 0.007550, mean_q: 0.040306, mean_eps: 0.990224\n",
      "  530/5000: episode: 53, duration: 0.319s, episode steps:  10, steps per second:  31, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3105.600 [70.000, 6250.000],  loss: 0.004571, mae: 0.008335, mean_q: 0.053590, mean_eps: 0.990034\n",
      "  540/5000: episode: 54, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5124.100 [623.000, 9077.000],  loss: 0.000080, mae: 0.008821, mean_q: 0.072920, mean_eps: 0.989844\n",
      "  550/5000: episode: 55, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5246.700 [783.000, 9631.000],  loss: 0.002936, mae: 0.008293, mean_q: 0.077817, mean_eps: 0.989654\n",
      "  560/5000: episode: 56, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3950.300 [207.000, 7241.000],  loss: 0.008913, mae: 0.009829, mean_q: 0.099133, mean_eps: 0.989464\n",
      "  570/5000: episode: 57, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4544.600 [276.000, 8416.000],  loss: 0.005437, mae: 0.014217, mean_q: 0.147935, mean_eps: 0.989274\n",
      "  580/5000: episode: 58, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6269.400 [1017.000, 9511.000],  loss: 0.000211, mae: 0.021944, mean_q: 0.234072, mean_eps: 0.989085\n",
      "  590/5000: episode: 59, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6203.800 [2583.000, 8003.000],  loss: 0.003327, mae: 0.025262, mean_q: 0.288686, mean_eps: 0.988895\n",
      "  600/5000: episode: 60, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4712.400 [251.000, 9131.000],  loss: 0.001437, mae: 0.027149, mean_q: 0.312114, mean_eps: 0.988705\n",
      "  610/5000: episode: 61, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5028.700 [428.000, 8339.000],  loss: 0.002902, mae: 0.028157, mean_q: 0.352438, mean_eps: 0.988514\n",
      "  620/5000: episode: 62, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4638.400 [328.000, 8720.000],  loss: 0.001946, mae: 0.036031, mean_q: 0.482119, mean_eps: 0.988325\n",
      "  630/5000: episode: 63, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3279.100 [361.000, 9700.000],  loss: 0.002287, mae: 0.038625, mean_q: 0.553292, mean_eps: 0.988134\n",
      "  640/5000: episode: 64, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6359.500 [914.000, 9614.000],  loss: 0.003765, mae: 0.039381, mean_q: 0.565118, mean_eps: 0.987945\n",
      "  650/5000: episode: 65, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4723.200 [63.000, 9463.000],  loss: 0.004133, mae: 0.052391, mean_q: 0.805282, mean_eps: 0.987755\n",
      "  660/5000: episode: 66, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5244.700 [1105.000, 9326.000],  loss: 0.004073, mae: 0.057418, mean_q: 0.945511, mean_eps: 0.987565\n",
      "  670/5000: episode: 67, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4857.400 [1270.000, 8644.000],  loss: 0.005323, mae: 0.070286, mean_q: 1.283516, mean_eps: 0.987375\n",
      "  680/5000: episode: 68, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4797.300 [806.000, 7932.000],  loss: 0.006659, mae: 0.081033, mean_q: 1.599149, mean_eps: 0.987185\n",
      "  690/5000: episode: 69, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5315.600 [1004.000, 8706.000],  loss: 0.008600, mae: 0.097494, mean_q: 1.929970, mean_eps: 0.986995\n",
      "  700/5000: episode: 70, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5253.900 [2308.000, 7879.000],  loss: 0.017947, mae: 0.115933, mean_q: 2.261309, mean_eps: 0.986805\n",
      "  710/5000: episode: 71, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4532.400 [903.000, 9173.000],  loss: 0.017433, mae: 0.134469, mean_q: 2.583904, mean_eps: 0.986614\n",
      "  720/5000: episode: 72, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4622.800 [364.000, 8384.000],  loss: 0.024756, mae: 0.151770, mean_q: 2.771155, mean_eps: 0.986424\n",
      "  730/5000: episode: 73, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3703.500 [407.000, 8044.000],  loss: 0.030479, mae: 0.179718, mean_q: 2.829922, mean_eps: 0.986234\n",
      "  740/5000: episode: 74, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3936.200 [151.000, 8627.000],  loss: 0.039868, mae: 0.202613, mean_q: 2.760311, mean_eps: 0.986044\n",
      "  750/5000: episode: 75, duration: 0.361s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5687.000 [819.000, 9493.000],  loss: 0.041595, mae: 0.193257, mean_q: 2.492254, mean_eps: 0.985854\n",
      "  760/5000: episode: 76, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6299.500 [501.000, 9435.000],  loss: 0.059591, mae: 0.200568, mean_q: 2.511860, mean_eps: 0.985664\n",
      "  770/5000: episode: 77, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5746.000 [756.000, 8656.000],  loss: 0.044829, mae: 0.162876, mean_q: 1.960979, mean_eps: 0.985474\n",
      "  780/5000: episode: 78, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3697.000 [374.000, 9504.000],  loss: 0.112413, mae: 0.125643, mean_q: 1.545539, mean_eps: 0.985285\n",
      "  790/5000: episode: 79, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4246.000 [383.000, 8500.000],  loss: 0.085352, mae: 0.228340, mean_q: 2.818536, mean_eps: 0.985094\n",
      "  800/5000: episode: 80, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4407.700 [596.000, 9441.000],  loss: 0.059999, mae: 0.199709, mean_q: 2.320502, mean_eps: 0.984905\n",
      "  810/5000: episode: 81, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4675.000 [2327.000, 7415.000],  loss: 0.052493, mae: 0.187096, mean_q: 2.129143, mean_eps: 0.984714\n",
      "  820/5000: episode: 82, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4654.800 [1275.000, 8327.000],  loss: 0.059250, mae: 0.220751, mean_q: 2.485732, mean_eps: 0.984525\n",
      "  830/5000: episode: 83, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4115.100 [735.000, 8848.000],  loss: 0.041946, mae: 0.206163, mean_q: 2.313060, mean_eps: 0.984334\n",
      "  840/5000: episode: 84, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6799.900 [4190.000, 9637.000],  loss: 0.054160, mae: 0.216578, mean_q: 2.347525, mean_eps: 0.984144\n",
      "  850/5000: episode: 85, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3393.000 [82.000, 7875.000],  loss: 0.084459, mae: 0.271667, mean_q: 2.919570, mean_eps: 0.983955\n",
      "  860/5000: episode: 86, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4191.800 [293.000, 5965.000],  loss: 0.054697, mae: 0.252900, mean_q: 2.662548, mean_eps: 0.983765\n",
      "  870/5000: episode: 87, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4952.800 [188.000, 9160.000],  loss: 0.083275, mae: 0.254767, mean_q: 2.684043, mean_eps: 0.983575\n",
      "  880/5000: episode: 88, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3307.700 [705.000, 7746.000],  loss: 0.082389, mae: 0.279889, mean_q: 2.934985, mean_eps: 0.983384\n",
      "  890/5000: episode: 89, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4685.800 [133.000, 8672.000],  loss: 0.048851, mae: 0.247255, mean_q: 2.625821, mean_eps: 0.983195\n",
      "  900/5000: episode: 90, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5614.100 [826.000, 8911.000],  loss: 0.066868, mae: 0.258439, mean_q: 2.761185, mean_eps: 0.983005\n",
      "  910/5000: episode: 91, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5846.400 [2262.000, 8899.000],  loss: 0.090826, mae: 0.320542, mean_q: 3.441662, mean_eps: 0.982814\n",
      "  920/5000: episode: 92, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2551.400 [984.000, 6683.000],  loss: 0.112191, mae: 0.306034, mean_q: 3.226398, mean_eps: 0.982625\n",
      "  930/5000: episode: 93, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5295.900 [1327.000, 9568.000],  loss: 0.118818, mae: 0.339161, mean_q: 3.542085, mean_eps: 0.982434\n",
      "  940/5000: episode: 94, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3895.500 [837.000, 8521.000],  loss: 0.148779, mae: 0.373436, mean_q: 3.947765, mean_eps: 0.982244\n",
      "  950/5000: episode: 95, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4810.400 [1045.000, 9421.000],  loss: 0.178820, mae: 0.428497, mean_q: 4.859537, mean_eps: 0.982054\n",
      "  960/5000: episode: 96, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4669.800 [184.000, 9712.000],  loss: 0.167704, mae: 0.455801, mean_q: 5.239933, mean_eps: 0.981865\n",
      "  970/5000: episode: 97, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3615.500 [135.000, 8792.000],  loss: 0.209645, mae: 0.492039, mean_q: 5.468727, mean_eps: 0.981674\n",
      "  980/5000: episode: 98, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3597.800 [241.000, 8503.000],  loss: 0.208125, mae: 0.546696, mean_q: 5.430615, mean_eps: 0.981484\n",
      "  990/5000: episode: 99, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4785.100 [121.000, 9385.000],  loss: 0.256349, mae: 0.497937, mean_q: 4.916889, mean_eps: 0.981294\n",
      " 1000/5000: episode: 100, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5442.400 [989.000, 9492.000],  loss: 0.336074, mae: 0.616855, mean_q: 6.794332, mean_eps: 0.981105\n",
      " 1010/5000: episode: 101, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5600.400 [117.000, 9523.000],  loss: 0.377727, mae: 0.660856, mean_q: 7.085656, mean_eps: 0.980915\n",
      " 1020/5000: episode: 102, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4866.100 [118.000, 9139.000],  loss: 0.382827, mae: 0.631401, mean_q: 6.661351, mean_eps: 0.980724\n",
      " 1030/5000: episode: 103, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2918.000 [275.000, 9604.000],  loss: 0.385748, mae: 0.708925, mean_q: 7.255437, mean_eps: 0.980535\n",
      " 1040/5000: episode: 104, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4539.100 [684.000, 8444.000],  loss: 0.400173, mae: 0.747344, mean_q: 7.547962, mean_eps: 0.980344\n",
      " 1050/5000: episode: 105, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4765.400 [1362.000, 8179.000],  loss: 0.396462, mae: 0.784571, mean_q: 7.815512, mean_eps: 0.980155\n",
      " 1060/5000: episode: 106, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4911.300 [963.000, 9647.000],  loss: 0.509556, mae: 0.825298, mean_q: 7.644855, mean_eps: 0.979965\n",
      " 1070/5000: episode: 107, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5822.300 [1491.000, 9479.000],  loss: 0.513110, mae: 0.734700, mean_q: 6.701509, mean_eps: 0.979775\n",
      " 1080/5000: episode: 108, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4113.000 [621.000, 8054.000],  loss: 0.731363, mae: 0.829035, mean_q: 7.565347, mean_eps: 0.979585\n",
      " 1090/5000: episode: 109, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4070.400 [207.000, 9423.000],  loss: 1.066676, mae: 0.919937, mean_q: 8.439649, mean_eps: 0.979395\n",
      " 1100/5000: episode: 110, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5111.000 [423.000, 9359.000],  loss: 0.709225, mae: 1.017566, mean_q: 9.437653, mean_eps: 0.979205\n",
      " 1110/5000: episode: 111, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4864.100 [43.000, 8621.000],  loss: 0.622537, mae: 1.092070, mean_q: 10.151795, mean_eps: 0.979015\n",
      " 1120/5000: episode: 112, duration: 0.267s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6476.300 [383.000, 9657.000],  loss: 1.151485, mae: 0.897817, mean_q: 8.669638, mean_eps: 0.978824\n",
      " 1130/5000: episode: 113, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5315.200 [1057.000, 9363.000],  loss: 0.748727, mae: 1.026669, mean_q: 10.113185, mean_eps: 0.978634\n",
      " 1140/5000: episode: 114, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5926.300 [788.000, 9262.000],  loss: 0.607307, mae: 0.996865, mean_q: 9.224708, mean_eps: 0.978444\n",
      " 1150/5000: episode: 115, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4810.600 [146.000, 9660.000],  loss: 3.190890, mae: 0.891766, mean_q: 7.960810, mean_eps: 0.978254\n",
      " 1160/5000: episode: 116, duration: 0.305s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3997.600 [1710.000, 7590.000],  loss: 1.441263, mae: 1.351227, mean_q: 13.225448, mean_eps: 0.978065\n",
      " 1170/5000: episode: 117, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4963.400 [1487.000, 8928.000],  loss: 1.806690, mae: 1.315782, mean_q: 12.842475, mean_eps: 0.977874\n",
      " 1180/5000: episode: 118, duration: 0.308s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6254.400 [800.000, 9388.000],  loss: 1.756800, mae: 1.240860, mean_q: 11.848768, mean_eps: 0.977684\n",
      " 1190/5000: episode: 119, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4495.200 [1043.000, 9126.000],  loss: 1.449075, mae: 1.135401, mean_q: 10.838879, mean_eps: 0.977494\n",
      " 1200/5000: episode: 120, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4717.100 [38.000, 9217.000],  loss: 1.063869, mae: 1.234768, mean_q: 11.860429, mean_eps: 0.977305\n",
      " 1210/5000: episode: 121, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5309.700 [2170.000, 8381.000],  loss: 1.163283, mae: 1.256777, mean_q: 12.062673, mean_eps: 0.977115\n",
      " 1220/5000: episode: 122, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4981.700 [1683.000, 7963.000],  loss: 1.453349, mae: 1.351758, mean_q: 12.750400, mean_eps: 0.976924\n",
      " 1230/5000: episode: 123, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5281.400 [826.000, 9237.000],  loss: 1.644522, mae: 1.550000, mean_q: 14.471624, mean_eps: 0.976735\n",
      " 1240/5000: episode: 124, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5335.400 [1234.000, 9384.000],  loss: 2.103635, mae: 1.503992, mean_q: 13.929531, mean_eps: 0.976544\n",
      " 1250/5000: episode: 125, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6035.100 [1398.000, 8494.000],  loss: 1.842769, mae: 1.676177, mean_q: 15.443274, mean_eps: 0.976355\n",
      " 1260/5000: episode: 126, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4188.300 [1369.000, 7778.000],  loss: 1.623137, mae: 1.757050, mean_q: 16.139042, mean_eps: 0.976164\n",
      " 1270/5000: episode: 127, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4966.500 [1369.000, 8815.000],  loss: 2.765471, mae: 1.906209, mean_q: 17.395312, mean_eps: 0.975975\n",
      " 1280/5000: episode: 128, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6610.000 [1027.000, 9315.000],  loss: 4.311833, mae: 1.968749, mean_q: 17.886732, mean_eps: 0.975785\n",
      " 1290/5000: episode: 129, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5907.600 [525.000, 9568.000],  loss: 4.697557, mae: 2.144734, mean_q: 19.534519, mean_eps: 0.975595\n",
      " 1300/5000: episode: 130, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4410.200 [1704.000, 8396.000],  loss: 3.314784, mae: 2.299013, mean_q: 20.909905, mean_eps: 0.975405\n",
      " 1310/5000: episode: 131, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5037.100 [558.000, 9568.000],  loss: 1.819822, mae: 2.360865, mean_q: 20.923065, mean_eps: 0.975215\n",
      " 1320/5000: episode: 132, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4305.200 [1432.000, 6801.000],  loss: 5.894582, mae: 2.623091, mean_q: 22.331671, mean_eps: 0.975024\n",
      " 1330/5000: episode: 133, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6237.500 [1273.000, 9452.000],  loss: 5.660462, mae: 2.309523, mean_q: 18.896825, mean_eps: 0.974834\n",
      " 1340/5000: episode: 134, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5714.700 [622.000, 9704.000],  loss: 12.890611, mae: 1.635001, mean_q: 13.494855, mean_eps: 0.974644\n",
      " 1350/5000: episode: 135, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5408.700 [980.000, 9504.000],  loss: 6.941971, mae: 1.950927, mean_q: 17.014987, mean_eps: 0.974454\n",
      " 1360/5000: episode: 136, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5375.500 [100.000, 9726.000],  loss: 4.831866, mae: 2.687205, mean_q: 24.194331, mean_eps: 0.974264\n",
      " 1370/5000: episode: 137, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3957.300 [491.000, 9196.000],  loss: 5.720968, mae: 2.609266, mean_q: 23.838566, mean_eps: 0.974074\n",
      " 1380/5000: episode: 138, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5775.400 [2152.000, 8808.000],  loss: 5.368311, mae: 2.807736, mean_q: 25.643962, mean_eps: 0.973884\n",
      " 1390/5000: episode: 139, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4306.900 [113.000, 7447.000],  loss: 10.497478, mae: 3.072620, mean_q: 26.814418, mean_eps: 0.973695\n",
      " 1400/5000: episode: 140, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5652.500 [530.000, 9542.000],  loss: 6.203642, mae: 2.713929, mean_q: 23.376588, mean_eps: 0.973504\n",
      " 1410/5000: episode: 141, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3874.600 [442.000, 7446.000],  loss: 4.624068, mae: 3.049342, mean_q: 26.023975, mean_eps: 0.973315\n",
      " 1420/5000: episode: 142, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6006.800 [3031.000, 8777.000],  loss: 5.075133, mae: 3.304336, mean_q: 28.040343, mean_eps: 0.973124\n",
      " 1430/5000: episode: 143, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5179.600 [344.000, 9197.000],  loss: 8.931391, mae: 3.402119, mean_q: 28.787465, mean_eps: 0.972935\n",
      " 1440/5000: episode: 144, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6753.000 [1097.000, 9705.000],  loss: 10.292261, mae: 3.528528, mean_q: 30.178543, mean_eps: 0.972744\n",
      " 1450/5000: episode: 145, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3465.900 [365.000, 8366.000],  loss: 10.597791, mae: 3.263196, mean_q: 28.271027, mean_eps: 0.972554\n",
      " 1460/5000: episode: 146, duration: 0.350s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6022.800 [1868.000, 8499.000],  loss: 7.973514, mae: 3.746104, mean_q: 32.656491, mean_eps: 0.972365\n",
      " 1470/5000: episode: 147, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5846.400 [1206.000, 9492.000],  loss: 9.789110, mae: 3.861837, mean_q: 33.621241, mean_eps: 0.972175\n",
      " 1480/5000: episode: 148, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5860.400 [440.000, 9095.000],  loss: 7.887370, mae: 4.081052, mean_q: 35.119730, mean_eps: 0.971985\n",
      " 1490/5000: episode: 149, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4351.000 [92.000, 8166.000],  loss: 12.951978, mae: 4.443701, mean_q: 37.814613, mean_eps: 0.971795\n",
      " 1500/5000: episode: 150, duration: 0.245s, episode steps:  10, steps per second:  41, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5594.300 [348.000, 9077.000],  loss: 14.322746, mae: 4.869568, mean_q: 41.120053, mean_eps: 0.971605\n",
      " 1510/5000: episode: 151, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4363.300 [279.000, 8994.000],  loss: 18.594181, mae: 4.932081, mean_q: 41.146101, mean_eps: 0.971415\n",
      " 1520/5000: episode: 152, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5541.200 [159.000, 9492.000],  loss: 15.497487, mae: 5.321741, mean_q: 43.685710, mean_eps: 0.971224\n",
      " 1530/5000: episode: 153, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4419.600 [1008.000, 9189.000],  loss: 21.104696, mae: 5.557389, mean_q: 44.886537, mean_eps: 0.971034\n",
      " 1540/5000: episode: 154, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4238.600 [625.000, 9716.000],  loss: 20.909176, mae: 6.229552, mean_q: 49.902839, mean_eps: 0.970844\n",
      " 1550/5000: episode: 155, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4023.000 [585.000, 9078.000],  loss: 18.953954, mae: 5.649298, mean_q: 46.658011, mean_eps: 0.970654\n",
      " 1560/5000: episode: 156, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4259.000 [610.000, 7793.000],  loss: 16.393263, mae: 5.252667, mean_q: 44.213927, mean_eps: 0.970464\n",
      " 1570/5000: episode: 157, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4748.800 [851.000, 8369.000],  loss: 16.740776, mae: 5.987339, mean_q: 50.295378, mean_eps: 0.970274\n",
      " 1580/5000: episode: 158, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5353.600 [1035.000, 9652.000],  loss: 26.683171, mae: 5.982149, mean_q: 50.005362, mean_eps: 0.970084\n",
      " 1590/5000: episode: 159, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6267.400 [1472.000, 8993.000],  loss: 25.369816, mae: 6.775115, mean_q: 56.243913, mean_eps: 0.969894\n",
      " 1600/5000: episode: 160, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7198.300 [1324.000, 9481.000],  loss: 26.643630, mae: 6.785641, mean_q: 56.468983, mean_eps: 0.969704\n",
      " 1610/5000: episode: 161, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4466.000 [1384.000, 9314.000],  loss: 39.856623, mae: 6.638118, mean_q: 54.911191, mean_eps: 0.969515\n",
      " 1620/5000: episode: 162, duration: 0.260s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6013.800 [1418.000, 9546.000],  loss: 34.596733, mae: 7.686828, mean_q: 61.939689, mean_eps: 0.969324\n",
      " 1630/5000: episode: 163, duration: 0.290s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3788.300 [602.000, 8747.000],  loss: 32.902503, mae: 8.426157, mean_q: 67.912410, mean_eps: 0.969135\n",
      " 1640/5000: episode: 164, duration: 0.260s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4297.600 [236.000, 9657.000],  loss: 70.383406, mae: 9.022784, mean_q: 72.521686, mean_eps: 0.968945\n",
      " 1650/5000: episode: 165, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3620.700 [766.000, 6971.000],  loss: 45.446460, mae: 9.146924, mean_q: 73.104575, mean_eps: 0.968754\n",
      " 1660/5000: episode: 166, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6368.000 [466.000, 9354.000],  loss: 39.526523, mae: 9.954625, mean_q: 78.949686, mean_eps: 0.968565\n",
      " 1670/5000: episode: 167, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5030.200 [79.000, 8556.000],  loss: 52.723251, mae: 10.766768, mean_q: 82.809501, mean_eps: 0.968375\n",
      " 1680/5000: episode: 168, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5191.300 [1752.000, 9369.000],  loss: 60.847632, mae: 10.913902, mean_q: 83.652930, mean_eps: 0.968185\n",
      " 1690/5000: episode: 169, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4123.100 [367.000, 9561.000],  loss: 112.764092, mae: 10.150142, mean_q: 77.970985, mean_eps: 0.967995\n",
      " 1700/5000: episode: 170, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5665.700 [579.000, 8803.000],  loss: 99.523785, mae: 10.773000, mean_q: 81.863278, mean_eps: 0.967805\n",
      " 1710/5000: episode: 171, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3756.300 [512.000, 9260.000],  loss: 91.878513, mae: 12.539548, mean_q: 94.000549, mean_eps: 0.967615\n",
      " 1720/5000: episode: 172, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6274.800 [2251.000, 9419.000],  loss: 88.785492, mae: 13.335944, mean_q: 100.591039, mean_eps: 0.967424\n",
      " 1730/5000: episode: 173, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4084.000 [1090.000, 6917.000],  loss: 98.024818, mae: 11.911626, mean_q: 90.773278, mean_eps: 0.967234\n",
      " 1740/5000: episode: 174, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4721.600 [628.000, 9637.000],  loss: 87.432564, mae: 12.500648, mean_q: 95.116322, mean_eps: 0.967044\n",
      " 1750/5000: episode: 175, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5593.700 [1869.000, 8797.000],  loss: 91.377400, mae: 14.145293, mean_q: 107.027534, mean_eps: 0.966854\n",
      " 1760/5000: episode: 176, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4212.800 [527.000, 7544.000],  loss: 90.518998, mae: 14.325893, mean_q: 108.001726, mean_eps: 0.966664\n",
      " 1770/5000: episode: 177, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5984.000 [288.000, 9571.000],  loss: 119.990703, mae: 15.380085, mean_q: 115.513237, mean_eps: 0.966475\n",
      " 1780/5000: episode: 178, duration: 0.578s, episode steps:  10, steps per second:  17, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4316.000 [43.000, 9718.000],  loss: 150.560453, mae: 16.323669, mean_q: 122.031110, mean_eps: 0.966284\n",
      " 1790/5000: episode: 179, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4839.100 [796.000, 8854.000],  loss: 152.352273, mae: 16.323419, mean_q: 120.535673, mean_eps: 0.966094\n",
      " 1800/5000: episode: 180, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6095.800 [1637.000, 9689.000],  loss: 111.933372, mae: 14.568520, mean_q: 108.206291, mean_eps: 0.965904\n",
      " 1810/5000: episode: 181, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4458.000 [1418.000, 8322.000],  loss: 136.425457, mae: 13.655688, mean_q: 101.563463, mean_eps: 0.965715\n",
      " 1820/5000: episode: 182, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4712.500 [433.000, 9390.000],  loss: 106.463198, mae: 15.035915, mean_q: 110.646978, mean_eps: 0.965525\n",
      " 1830/5000: episode: 183, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4308.500 [516.000, 9725.000],  loss: 112.973965, mae: 17.102044, mean_q: 124.703484, mean_eps: 0.965334\n",
      " 1840/5000: episode: 184, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5301.800 [334.000, 8475.000],  loss: 157.494864, mae: 16.871124, mean_q: 122.515960, mean_eps: 0.965145\n",
      " 1850/5000: episode: 185, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5244.100 [67.000, 8115.000],  loss: 139.741612, mae: 19.077448, mean_q: 137.443495, mean_eps: 0.964954\n",
      " 1860/5000: episode: 186, duration: 0.385s, episode steps:  10, steps per second:  26, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5439.300 [458.000, 8991.000],  loss: 113.556060, mae: 18.074339, mean_q: 130.175221, mean_eps: 0.964765\n",
      " 1870/5000: episode: 187, duration: 0.302s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2999.500 [45.000, 9317.000],  loss: 316.324019, mae: 19.208259, mean_q: 147.506306, mean_eps: 0.964575\n",
      " 1880/5000: episode: 188, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5212.600 [850.000, 8984.000],  loss: 195.419065, mae: 22.581231, mean_q: 181.889592, mean_eps: 0.964385\n",
      " 1890/5000: episode: 189, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4352.500 [1045.000, 8831.000],  loss: 327.714226, mae: 24.602921, mean_q: 202.524986, mean_eps: 0.964195\n",
      " 1900/5000: episode: 190, duration: 0.267s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4070.100 [1.000, 8596.000],  loss: 363.584744, mae: 22.556262, mean_q: 190.267120, mean_eps: 0.964005\n",
      " 1910/5000: episode: 191, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5365.600 [592.000, 9087.000],  loss: 517.346750, mae: 23.791410, mean_q: 193.084438, mean_eps: 0.963815\n",
      " 1920/5000: episode: 192, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4233.100 [229.000, 8984.000],  loss: 475.958671, mae: 28.403721, mean_q: 214.979599, mean_eps: 0.963625\n",
      " 1930/5000: episode: 193, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4283.100 [461.000, 9434.000],  loss: 482.127081, mae: 29.460853, mean_q: 213.407077, mean_eps: 0.963434\n",
      " 1940/5000: episode: 194, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4699.400 [30.000, 8904.000],  loss: 365.215897, mae: 26.335175, mean_q: 192.491777, mean_eps: 0.963244\n",
      " 1950/5000: episode: 195, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4203.900 [329.000, 8135.000],  loss: 437.176538, mae: 26.242291, mean_q: 191.606773, mean_eps: 0.963054\n",
      " 1960/5000: episode: 196, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5531.300 [269.000, 9572.000],  loss: 486.572108, mae: 30.332648, mean_q: 217.283243, mean_eps: 0.962864\n",
      " 1970/5000: episode: 197, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4054.600 [244.000, 8923.000],  loss: 602.741510, mae: 33.553509, mean_q: 237.871069, mean_eps: 0.962674\n",
      " 1980/5000: episode: 198, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6460.000 [3645.000, 9555.000],  loss: 659.555852, mae: 35.542472, mean_q: 249.702028, mean_eps: 0.962484\n",
      " 1990/5000: episode: 199, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3276.000 [478.000, 8319.000],  loss: 896.085777, mae: 36.762016, mean_q: 257.831380, mean_eps: 0.962294\n",
      " 2000/5000: episode: 200, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3868.100 [190.000, 9597.000],  loss: 453.561684, mae: 39.218932, mean_q: 273.187279, mean_eps: 0.962105\n",
      " 2010/5000: episode: 201, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3355.700 [452.000, 8520.000],  loss: 894.443568, mae: 40.461886, mean_q: 279.458289, mean_eps: 0.961915\n",
      " 2020/5000: episode: 202, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5108.000 [1370.000, 9522.000],  loss: 1076.376091, mae: 43.901385, mean_q: 301.334375, mean_eps: 0.961725\n",
      " 2030/5000: episode: 203, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4393.700 [645.000, 9520.000],  loss: 708.766284, mae: 46.146757, mean_q: 315.075015, mean_eps: 0.961534\n",
      " 2040/5000: episode: 204, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5139.500 [1802.000, 9676.000],  loss: 896.602292, mae: 47.501265, mean_q: 322.521341, mean_eps: 0.961345\n",
      " 2050/5000: episode: 205, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5806.400 [1460.000, 9522.000],  loss: 1140.056927, mae: 52.001366, mean_q: 349.331998, mean_eps: 0.961154\n",
      " 2060/5000: episode: 206, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4733.600 [525.000, 7892.000],  loss: 817.905740, mae: 50.924930, mean_q: 339.500165, mean_eps: 0.960964\n",
      " 2070/5000: episode: 207, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3158.900 [461.000, 7075.000],  loss: 1104.317328, mae: 53.133897, mean_q: 352.805997, mean_eps: 0.960775\n",
      " 2080/5000: episode: 208, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4555.300 [951.000, 7937.000],  loss: 1082.621700, mae: 56.252211, mean_q: 371.898343, mean_eps: 0.960585\n",
      " 2090/5000: episode: 209, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3804.300 [476.000, 9048.000],  loss: 980.270062, mae: 58.832723, mean_q: 387.382578, mean_eps: 0.960395\n",
      " 2100/5000: episode: 210, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4952.900 [343.000, 9676.000],  loss: 1386.693704, mae: 59.257927, mean_q: 388.589236, mean_eps: 0.960205\n",
      " 2110/5000: episode: 211, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4630.000 [377.000, 9396.000],  loss: 1710.450974, mae: 63.968032, mean_q: 418.320609, mean_eps: 0.960015\n",
      " 2120/5000: episode: 212, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6159.500 [3456.000, 9254.000],  loss: 1002.958679, mae: 63.972432, mean_q: 419.078149, mean_eps: 0.959825\n",
      " 2130/5000: episode: 213, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4214.900 [2262.000, 7083.000],  loss: 1710.112210, mae: 66.074921, mean_q: 431.668686, mean_eps: 0.959634\n",
      " 2140/5000: episode: 214, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5792.700 [713.000, 9717.000],  loss: 3326.065881, mae: 70.269361, mean_q: 455.955505, mean_eps: 0.959444\n",
      " 2150/5000: episode: 215, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4069.000 [1159.000, 7346.000],  loss: 2098.156848, mae: 73.452048, mean_q: 477.804413, mean_eps: 0.959254\n",
      " 2160/5000: episode: 216, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4669.900 [305.000, 8503.000],  loss: 2174.335010, mae: 70.150976, mean_q: 461.662109, mean_eps: 0.959064\n",
      " 2170/5000: episode: 217, duration: 0.315s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3453.700 [61.000, 7648.000],  loss: 3181.801538, mae: 66.394032, mean_q: 438.778223, mean_eps: 0.958874\n",
      " 2180/5000: episode: 218, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5251.800 [916.000, 9058.000],  loss: 3966.516968, mae: 72.598880, mean_q: 487.324158, mean_eps: 0.958684\n",
      " 2190/5000: episode: 219, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5234.500 [1416.000, 9302.000],  loss: 2367.062134, mae: 80.325818, mean_q: 543.936334, mean_eps: 0.958494\n",
      " 2200/5000: episode: 220, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4100.700 [540.000, 8864.000],  loss: 3583.450732, mae: 78.363819, mean_q: 544.650061, mean_eps: 0.958305\n",
      " 2210/5000: episode: 221, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5416.500 [681.000, 9696.000],  loss: 3522.452774, mae: 82.947864, mean_q: 578.761664, mean_eps: 0.958114\n",
      " 2220/5000: episode: 222, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6320.300 [1895.000, 9577.000],  loss: 2854.999701, mae: 89.886195, mean_q: 589.607141, mean_eps: 0.957925\n",
      " 2230/5000: episode: 223, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3492.200 [6.000, 8491.000],  loss: 2876.658643, mae: 93.700344, mean_q: 610.867346, mean_eps: 0.957734\n",
      " 2240/5000: episode: 224, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4808.400 [361.000, 9577.000],  loss: 3678.765332, mae: 98.428163, mean_q: 638.406781, mean_eps: 0.957545\n",
      " 2250/5000: episode: 225, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5621.000 [481.000, 9577.000],  loss: 4695.213910, mae: 103.565952, mean_q: 668.650348, mean_eps: 0.957354\n",
      " 2260/5000: episode: 226, duration: 0.253s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4768.100 [417.000, 9161.000],  loss: 4627.585629, mae: 105.997083, mean_q: 681.054657, mean_eps: 0.957164\n",
      " 2270/5000: episode: 227, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4937.500 [76.000, 9655.000],  loss: 5355.136633, mae: 111.736336, mean_q: 712.815027, mean_eps: 0.956975\n",
      " 2280/5000: episode: 228, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3822.400 [22.000, 6269.000],  loss: 8065.219153, mae: 122.877333, mean_q: 777.316278, mean_eps: 0.956785\n",
      " 2290/5000: episode: 229, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6287.800 [742.000, 9490.000],  loss: 8055.233435, mae: 124.258754, mean_q: 782.116559, mean_eps: 0.956595\n",
      " 2300/5000: episode: 230, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5018.800 [994.000, 8467.000],  loss: 5967.557690, mae: 131.086874, mean_q: 820.874634, mean_eps: 0.956405\n",
      " 2310/5000: episode: 231, duration: 0.310s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4929.000 [1006.000, 8865.000],  loss: 11451.465698, mae: 137.413861, mean_q: 864.041284, mean_eps: 0.956215\n",
      " 2320/5000: episode: 232, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5151.900 [1938.000, 8511.000],  loss: 11551.856006, mae: 144.162508, mean_q: 919.561389, mean_eps: 0.956025\n",
      " 2330/5000: episode: 233, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5137.800 [517.000, 9640.000],  loss: 12396.438672, mae: 153.313676, mean_q: 984.473401, mean_eps: 0.955834\n",
      " 2340/5000: episode: 234, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4001.800 [462.000, 8795.000],  loss: 9031.324451, mae: 157.741982, mean_q: 1013.221783, mean_eps: 0.955645\n",
      " 2350/5000: episode: 235, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3871.400 [559.000, 9076.000],  loss: 7268.240430, mae: 162.615933, mean_q: 1054.716333, mean_eps: 0.955454\n",
      " 2360/5000: episode: 236, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5537.100 [469.000, 9359.000],  loss: 7516.532935, mae: 165.672691, mean_q: 1047.407758, mean_eps: 0.955264\n",
      " 2370/5000: episode: 237, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5249.700 [1769.000, 9246.000],  loss: 13999.453653, mae: 159.541862, mean_q: 1000.461041, mean_eps: 0.955074\n",
      " 2380/5000: episode: 238, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5059.400 [40.000, 9164.000],  loss: 8691.791357, mae: 166.388484, mean_q: 1081.325116, mean_eps: 0.954885\n",
      " 2390/5000: episode: 239, duration: 0.315s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3751.500 [1286.000, 7212.000],  loss: 9376.896869, mae: 158.416411, mean_q: 1068.561292, mean_eps: 0.954694\n",
      " 2400/5000: episode: 240, duration: 0.376s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4927.700 [570.000, 8734.000],  loss: 20805.263867, mae: 170.716873, mean_q: 1158.652454, mean_eps: 0.954504\n",
      " 2410/5000: episode: 241, duration: 0.318s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4740.100 [1442.000, 8175.000],  loss: 12958.483691, mae: 176.241130, mean_q: 1174.464368, mean_eps: 0.954314\n",
      " 2420/5000: episode: 242, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5655.300 [2088.000, 8081.000],  loss: 21447.825903, mae: 197.575949, mean_q: 1296.110400, mean_eps: 0.954125\n",
      " 2430/5000: episode: 243, duration: 0.326s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6900.600 [2818.000, 8963.000],  loss: 16446.953809, mae: 205.840999, mean_q: 1347.350183, mean_eps: 0.953935\n",
      " 2440/5000: episode: 244, duration: 0.322s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4647.900 [449.000, 9661.000],  loss: 24123.579980, mae: 214.313628, mean_q: 1395.962451, mean_eps: 0.953744\n",
      " 2450/5000: episode: 245, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4444.500 [1048.000, 8479.000],  loss: 14408.308594, mae: 230.257753, mean_q: 1490.816260, mean_eps: 0.953555\n",
      " 2460/5000: episode: 246, duration: 0.324s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3314.300 [96.000, 8840.000],  loss: 22154.530273, mae: 246.513058, mean_q: 1511.522595, mean_eps: 0.953364\n",
      " 2470/5000: episode: 247, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6060.700 [1949.000, 9669.000],  loss: 21214.434521, mae: 250.257301, mean_q: 1504.559705, mean_eps: 0.953175\n",
      " 2480/5000: episode: 248, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3942.600 [70.000, 8140.000],  loss: 26096.278320, mae: 266.992923, mean_q: 1596.944373, mean_eps: 0.952985\n",
      " 2490/5000: episode: 249, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5743.300 [395.000, 9550.000],  loss: 34353.686426, mae: 272.412840, mean_q: 1634.801929, mean_eps: 0.952795\n",
      " 2500/5000: episode: 250, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4852.700 [1294.000, 8750.000],  loss: 19044.819238, mae: 287.508304, mean_q: 1731.014880, mean_eps: 0.952605\n",
      " 2510/5000: episode: 251, duration: 0.302s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7683.400 [2721.000, 9562.000],  loss: 32998.221924, mae: 299.127292, mean_q: 1796.199084, mean_eps: 0.952415\n",
      " 2520/5000: episode: 252, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4580.600 [91.000, 9506.000],  loss: 63576.636987, mae: 307.140604, mean_q: 1849.190198, mean_eps: 0.952225\n",
      " 2530/5000: episode: 253, duration: 0.306s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4769.700 [1450.000, 7637.000],  loss: 52342.624219, mae: 299.075204, mean_q: 1871.424329, mean_eps: 0.952035\n",
      " 2540/5000: episode: 254, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4224.600 [272.000, 8468.000],  loss: 28073.753906, mae: 278.056549, mean_q: 1781.694287, mean_eps: 0.951844\n",
      " 2550/5000: episode: 255, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5729.900 [1503.000, 9363.000],  loss: 33222.301562, mae: 263.015268, mean_q: 1701.179651, mean_eps: 0.951654\n",
      " 2560/5000: episode: 256, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4077.900 [654.000, 8197.000],  loss: 42407.079492, mae: 278.354285, mean_q: 1777.966284, mean_eps: 0.951464\n",
      " 2570/5000: episode: 257, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2498.800 [169.000, 6699.000],  loss: 41841.885254, mae: 296.082211, mean_q: 1864.461560, mean_eps: 0.951274\n",
      " 2580/5000: episode: 258, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4746.200 [921.000, 8354.000],  loss: 43125.309229, mae: 312.323758, mean_q: 1949.577661, mean_eps: 0.951084\n",
      " 2590/5000: episode: 259, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4789.800 [20.000, 8865.000],  loss: 44456.656445, mae: 329.908505, mean_q: 2044.960083, mean_eps: 0.950894\n",
      " 2600/5000: episode: 260, duration: 0.250s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5492.400 [1438.000, 8911.000],  loss: 48334.019434, mae: 355.525061, mean_q: 2189.912415, mean_eps: 0.950704\n",
      " 2610/5000: episode: 261, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6660.300 [5010.000, 9106.000],  loss: 56391.539648, mae: 376.037500, mean_q: 2226.060474, mean_eps: 0.950514\n",
      " 2620/5000: episode: 262, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5515.400 [79.000, 9455.000],  loss: 62476.310937, mae: 402.160345, mean_q: 2366.046143, mean_eps: 0.950325\n",
      " 2630/5000: episode: 263, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5068.800 [463.000, 9599.000],  loss: 126667.009375, mae: 411.275592, mean_q: 2446.545264, mean_eps: 0.950135\n",
      " 2640/5000: episode: 264, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4859.500 [1011.000, 9687.000],  loss: 84215.573438, mae: 390.525516, mean_q: 2333.531909, mean_eps: 0.949944\n",
      " 2650/5000: episode: 265, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5474.900 [1242.000, 8226.000],  loss: 72290.965039, mae: 385.108212, mean_q: 2306.218945, mean_eps: 0.949755\n",
      " 2660/5000: episode: 266, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4736.300 [1271.000, 9308.000],  loss: 129724.510938, mae: 387.176279, mean_q: 2328.345850, mean_eps: 0.949564\n",
      " 2670/5000: episode: 267, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4241.500 [1556.000, 7139.000],  loss: 114156.891211, mae: 442.572589, mean_q: 2812.301074, mean_eps: 0.949375\n",
      " 2680/5000: episode: 268, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5269.600 [167.000, 9452.000],  loss: 101204.997656, mae: 473.508630, mean_q: 3057.058984, mean_eps: 0.949185\n",
      " 2690/5000: episode: 269, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4913.000 [1160.000, 7709.000],  loss: 81312.575000, mae: 510.929807, mean_q: 3292.126196, mean_eps: 0.948995\n",
      " 2700/5000: episode: 270, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5274.300 [44.000, 9598.000],  loss: 140310.143555, mae: 556.112018, mean_q: 3564.825684, mean_eps: 0.948805\n",
      " 2710/5000: episode: 271, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3908.200 [1178.000, 9452.000],  loss: 135493.463281, mae: 595.495886, mean_q: 3731.704028, mean_eps: 0.948615\n",
      " 2720/5000: episode: 272, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5687.400 [266.000, 8899.000],  loss: 165946.185156, mae: 618.220325, mean_q: 3619.567725, mean_eps: 0.948425\n",
      " 2730/5000: episode: 273, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5630.100 [2031.000, 9520.000],  loss: 184539.710547, mae: 633.109900, mean_q: 3689.668701, mean_eps: 0.948235\n",
      " 2740/5000: episode: 274, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2582.500 [576.000, 7637.000],  loss: 265090.335156, mae: 647.271918, mean_q: 3752.678223, mean_eps: 0.948044\n",
      " 2750/5000: episode: 275, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4012.600 [707.000, 8646.000],  loss: 176433.040234, mae: 669.626709, mean_q: 3858.718555, mean_eps: 0.947854\n",
      " 2760/5000: episode: 276, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4640.700 [104.000, 9307.000],  loss: 164831.411328, mae: 718.255115, mean_q: 4147.527808, mean_eps: 0.947664\n",
      " 2770/5000: episode: 277, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5990.000 [1509.000, 9020.000],  loss: 236726.078125, mae: 731.193805, mean_q: 4319.971143, mean_eps: 0.947474\n",
      " 2780/5000: episode: 278, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6300.000 [4223.000, 9437.000],  loss: 214603.596094, mae: 731.724524, mean_q: 4347.388330, mean_eps: 0.947284\n",
      " 2790/5000: episode: 279, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6469.300 [133.000, 9579.000],  loss: 211252.721875, mae: 782.506421, mean_q: 4615.071924, mean_eps: 0.947094\n",
      " 2800/5000: episode: 280, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4840.000 [273.000, 9369.000],  loss: 245575.377734, mae: 833.579199, mean_q: 4873.642236, mean_eps: 0.946904\n",
      " 2810/5000: episode: 281, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5787.000 [1644.000, 8858.000],  loss: 273338.757812, mae: 869.289612, mean_q: 5039.284863, mean_eps: 0.946715\n",
      " 2820/5000: episode: 282, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5564.700 [748.000, 9411.000],  loss: 234672.775586, mae: 902.746924, mean_q: 5172.395898, mean_eps: 0.946524\n",
      " 2830/5000: episode: 283, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6127.700 [4003.000, 9089.000],  loss: 254398.982031, mae: 956.314850, mean_q: 5494.452881, mean_eps: 0.946335\n",
      " 2840/5000: episode: 284, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6218.900 [526.000, 9515.000],  loss: 314273.568555, mae: 971.237024, mean_q: 5882.387402, mean_eps: 0.946144\n",
      " 2850/5000: episode: 285, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5430.900 [2261.000, 9370.000],  loss: 423980.071875, mae: 980.658588, mean_q: 6056.506787, mean_eps: 0.945955\n",
      " 2860/5000: episode: 286, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5690.900 [1405.000, 8945.000],  loss: 544405.007812, mae: 1023.684320, mean_q: 6326.750244, mean_eps: 0.945764\n",
      " 2870/5000: episode: 287, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6430.900 [108.000, 8787.000],  loss: 448624.081250, mae: 1093.774573, mean_q: 6721.730273, mean_eps: 0.945574\n",
      " 2880/5000: episode: 288, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5858.100 [38.000, 8646.000],  loss: 888547.076562, mae: 1160.762207, mean_q: 7083.304492, mean_eps: 0.945385\n",
      " 2890/5000: episode: 289, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5605.300 [2169.000, 9472.000],  loss: 525805.626563, mae: 1219.024133, mean_q: 7384.254102, mean_eps: 0.945195\n",
      " 2900/5000: episode: 290, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4953.500 [1294.000, 8646.000],  loss: 1028800.146875, mae: 1271.882715, mean_q: 7410.140674, mean_eps: 0.945005\n",
      " 2910/5000: episode: 291, duration: 0.361s, episode steps:  10, steps per second:  28, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3753.400 [1393.000, 9319.000],  loss: 639306.546875, mae: 1318.112915, mean_q: 7401.866553, mean_eps: 0.944815\n",
      " 2920/5000: episode: 292, duration: 0.247s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4552.200 [524.000, 9134.000],  loss: 556405.694531, mae: 1359.485767, mean_q: 7623.630957, mean_eps: 0.944625\n",
      " 2930/5000: episode: 293, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3659.700 [594.000, 8511.000],  loss: 1105156.571875, mae: 1455.694360, mean_q: 8142.260645, mean_eps: 0.944435\n",
      " 2940/5000: episode: 294, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5084.900 [35.000, 9069.000],  loss: 808480.003125, mae: 1457.354321, mean_q: 8296.102148, mean_eps: 0.944244\n",
      " 2950/5000: episode: 295, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5958.500 [973.000, 9517.000],  loss: 674895.925000, mae: 1447.371179, mean_q: 8283.015967, mean_eps: 0.944055\n",
      " 2960/5000: episode: 296, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5097.000 [917.000, 8517.000],  loss: 918544.909375, mae: 1506.526477, mean_q: 8599.618457, mean_eps: 0.943864\n",
      " 2970/5000: episode: 297, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5589.000 [486.000, 9717.000],  loss: 1229832.387500, mae: 1564.957288, mean_q: 8879.207666, mean_eps: 0.943674\n",
      " 2980/5000: episode: 298, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5901.000 [2336.000, 9306.000],  loss: 993953.757812, mae: 1625.574585, mean_q: 9158.554297, mean_eps: 0.943484\n",
      " 2990/5000: episode: 299, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5421.300 [811.000, 8266.000],  loss: 1144451.243750, mae: 1753.291467, mean_q: 9736.201855, mean_eps: 0.943295\n",
      " 3000/5000: episode: 300, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4132.400 [1778.000, 8300.000],  loss: 1050809.153125, mae: 1757.109009, mean_q: 9628.668750, mean_eps: 0.943104\n",
      " 3010/5000: episode: 301, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4306.600 [397.000, 8795.000],  loss: 762587.175000, mae: 1695.382849, mean_q: 9619.518066, mean_eps: 0.942914\n",
      " 3020/5000: episode: 302, duration: 0.267s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6604.000 [1858.000, 8646.000],  loss: 731529.918750, mae: 1617.957092, mean_q: 9345.389746, mean_eps: 0.942724\n",
      " 3030/5000: episode: 303, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5864.500 [1822.000, 9069.000],  loss: 1313590.893750, mae: 1661.737634, mean_q: 9615.378320, mean_eps: 0.942535\n",
      " 3040/5000: episode: 304, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4672.100 [488.000, 9456.000],  loss: 1585607.967188, mae: 1654.269287, mean_q: 9177.412207, mean_eps: 0.942345\n",
      " 3050/5000: episode: 305, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4789.900 [537.000, 9128.000],  loss: 2204818.868750, mae: 1864.890161, mean_q: 10098.211621, mean_eps: 0.942155\n",
      " 3060/5000: episode: 306, duration: 0.313s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4687.500 [372.000, 9011.000],  loss: 1894934.487500, mae: 1928.174927, mean_q: 10481.446094, mean_eps: 0.941965\n",
      " 3070/5000: episode: 307, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6666.900 [719.000, 9334.000],  loss: 1361939.678125, mae: 2035.294714, mean_q: 11134.073340, mean_eps: 0.941774\n",
      " 3080/5000: episode: 308, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5010.900 [15.000, 9632.000],  loss: 1746210.400000, mae: 2095.404675, mean_q: 11462.790820, mean_eps: 0.941585\n",
      " 3090/5000: episode: 309, duration: 0.255s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5382.300 [3382.000, 7760.000],  loss: 2420139.518750, mae: 2260.635107, mean_q: 12303.832227, mean_eps: 0.941395\n",
      " 3100/5000: episode: 310, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5777.500 [1497.000, 9062.000],  loss: 3580787.250000, mae: 2427.413989, mean_q: 13128.504883, mean_eps: 0.941205\n",
      " 3110/5000: episode: 311, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3680.600 [40.000, 7482.000],  loss: 1903019.093750, mae: 2433.670361, mean_q: 13331.978027, mean_eps: 0.941015\n",
      " 3120/5000: episode: 312, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5324.400 [136.000, 8569.000],  loss: 2061868.268750, mae: 2357.654419, mean_q: 13000.343359, mean_eps: 0.940825\n",
      " 3130/5000: episode: 313, duration: 0.385s, episode steps:  10, steps per second:  26, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3611.100 [241.000, 6174.000],  loss: 2408887.237500, mae: 2477.650146, mean_q: 13648.001270, mean_eps: 0.940635\n",
      " 3140/5000: episode: 314, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4034.200 [352.000, 9450.000],  loss: 3777300.212500, mae: 2508.931079, mean_q: 13754.572363, mean_eps: 0.940445\n",
      " 3150/5000: episode: 315, duration: 0.316s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6189.600 [875.000, 9493.000],  loss: 2663887.987500, mae: 2649.809619, mean_q: 14433.386328, mean_eps: 0.940254\n",
      " 3160/5000: episode: 316, duration: 0.323s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5406.800 [610.000, 9423.000],  loss: 3903879.943750, mae: 2794.472437, mean_q: 15110.655859, mean_eps: 0.940064\n",
      " 3170/5000: episode: 317, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6058.100 [1878.000, 9083.000],  loss: 4482795.462500, mae: 2933.031982, mean_q: 15646.960938, mean_eps: 0.939874\n",
      " 3180/5000: episode: 318, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4518.100 [201.000, 9722.000],  loss: 5071800.543750, mae: 3008.761401, mean_q: 16315.174414, mean_eps: 0.939684\n",
      " 3190/5000: episode: 319, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7832.800 [5190.000, 9308.000],  loss: 4347027.412500, mae: 3185.358813, mean_q: 17335.328906, mean_eps: 0.939495\n",
      " 3200/5000: episode: 320, duration: 0.331s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4011.000 [1499.000, 6898.000],  loss: 4436804.850000, mae: 3295.775684, mean_q: 17865.115625, mean_eps: 0.939304\n",
      " 3210/5000: episode: 321, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5445.300 [289.000, 9394.000],  loss: 4070570.325000, mae: 3421.682886, mean_q: 18433.927148, mean_eps: 0.939114\n",
      " 3220/5000: episode: 322, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5729.700 [1098.000, 9643.000],  loss: 6690580.550000, mae: 3557.839038, mean_q: 19037.885156, mean_eps: 0.938924\n",
      " 3230/5000: episode: 323, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5037.500 [1733.000, 9294.000],  loss: 4985684.843750, mae: 3818.542944, mean_q: 20302.200781, mean_eps: 0.938735\n",
      " 3240/5000: episode: 324, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4132.700 [492.000, 9689.000],  loss: 7623259.675000, mae: 4037.157251, mean_q: 21338.756445, mean_eps: 0.938545\n",
      " 3250/5000: episode: 325, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4272.000 [991.000, 9092.000],  loss: 5348793.687500, mae: 4137.018726, mean_q: 21901.377930, mean_eps: 0.938354\n",
      " 3260/5000: episode: 326, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3603.300 [289.000, 8813.000],  loss: 6874706.625000, mae: 4293.393140, mean_q: 22658.574414, mean_eps: 0.938165\n",
      " 3270/5000: episode: 327, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5326.400 [829.000, 9025.000],  loss: 11590908.525000, mae: 4568.013818, mean_q: 23991.474414, mean_eps: 0.937974\n",
      " 3280/5000: episode: 328, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4449.300 [503.000, 9479.000],  loss: 7660402.850000, mae: 4713.677930, mean_q: 24618.692578, mean_eps: 0.937785\n",
      " 3290/5000: episode: 329, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3190.300 [294.000, 7205.000],  loss: 8369545.950000, mae: 4849.893652, mean_q: 25186.337305, mean_eps: 0.937594\n",
      " 3300/5000: episode: 330, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5575.800 [1544.000, 8983.000],  loss: 9257440.900000, mae: 5025.010889, mean_q: 25940.666602, mean_eps: 0.937405\n",
      " 3310/5000: episode: 331, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4823.800 [625.000, 9326.000],  loss: 11454002.950000, mae: 5204.602441, mean_q: 26700.627344, mean_eps: 0.937215\n",
      " 3320/5000: episode: 332, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4608.700 [249.000, 9501.000],  loss: 8311932.675000, mae: 5494.103711, mean_q: 28069.183594, mean_eps: 0.937025\n",
      " 3330/5000: episode: 333, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 6507.900 [2122.000, 9492.000],  loss: 7884125.531250, mae: 5386.920557, mean_q: 27555.780859, mean_eps: 0.936835\n",
      " 3340/5000: episode: 334, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5628.300 [2026.000, 9568.000],  loss: 14095731.350000, mae: 5461.279688, mean_q: 27907.845117, mean_eps: 0.936645\n",
      " 3350/5000: episode: 335, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4218.800 [177.000, 9568.000],  loss: 14123510.100000, mae: 5815.309473, mean_q: 29421.298633, mean_eps: 0.936454\n",
      " 3360/5000: episode: 336, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3710.000 [68.000, 7801.000],  loss: 11654439.200000, mae: 5832.768848, mean_q: 29421.579102, mean_eps: 0.936264\n",
      " 3370/5000: episode: 337, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3670.500 [48.000, 7881.000],  loss: 9011714.425000, mae: 5913.306934, mean_q: 29986.507812, mean_eps: 0.936074\n",
      " 3380/5000: episode: 338, duration: 0.290s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4118.500 [383.000, 8644.000],  loss: 14600177.875000, mae: 6339.912598, mean_q: 32407.405078, mean_eps: 0.935884\n",
      " 3390/5000: episode: 339, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3753.600 [505.000, 8644.000],  loss: 16436910.550000, mae: 6452.166504, mean_q: 33015.000391, mean_eps: 0.935694\n",
      " 3400/5000: episode: 340, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4622.800 [639.000, 8662.000],  loss: 16647779.325000, mae: 6759.714502, mean_q: 34469.086328, mean_eps: 0.935504\n",
      " 3410/5000: episode: 341, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5837.300 [1893.000, 9664.000],  loss: 17447276.250000, mae: 6707.607617, mean_q: 33939.492969, mean_eps: 0.935314\n",
      " 3420/5000: episode: 342, duration: 0.325s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4454.500 [485.000, 9611.000],  loss: 21934340.800000, mae: 7081.400488, mean_q: 35938.008203, mean_eps: 0.935125\n",
      " 3430/5000: episode: 343, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4537.800 [729.000, 9358.000],  loss: 20210370.950000, mae: 7391.251514, mean_q: 37370.892578, mean_eps: 0.934935\n",
      " 3440/5000: episode: 344, duration: 0.260s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4304.000 [964.000, 8730.000],  loss: 26396738.950000, mae: 7816.090625, mean_q: 39662.171484, mean_eps: 0.934745\n",
      " 3450/5000: episode: 345, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4981.300 [1244.000, 9228.000],  loss: 28437654.000000, mae: 7782.461963, mean_q: 40344.117578, mean_eps: 0.934554\n",
      " 3460/5000: episode: 346, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5824.900 [1273.000, 9450.000],  loss: 28602396.300000, mae: 8307.355518, mean_q: 43214.198047, mean_eps: 0.934365\n",
      " 3470/5000: episode: 347, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7195.300 [4270.000, 9660.000],  loss: 25955411.900000, mae: 8501.940576, mean_q: 44283.589844, mean_eps: 0.934174\n",
      " 3480/5000: episode: 348, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6566.400 [2106.000, 8865.000],  loss: 46754193.400000, mae: 8978.649023, mean_q: 47144.841406, mean_eps: 0.933984\n",
      " 3490/5000: episode: 349, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5183.300 [855.000, 8440.000],  loss: 30495498.500000, mae: 9279.662500, mean_q: 48713.803516, mean_eps: 0.933795\n",
      " 3500/5000: episode: 350, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6162.100 [222.000, 9689.000],  loss: 49089310.700000, mae: 9638.998047, mean_q: 49835.122266, mean_eps: 0.933605\n",
      " 3510/5000: episode: 351, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5375.500 [1766.000, 8038.000],  loss: 51155796.000000, mae: 10004.223535, mean_q: 49899.545703, mean_eps: 0.933415\n",
      " 3520/5000: episode: 352, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4524.600 [435.000, 7833.000],  loss: 29589162.500000, mae: 10422.048242, mean_q: 51788.503906, mean_eps: 0.933225\n",
      " 3530/5000: episode: 353, duration: 0.316s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5520.800 [726.000, 8738.000],  loss: 44455832.000000, mae: 10609.348438, mean_q: 52786.941406, mean_eps: 0.933035\n",
      " 3540/5000: episode: 354, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5406.400 [1257.000, 8120.000],  loss: 51436845.625000, mae: 10987.345801, mean_q: 54509.683984, mean_eps: 0.932845\n",
      " 3550/5000: episode: 355, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6083.900 [666.000, 9081.000],  loss: 73294467.400000, mae: 11618.289941, mean_q: 57374.300781, mean_eps: 0.932654\n",
      " 3560/5000: episode: 356, duration: 0.291s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5328.600 [3713.000, 6707.000],  loss: 56172333.900000, mae: 11690.250391, mean_q: 57406.373047, mean_eps: 0.932465\n",
      " 3570/5000: episode: 357, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4976.900 [366.000, 9317.000],  loss: 59234436.000000, mae: 11921.370117, mean_q: 58326.122266, mean_eps: 0.932274\n",
      " 3580/5000: episode: 358, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4252.500 [935.000, 8792.000],  loss: 81070739.400000, mae: 12708.578809, mean_q: 62365.480078, mean_eps: 0.932084\n",
      " 3590/5000: episode: 359, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2907.300 [254.000, 5775.000],  loss: 84519764.100000, mae: 12623.975098, mean_q: 61857.994922, mean_eps: 0.931894\n",
      " 3600/5000: episode: 360, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3929.400 [717.000, 9321.000],  loss: 82507367.600000, mae: 12959.323535, mean_q: 63418.216016, mean_eps: 0.931704\n",
      " 3610/5000: episode: 361, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5924.600 [1005.000, 8985.000],  loss: 61425386.600000, mae: 13202.191113, mean_q: 64587.988281, mean_eps: 0.931514\n",
      " 3620/5000: episode: 362, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6242.200 [2118.000, 9215.000],  loss: 67272493.200000, mae: 13957.394238, mean_q: 68120.995313, mean_eps: 0.931324\n",
      " 3630/5000: episode: 363, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5568.700 [166.000, 9422.000],  loss: 63022012.400000, mae: 14275.137891, mean_q: 69275.108203, mean_eps: 0.931134\n",
      " 3640/5000: episode: 364, duration: 0.295s, episode steps:  10, steps per second:  34, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5556.100 [190.000, 9215.000],  loss: 81101184.800000, mae: 14809.603809, mean_q: 71556.346875, mean_eps: 0.930945\n",
      " 3650/5000: episode: 365, duration: 0.290s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4006.300 [695.000, 8912.000],  loss: 97261295.600000, mae: 15187.553125, mean_q: 73211.794531, mean_eps: 0.930754\n",
      " 3660/5000: episode: 366, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4667.100 [340.000, 9553.000],  loss: 88907913.600000, mae: 15654.446289, mean_q: 74891.265625, mean_eps: 0.930565\n",
      " 3670/5000: episode: 367, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5385.300 [1760.000, 9526.000],  loss: 104066173.400000, mae: 15858.204199, mean_q: 75821.079687, mean_eps: 0.930375\n",
      " 3680/5000: episode: 368, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5338.600 [246.000, 8632.000],  loss: 143109322.400000, mae: 16284.353809, mean_q: 77708.387500, mean_eps: 0.930184\n",
      " 3690/5000: episode: 369, duration: 0.320s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4395.700 [11.000, 8514.000],  loss: 181478431.200000, mae: 16694.199609, mean_q: 79583.737500, mean_eps: 0.929995\n",
      " 3700/5000: episode: 370, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4840.100 [59.000, 8514.000],  loss: 115096290.000000, mae: 17684.191602, mean_q: 85435.179688, mean_eps: 0.929805\n",
      " 3710/5000: episode: 371, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5573.900 [1819.000, 9481.000],  loss: 116091808.000000, mae: 18323.119531, mean_q: 86948.373438, mean_eps: 0.929615\n",
      " 3720/5000: episode: 372, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3643.700 [62.000, 8294.000],  loss: 115672586.800000, mae: 18290.885937, mean_q: 85884.040625, mean_eps: 0.929425\n",
      " 3730/5000: episode: 373, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4861.800 [458.000, 9054.000],  loss: 169046994.800000, mae: 18930.113477, mean_q: 88686.110937, mean_eps: 0.929235\n",
      " 3740/5000: episode: 374, duration: 0.253s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6430.800 [2071.000, 8295.000],  loss: 148638203.200000, mae: 19929.762695, mean_q: 94125.170313, mean_eps: 0.929045\n",
      " 3750/5000: episode: 375, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3975.200 [373.000, 8135.000],  loss: 134664661.200000, mae: 20033.252930, mean_q: 94914.445312, mean_eps: 0.928854\n",
      " 3760/5000: episode: 376, duration: 0.251s, episode steps:  10, steps per second:  40, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 6393.200 [338.000, 9587.000],  loss: 185560331.200000, mae: 20797.861523, mean_q: 98308.666406, mean_eps: 0.928665\n",
      " 3770/5000: episode: 377, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5109.800 [271.000, 8367.000],  loss: 209732840.800000, mae: 21943.036328, mean_q: 103344.994531, mean_eps: 0.928474\n",
      " 3780/5000: episode: 378, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4731.000 [209.000, 9158.000],  loss: 219273167.600000, mae: 21650.601562, mean_q: 101752.333594, mean_eps: 0.928284\n",
      " 3790/5000: episode: 379, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7021.200 [2190.000, 9491.000],  loss: 191248266.400000, mae: 22808.684961, mean_q: 108083.860937, mean_eps: 0.928094\n",
      " 3800/5000: episode: 380, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2919.700 [35.000, 8028.000],  loss: 176560995.200000, mae: 22977.297266, mean_q: 114025.682031, mean_eps: 0.927905\n",
      " 3810/5000: episode: 381, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 3770.200 [177.000, 9491.000],  loss: 162363887.200000, mae: 24314.622070, mean_q: 122812.632031, mean_eps: 0.927714\n",
      " 3820/5000: episode: 382, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5618.100 [966.000, 9698.000],  loss: 327223156.000000, mae: 24160.856445, mean_q: 122066.569531, mean_eps: 0.927524\n",
      " 3830/5000: episode: 383, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6338.400 [2218.000, 9491.000],  loss: 329120810.400000, mae: 25910.684766, mean_q: 130303.089063, mean_eps: 0.927334\n",
      " 3840/5000: episode: 384, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4784.400 [87.000, 9467.000],  loss: 329299516.800000, mae: 26531.154102, mean_q: 130465.657812, mean_eps: 0.927145\n",
      " 3850/5000: episode: 385, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5110.500 [791.000, 8942.000],  loss: 339287064.000000, mae: 26725.211133, mean_q: 124028.986719, mean_eps: 0.926955\n",
      " 3860/5000: episode: 386, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4140.600 [263.000, 8619.000],  loss: 296057004.800000, mae: 28236.645117, mean_q: 130496.058594, mean_eps: 0.926764\n",
      " 3870/5000: episode: 387, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6585.600 [2285.000, 8605.000],  loss: 298646860.000000, mae: 28815.924219, mean_q: 133177.534375, mean_eps: 0.926575\n",
      " 3880/5000: episode: 388, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6380.700 [1950.000, 9689.000],  loss: 270131884.000000, mae: 28276.747070, mean_q: 131385.191406, mean_eps: 0.926384\n",
      " 3890/5000: episode: 389, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4942.500 [1055.000, 9689.000],  loss: 389949933.600000, mae: 30183.406641, mean_q: 140341.251562, mean_eps: 0.926195\n",
      " 3900/5000: episode: 390, duration: 0.307s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5665.800 [2429.000, 7985.000],  loss: 461126180.800000, mae: 31653.858789, mean_q: 146727.087500, mean_eps: 0.926005\n",
      " 3910/5000: episode: 391, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5402.800 [1377.000, 9519.000],  loss: 276730206.000000, mae: 31995.881055, mean_q: 149721.440625, mean_eps: 0.925815\n",
      " 3920/5000: episode: 392, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6177.600 [33.000, 9479.000],  loss: 449141418.400000, mae: 32699.597266, mean_q: 153626.954687, mean_eps: 0.925625\n",
      " 3930/5000: episode: 393, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4944.100 [912.000, 8294.000],  loss: 517642924.800000, mae: 34143.907422, mean_q: 160063.714062, mean_eps: 0.925435\n",
      " 3940/5000: episode: 394, duration: 0.403s, episode steps:  10, steps per second:  25, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4330.300 [130.000, 9687.000],  loss: 366022521.600000, mae: 34332.318750, mean_q: 160294.529688, mean_eps: 0.925245\n",
      " 3950/5000: episode: 395, duration: 0.330s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5022.900 [785.000, 9480.000],  loss: 586866918.400000, mae: 36054.146094, mean_q: 167253.921875, mean_eps: 0.925055\n",
      " 3960/5000: episode: 396, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5206.500 [998.000, 8255.000],  loss: 671656928.000000, mae: 37393.139844, mean_q: 172090.553125, mean_eps: 0.924864\n",
      " 3970/5000: episode: 397, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4681.300 [582.000, 9492.000],  loss: 775870374.400000, mae: 36449.253516, mean_q: 166786.045313, mean_eps: 0.924674\n",
      " 3980/5000: episode: 398, duration: 0.302s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5163.300 [710.000, 9128.000],  loss: 643493808.000000, mae: 38100.182813, mean_q: 173304.631250, mean_eps: 0.924484\n",
      " 3990/5000: episode: 399, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3854.000 [170.000, 6795.000],  loss: 643864292.000000, mae: 38934.204297, mean_q: 175989.540625, mean_eps: 0.924294\n",
      " 4000/5000: episode: 400, duration: 0.370s, episode steps:  10, steps per second:  27, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5978.300 [1603.000, 9141.000],  loss: 784758499.200000, mae: 41570.291016, mean_q: 187428.854687, mean_eps: 0.924104\n",
      " 4010/5000: episode: 401, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5085.400 [548.000, 9730.000],  loss: 750897697.600000, mae: 41323.448438, mean_q: 186197.712500, mean_eps: 0.923914\n",
      " 4020/5000: episode: 402, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4731.800 [891.000, 8566.000],  loss: 1006832412.800000, mae: 43969.367578, mean_q: 197490.889063, mean_eps: 0.923724\n",
      " 4030/5000: episode: 403, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5659.800 [1521.000, 9515.000],  loss: 691498024.000000, mae: 44015.616797, mean_q: 197459.395313, mean_eps: 0.923535\n",
      " 4040/5000: episode: 404, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5409.100 [535.000, 9128.000],  loss: 629659998.400000, mae: 44991.492188, mean_q: 202011.787500, mean_eps: 0.923345\n",
      " 4050/5000: episode: 405, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5022.200 [2554.000, 8252.000],  loss: 594702838.400000, mae: 46401.786719, mean_q: 207620.503125, mean_eps: 0.923155\n",
      " 4060/5000: episode: 406, duration: 0.253s, episode steps:  10, steps per second:  40, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4847.400 [363.000, 9010.000],  loss: 988036828.800000, mae: 46275.167969, mean_q: 207846.653125, mean_eps: 0.922964\n",
      " 4070/5000: episode: 407, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5546.600 [1741.000, 8936.000],  loss: 962686121.600000, mae: 48599.585547, mean_q: 221663.987500, mean_eps: 0.922775\n",
      " 4080/5000: episode: 408, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5480.500 [654.000, 9347.000],  loss: 1018318569.600000, mae: 49812.451172, mean_q: 221448.345312, mean_eps: 0.922584\n",
      " 4090/5000: episode: 409, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5962.000 [1301.000, 9730.000],  loss: 1459339776.000000, mae: 49911.361328, mean_q: 224146.623438, mean_eps: 0.922395\n",
      " 4100/5000: episode: 410, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6350.200 [1940.000, 9730.000],  loss: 1682951257.600000, mae: 51110.185937, mean_q: 229797.190625, mean_eps: 0.922205\n",
      " 4110/5000: episode: 411, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7327.100 [2011.000, 9730.000],  loss: 1176131112.000000, mae: 53251.750391, mean_q: 239712.976562, mean_eps: 0.922015\n",
      " 4120/5000: episode: 412, duration: 0.512s, episode steps:  10, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4920.500 [90.000, 9414.000],  loss: 1150229964.800000, mae: 54469.680469, mean_q: 246673.532812, mean_eps: 0.921825\n",
      " 4130/5000: episode: 413, duration: 0.300s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5935.900 [2372.000, 8890.000],  loss: 1313599100.800000, mae: 55185.207422, mean_q: 249517.417187, mean_eps: 0.921635\n",
      " 4140/5000: episode: 414, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5122.400 [2091.000, 8560.000],  loss: 1525858860.800000, mae: 56425.817969, mean_q: 255021.723438, mean_eps: 0.921445\n",
      " 4150/5000: episode: 415, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4052.400 [930.000, 9657.000],  loss: 1094499289.600000, mae: 59234.672266, mean_q: 268761.734375, mean_eps: 0.921255\n",
      " 4160/5000: episode: 416, duration: 0.322s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6230.000 [979.000, 9680.000],  loss: 1475040908.800000, mae: 60884.868359, mean_q: 277561.765625, mean_eps: 0.921064\n",
      " 4170/5000: episode: 417, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6151.000 [624.000, 9657.000],  loss: 2221155366.400000, mae: 61939.015234, mean_q: 282676.106250, mean_eps: 0.920875\n",
      " 4180/5000: episode: 418, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4621.200 [443.000, 8932.000],  loss: 1946755654.400000, mae: 64383.288672, mean_q: 287684.187500, mean_eps: 0.920684\n",
      " 4190/5000: episode: 419, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4743.300 [98.000, 9741.000],  loss: 1973373990.400000, mae: 62447.995703, mean_q: 277682.843750, mean_eps: 0.920494\n",
      " 4200/5000: episode: 420, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6715.400 [2964.000, 9676.000],  loss: 2596230681.600000, mae: 66852.047656, mean_q: 296337.453125, mean_eps: 0.920304\n",
      " 4210/5000: episode: 421, duration: 0.309s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4068.900 [1276.000, 9676.000],  loss: 1650015638.400000, mae: 68252.376953, mean_q: 303780.325000, mean_eps: 0.920114\n",
      " 4220/5000: episode: 422, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6238.600 [1989.000, 9676.000],  loss: 2328008876.800000, mae: 69167.037891, mean_q: 307486.031250, mean_eps: 0.919924\n",
      " 4230/5000: episode: 423, duration: 0.308s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4952.000 [1309.000, 9597.000],  loss: 2379876108.800000, mae: 71841.165625, mean_q: 316835.884375, mean_eps: 0.919735\n",
      " 4240/5000: episode: 424, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 2774.300 [74.000, 7864.000],  loss: 3355908966.400000, mae: 75308.439844, mean_q: 327754.800000, mean_eps: 0.919544\n",
      " 4250/5000: episode: 425, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6321.700 [2595.000, 9706.000],  loss: 3234537408.000000, mae: 77055.667969, mean_q: 339065.915625, mean_eps: 0.919355\n",
      " 4260/5000: episode: 426, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5104.700 [1605.000, 9086.000],  loss: 2865048934.400000, mae: 73808.732813, mean_q: 325469.228125, mean_eps: 0.919164\n",
      " 4270/5000: episode: 427, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4442.600 [63.000, 9321.000],  loss: 2507962796.800000, mae: 78851.577344, mean_q: 346762.421875, mean_eps: 0.918975\n",
      " 4280/5000: episode: 428, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5864.600 [2899.000, 8939.000],  loss: 3478724230.400000, mae: 77146.635938, mean_q: 337602.665625, mean_eps: 0.918784\n",
      " 4290/5000: episode: 429, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5408.500 [1068.000, 9490.000],  loss: 2766565452.800000, mae: 81321.318750, mean_q: 352378.690625, mean_eps: 0.918594\n",
      " 4300/5000: episode: 430, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5570.600 [5.000, 9490.000],  loss: 2404921235.200000, mae: 80724.792969, mean_q: 346362.800000, mean_eps: 0.918405\n",
      " 4310/5000: episode: 431, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5540.900 [501.000, 9715.000],  loss: 2455988512.000000, mae: 84145.767969, mean_q: 361261.640625, mean_eps: 0.918215\n",
      " 4320/5000: episode: 432, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4677.800 [1324.000, 8825.000],  loss: 3808726406.400000, mae: 86253.360156, mean_q: 370444.931250, mean_eps: 0.918025\n",
      " 4330/5000: episode: 433, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5897.400 [613.000, 9490.000],  loss: 4023283200.000000, mae: 90177.389063, mean_q: 382282.446875, mean_eps: 0.917835\n",
      " 4340/5000: episode: 434, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4446.500 [386.000, 8942.000],  loss: 3025343411.200000, mae: 89975.343750, mean_q: 379956.421875, mean_eps: 0.917645\n",
      " 4350/5000: episode: 435, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4584.500 [679.000, 7955.000],  loss: 4139040768.000000, mae: 92075.141406, mean_q: 390607.834375, mean_eps: 0.917455\n",
      " 4360/5000: episode: 436, duration: 0.269s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3849.700 [760.000, 7737.000],  loss: 4263691289.600000, mae: 93935.473437, mean_q: 398082.921875, mean_eps: 0.917264\n",
      " 4370/5000: episode: 437, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5538.600 [1354.000, 9060.000],  loss: 3988100249.600000, mae: 97105.071094, mean_q: 409962.659375, mean_eps: 0.917075\n",
      " 4380/5000: episode: 438, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  3.000, mean reward:  0.300 [ 0.000,  1.000], mean action: 3996.000 [25.000, 8997.000],  loss: 3350642604.800000, mae: 93464.132812, mean_q: 391789.928125, mean_eps: 0.916884\n",
      " 4390/5000: episode: 439, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4728.900 [7.000, 9631.000],  loss: 4261723584.000000, mae: 102691.929688, mean_q: 432550.965625, mean_eps: 0.916694\n",
      " 4400/5000: episode: 440, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5738.700 [848.000, 9552.000],  loss: 4247613888.000000, mae: 101630.379687, mean_q: 429565.818750, mean_eps: 0.916504\n",
      " 4410/5000: episode: 441, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3879.700 [306.000, 9631.000],  loss: 4857896780.800000, mae: 104706.797656, mean_q: 448228.456250, mean_eps: 0.916315\n",
      " 4420/5000: episode: 442, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4865.100 [261.000, 9631.000],  loss: 5012455475.200000, mae: 104981.300000, mean_q: 450242.159375, mean_eps: 0.916124\n",
      " 4430/5000: episode: 443, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5771.200 [202.000, 9631.000],  loss: 5965003084.800000, mae: 103781.714844, mean_q: 443868.996875, mean_eps: 0.915934\n",
      " 4440/5000: episode: 444, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5958.200 [619.000, 9676.000],  loss: 6504374796.800000, mae: 111465.402344, mean_q: 473701.559375, mean_eps: 0.915744\n",
      " 4450/5000: episode: 445, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4576.800 [951.000, 8932.000],  loss: 4649873625.600000, mae: 111499.861719, mean_q: 479222.581250, mean_eps: 0.915555\n",
      " 4460/5000: episode: 446, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5110.800 [15.000, 9069.000],  loss: 6140135603.200000, mae: 116940.139844, mean_q: 495025.453125, mean_eps: 0.915365\n",
      " 4470/5000: episode: 447, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5327.100 [124.000, 9275.000],  loss: 5880194560.000000, mae: 115009.746875, mean_q: 485215.659375, mean_eps: 0.915175\n",
      " 4480/5000: episode: 448, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3649.600 [224.000, 8853.000],  loss: 7304319795.200000, mae: 120567.473437, mean_q: 504951.656250, mean_eps: 0.914985\n",
      " 4490/5000: episode: 449, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5184.300 [2006.000, 9435.000],  loss: 7050166118.400000, mae: 122274.692969, mean_q: 511556.859375, mean_eps: 0.914794\n",
      " 4500/5000: episode: 450, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5026.200 [150.000, 9435.000],  loss: 8138860441.600000, mae: 128491.699219, mean_q: 536484.150000, mean_eps: 0.914605\n",
      " 4510/5000: episode: 451, duration: 0.287s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5192.500 [470.000, 9515.000],  loss: 8582806784.000000, mae: 129026.630469, mean_q: 542572.343750, mean_eps: 0.914415\n",
      " 4520/5000: episode: 452, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4359.700 [296.000, 9174.000],  loss: 7999425792.000000, mae: 132119.383594, mean_q: 563974.381250, mean_eps: 0.914225\n",
      " 4530/5000: episode: 453, duration: 0.294s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6353.100 [1049.000, 8918.000],  loss: 9211679347.200001, mae: 131318.828125, mean_q: 549839.621875, mean_eps: 0.914035\n",
      " 4540/5000: episode: 454, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5300.000 [1017.000, 9421.000],  loss: 10420791654.400000, mae: 133184.346094, mean_q: 552579.562500, mean_eps: 0.913845\n",
      " 4550/5000: episode: 455, duration: 0.284s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5230.300 [387.000, 9520.000],  loss: 8531624678.400000, mae: 137735.630469, mean_q: 568637.981250, mean_eps: 0.913655\n",
      " 4560/5000: episode: 456, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5120.700 [267.000, 9317.000],  loss: 12251409331.200001, mae: 143621.933594, mean_q: 590363.900000, mean_eps: 0.913465\n",
      " 4570/5000: episode: 457, duration: 0.478s, episode steps:  10, steps per second:  21, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4337.500 [64.000, 9083.000],  loss: 7006477932.800000, mae: 145702.835938, mean_q: 605169.218750, mean_eps: 0.913274\n",
      " 4580/5000: episode: 458, duration: 0.340s, episode steps:  10, steps per second:  29, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6023.400 [220.000, 9566.000],  loss: 7957107916.800000, mae: 142857.401562, mean_q: 599264.781250, mean_eps: 0.913084\n",
      " 4590/5000: episode: 459, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4861.700 [461.000, 8673.000],  loss: 9701050342.400000, mae: 146260.695312, mean_q: 614288.125000, mean_eps: 0.912894\n",
      " 4600/5000: episode: 460, duration: 0.280s, episode steps:  10, steps per second:  36, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 4382.300 [885.000, 8285.000],  loss: 11902196403.200001, mae: 152557.035938, mean_q: 639400.943750, mean_eps: 0.912704\n",
      " 4610/5000: episode: 461, duration: 0.260s, episode steps:  10, steps per second:  38, episode reward:  1.000, mean reward:  0.100 [ 0.000,  1.000], mean action: 5396.200 [1289.000, 9657.000],  loss: 12419709440.000000, mae: 156467.835938, mean_q: 643181.137500, mean_eps: 0.912515\n",
      " 4620/5000: episode: 462, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5137.700 [30.000, 9505.000],  loss: 12589913011.200001, mae: 157229.221875, mean_q: 650582.456250, mean_eps: 0.912324\n",
      " 4630/5000: episode: 463, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7549.600 [1209.000, 9631.000],  loss: 12821209753.600000, mae: 160096.178125, mean_q: 669570.537500, mean_eps: 0.912134\n",
      " 4640/5000: episode: 464, duration: 0.266s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5945.300 [2609.000, 8987.000],  loss: 15019660032.000000, mae: 167238.798437, mean_q: 702727.725000, mean_eps: 0.911944\n",
      " 4650/5000: episode: 465, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5461.900 [732.000, 9631.000],  loss: 12083330713.600000, mae: 167835.123438, mean_q: 708784.412500, mean_eps: 0.911755\n",
      " 4660/5000: episode: 466, duration: 0.259s, episode steps:  10, steps per second:  39, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7539.900 [3813.000, 9631.000],  loss: 16660448358.400000, mae: 171917.931250, mean_q: 712512.050000, mean_eps: 0.911565\n",
      " 4670/5000: episode: 467, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5919.700 [3074.000, 9618.000],  loss: 17272851558.400002, mae: 170703.885937, mean_q: 709797.550000, mean_eps: 0.911374\n",
      " 4680/5000: episode: 468, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5813.900 [1846.000, 9657.000],  loss: 13794708684.799999, mae: 180809.903125, mean_q: 753803.012500, mean_eps: 0.911185\n",
      " 4690/5000: episode: 469, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3992.300 [1653.000, 6680.000],  loss: 17646004889.599998, mae: 178011.685938, mean_q: 751868.956250, mean_eps: 0.910994\n",
      " 4700/5000: episode: 470, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6297.200 [833.000, 9730.000],  loss: 21924422144.000000, mae: 190107.085938, mean_q: 811070.950000, mean_eps: 0.910805\n",
      " 4710/5000: episode: 471, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5726.400 [204.000, 9657.000],  loss: 18908547788.799999, mae: 191317.329687, mean_q: 806455.918750, mean_eps: 0.910615\n",
      " 4720/5000: episode: 472, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4845.000 [491.000, 8631.000],  loss: 16783528883.200001, mae: 194303.134375, mean_q: 806800.056250, mean_eps: 0.910425\n",
      " 4730/5000: episode: 473, duration: 0.276s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6202.900 [833.000, 9346.000],  loss: 13166364979.200001, mae: 200668.625000, mean_q: 827417.775000, mean_eps: 0.910235\n",
      " 4740/5000: episode: 474, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7389.000 [3136.000, 9337.000],  loss: 19955342131.200001, mae: 201345.076563, mean_q: 821485.756250, mean_eps: 0.910045\n",
      " 4750/5000: episode: 475, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4082.400 [963.000, 8354.000],  loss: 17304727244.799999, mae: 203438.732813, mean_q: 824796.025000, mean_eps: 0.909855\n",
      " 4760/5000: episode: 476, duration: 0.271s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3233.300 [174.000, 6063.000],  loss: 18054840473.599998, mae: 212975.535938, mean_q: 859321.493750, mean_eps: 0.909665\n",
      " 4770/5000: episode: 477, duration: 0.285s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3545.900 [995.000, 6738.000],  loss: 19798587801.599998, mae: 203112.248438, mean_q: 810388.143750, mean_eps: 0.909474\n",
      " 4780/5000: episode: 478, duration: 0.282s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5747.500 [511.000, 8894.000],  loss: 21546245632.000000, mae: 210718.803125, mean_q: 842369.925000, mean_eps: 0.909284\n",
      " 4790/5000: episode: 479, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4741.400 [1211.000, 9519.000],  loss: 22112207052.799999, mae: 224354.510937, mean_q: 905078.993750, mean_eps: 0.909094\n",
      " 4800/5000: episode: 480, duration: 0.277s, episode steps:  10, steps per second:  36, episode reward:  2.000, mean reward:  0.200 [ 0.000,  1.000], mean action: 4620.200 [925.000, 9657.000],  loss: 25137420595.200001, mae: 222717.143750, mean_q: 905410.256250, mean_eps: 0.908904\n",
      " 4810/5000: episode: 481, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 3702.900 [238.000, 8017.000],  loss: 31415577395.200001, mae: 220987.587500, mean_q: 899960.637500, mean_eps: 0.908714\n",
      " 4820/5000: episode: 482, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5647.700 [402.000, 9287.000],  loss: 27144812646.400002, mae: 236141.604687, mean_q: 962259.543750, mean_eps: 0.908524\n",
      " 4830/5000: episode: 483, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4917.500 [425.000, 9110.000],  loss: 27293958348.799999, mae: 238128.437500, mean_q: 971691.293750, mean_eps: 0.908334\n",
      " 4840/5000: episode: 484, duration: 0.281s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4801.900 [5.000, 9636.000],  loss: 26051771187.200001, mae: 232665.868750, mean_q: 947029.237500, mean_eps: 0.908145\n",
      " 4850/5000: episode: 485, duration: 0.279s, episode steps:  10, steps per second:  36, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4475.600 [478.000, 8574.000],  loss: 34514528563.199997, mae: 247618.426563, mean_q: 1009937.850000, mean_eps: 0.907954\n",
      " 4860/5000: episode: 486, duration: 0.303s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5357.200 [47.000, 9699.000],  loss: 21012857804.799999, mae: 253726.243750, mean_q: 1038529.200000, mean_eps: 0.907765\n",
      " 4870/5000: episode: 487, duration: 0.304s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4419.200 [582.000, 7869.000],  loss: 27672860262.400002, mae: 253843.690625, mean_q: 1037671.437500, mean_eps: 0.907574\n",
      " 4880/5000: episode: 488, duration: 0.288s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5034.000 [279.000, 9095.000],  loss: 33381756928.000000, mae: 255993.832813, mean_q: 1043219.675000, mean_eps: 0.907385\n",
      " 4890/5000: episode: 489, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5043.100 [63.000, 9704.000],  loss: 34635489792.000000, mae: 263166.084375, mean_q: 1068363.400000, mean_eps: 0.907194\n",
      " 4900/5000: episode: 490, duration: 0.429s, episode steps:  10, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5629.400 [928.000, 9211.000],  loss: 44559023923.199997, mae: 266776.118750, mean_q: 1106518.375000, mean_eps: 0.907004\n",
      " 4910/5000: episode: 491, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5470.800 [318.000, 8928.000],  loss: 47022901862.400002, mae: 269713.117188, mean_q: 1140396.775000, mean_eps: 0.906815\n",
      " 4920/5000: episode: 492, duration: 0.267s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 7067.400 [1194.000, 9554.000],  loss: 36493782220.800003, mae: 276677.589062, mean_q: 1173144.037500, mean_eps: 0.906625\n",
      " 4930/5000: episode: 493, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6419.700 [1380.000, 9488.000],  loss: 47347706470.400002, mae: 274432.182812, mean_q: 1160306.025000, mean_eps: 0.906435\n",
      " 4940/5000: episode: 494, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5976.000 [1765.000, 9663.000],  loss: 50943221145.599998, mae: 292342.803125, mean_q: 1230167.187500, mean_eps: 0.906245\n",
      " 4950/5000: episode: 495, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5118.100 [238.000, 9450.000],  loss: 61697335500.800003, mae: 291895.828125, mean_q: 1229749.187500, mean_eps: 0.906055\n",
      " 4960/5000: episode: 496, duration: 0.334s, episode steps:  10, steps per second:  30, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5548.900 [118.000, 9019.000],  loss: 59386185318.400002, mae: 294364.665625, mean_q: 1249262.262500, mean_eps: 0.905865\n",
      " 4970/5000: episode: 497, duration: 0.326s, episode steps:  10, steps per second:  31, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 5188.600 [756.000, 8928.000],  loss: 61985341644.800003, mae: 300792.731250, mean_q: 1275930.750000, mean_eps: 0.905674\n",
      " 4980/5000: episode: 498, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4524.600 [2131.000, 7841.000],  loss: 61172753817.599998, mae: 301258.940625, mean_q: 1272897.937500, mean_eps: 0.905485\n",
      " 4990/5000: episode: 499, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 4742.900 [917.000, 9674.000],  loss: 67169649868.800003, mae: 309410.978125, mean_q: 1294341.937500, mean_eps: 0.905294\n",
      " 5000/5000: episode: 500, duration: 0.273s, episode steps:  10, steps per second:  37, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 6417.100 [3645.000, 8928.000],  loss: 75728436224.000000, mae: 321178.750000, mean_q: 1300505.025000, mean_eps: 0.905104\n",
      "done, took 131.733 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14739a880>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn2.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0300\n",
      "Recall@10: 0.0054\n",
      "NDCG@10: 0.0660\n",
      "MAP@10: 0.3000\n"
     ]
    }
   ],
   "source": [
    "precision, recall, ndcg, map_k = evaluate_dqn(dqn2, env, top_k=10, num_users=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Metrics\n",
    "metrics = ['Precision@10', 'Recall@10', 'NDCG@10', 'MAP@10']\n",
    "\n",
    "# Values\n",
    "before = [0.0290, 0.0045, 0.0638, 0.2900]\n",
    "after = [0.0300, 0.0054, 0.0660, 0.3000]\n",
    "\n",
    "x = np.arange(len(metrics))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot bars\n",
    "rects1 = ax.bar(x - width/2, before, width, label='Before Improvement', color='#d1c4e9')\n",
    "rects2 = ax.bar(x + width/2, after, width, label='After Improvement', color='#673ab7')\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('DQN: Comparison of Metrics Before and After Improvement')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Add bar labels\n",
    "ax.bar_label(rects1, padding=3, fmt='%.4f')\n",
    "ax.bar_label(rects2, padding=3, fmt='%.4f')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
